import torch
import os
from peft import get_peft_model,PeftModel
import json
from benchmark.pred_other import get_version
# from benchmark.config.code.config import CORPUS_PATH
from tqdm import tqdm
import warnings
import argparse
import traceback
import logging
import sys
from datetime import datetime
from utils.loraPathConfigure import pathConfigurator
from utils.getDatasetPacks import getPackVersions
from utils.loraTrain.loraTrainUtils import loraModelExists,get_dataloader,buildandTrainLoraModel,getDataExistence,load_lora_model,load_config,create_lora_config,save_lora_model,load_lora_model_withPeft,load_base_model,merge_lora_weights,inference
from utils.loraTrain.loraMerge import uniform_lora_merging
from transformers import AutoTokenizer
from benchmark.config.code.config import VSCC_LOW_BOUND,VSCC_HIGH_BOUND
from benchmark.config.code.config_lora import LORA_CONFIG_PATH
from utils.loraTrain.buildandloadData import collate_fn,getDataset,QADataset,DocstringDataset,getCodeParrotDataset,getSantaCoderFIMDataset
from utils.loraTrain.loraTrainUtils import getEquipAdaptorModel,train_lora_model_withPEFT
from torch.utils.data import DataLoader
from transformers import AutoModelForCausalLM
from utils.getPrompt import get_prompt,formatInput
from utils.loraTrain.loraPred import loadModelandPredictVersiBench,loadModelandPredict
from utils.loraTrain.dataset_loader import load_dataset
from utils.loraTrain.getVersicodeData import getPkgDocstringItems
from utils.getDependencyUtils import dict_to_pkg_ver_tuples
from utils.output_manager import OutputManager
from omegaconf import OmegaConf
from utils.iftModelManager import get_default_manager as get_ift_manager
from utils.iftCardManager import get_default_card_manager
# warnings.filterwarnings("error",category=UserWarning)

def setup_logging(args):
    """
    è®¾ç½®æ—¥å¿—é…ç½®ï¼Œåˆ›å»ºå¸¦æ—¶é—´æˆ³çš„æ—¥å¿—æ–‡ä»¶å¤¹
    """
    # åˆ›å»ºæ—¥å¿—æ ¹ç›®å½•
    log_base_dir = "logs"
    os.makedirs(log_base_dir, exist_ok=True)
    
    # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„æ—¥å¿—å­ç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    task_info = f"{args.task}_{args.knowledge_type}"
    if hasattr(args, 'rank') and args.world_size > 1:
        log_dir = os.path.join(log_base_dir, f"pred_lora_{task_info}_{timestamp}_rank{args.rank}")
    else:
        log_dir = os.path.join(log_base_dir, f"pred_lora_{task_info}_{timestamp}")
    os.makedirs(log_dir, exist_ok=True)
    
    # è®¾ç½®æ—¥å¿—æ–‡ä»¶è·¯å¾„
    log_file = os.path.join(log_dir, f"pred_lora_{task_info}.log")
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    # è®°å½•é…ç½®ä¿¡æ¯
    logging.info(f"æ—¥å¿—ä¿å­˜åˆ°: {log_file}")
    logging.info(f"é¢„æµ‹é…ç½®: task={args.task}, knowledge_type={args.knowledge_type}")
    logging.info(f"model_name={args.model_name}")
    logging.info(f"precision={args.precision}")
    logging.info(f"max_dependency_num={args.max_dependency_num}")
    logging.info(f"loraadaptor_save_path_base={args.loraadaptor_save_path_base}")
    if hasattr(args, 'rank'):
        logging.info(f"å¤šworkeræ¨¡å¼: rank={args.rank}, world_size={args.world_size}")
    
    return log_dir

def get_lora_pred(data,dependencies,args):
    '''
    Description:
        è·å–loraé¢„æµ‹ç»“æœã€‚
        1.å°è¯•loadå¯¹åº”packageçš„loraæƒé‡ï¼Œè‹¥æ²¡æœ‰ï¼Œåˆ™è¿›è¡Œè®­ç»ƒï¼Œè·å¾—å¯¹åº”æƒé‡
        2.ä½¿ç”¨è®­ç»ƒå¥½çš„loraæƒé‡è¿›è¡Œæ¨ç†
    Args:
        data: dict,æ•°æ®
        dependencies: list,ä¾èµ–
        args: å‚æ•°å¯¹è±¡ï¼ŒåŒ…å«æ‰€æœ‰å¿…è¦çš„é…ç½®ä¿¡æ¯
    Returns:
        dict: åŒ…å«idã€answerå’ŒadaptoråŠ è½½ä¿¡æ¯çš„å­—å…¸
    '''
    # è®°å½•æ˜¯å¦å¯ç”¨äº†IFT checkpointåŠŸèƒ½
    adopt_ift = getattr(args, 'adopt_IFT_checkpoint', False)
    if adopt_ift:
        logging.info("ğŸš€ IFT Checkpointæ¨¡å¼å·²å¯ç”¨ - å°†ä¼˜å…ˆåŠ è½½IFTæ¨¡å‹")
    else:
        logging.info("ğŸ“ æ ‡å‡†LoRAæ¨¡å¼ - å°†åŠ è½½æ™®é€šLoRAæ¨¡å‹")
    
    # ä»argsä¸­è·å–é…ç½®ä¿¡æ¯ï¼Œè€Œä¸æ˜¯å¤–éƒ¨åŠ è½½config
    # æ£€æŸ¥æ˜¯å¦æœ‰å¼ºåˆ¶GPUé€‰é¡¹
    force_gpu = getattr(args, 'force_gpu', False)
    base_model, tokenizer = load_base_model(args.model_name, args.device_map, args.precision, force_gpu=force_gpu)
    lora_models_path = {}
    attempted_packages = []
    successful_packages = []
    failed_packages = []
    
    for pkg,version in dict_to_pkg_ver_tuples(dependencies):
        attempted_packages.append(f"{pkg}-{version}")
        try:
            logging.info(f"åŠ è½½{pkg}çš„{version}çš„loraæ¨¡å‹")
            # ä½¿ç”¨argsä¸­çš„é…ç½®ä¿¡æ¯
            # config_dict = {
            #     "model_name": args.model_name,
            #     "save_path_base": args.adaptor_base_path,
            #     "device_map": args.device_map
            # }
            lora_model_path = load_lora_model(pkg, version, args.lora_config, args.knowledge_type, args)
            if pkg not in lora_models_path:
                lora_models_path[pkg] = [lora_model_path]
            else:
                lora_models_path[pkg].append(lora_model_path)
            successful_packages.append(f"{pkg}-{version}")
            logging.info(f"æˆåŠŸåŠ è½½{pkg}çš„{version}çš„loraæ¨¡å‹: {lora_model_path}")
        except Exception as e:
            failed_packages.append(f"{pkg}-{version}")
            logging.error(f"åŠ è½½{pkg}çš„{version}çš„loraæ¨¡å‹å¤±è´¥: {str(e)}")
            traceback.print_exc()
            continue
    
    # è®°å½•åŠ è½½ç»Ÿè®¡ä¿¡æ¯
    total_attempted = len(attempted_packages)
    total_successful = len(successful_packages)
    total_failed = len(failed_packages)
    
    logging.info(f"LoRA package loading summary:")
    logging.info(f"  Attempted: {total_attempted} packages: {attempted_packages}")
    logging.info(f"  Successful: {total_successful} packages: {successful_packages}")
    if failed_packages:
        logging.warning(f"  Failed: {total_failed} packages: {failed_packages}")
    
    # å‡†å¤‡adaptoråŠ è½½ä¿¡æ¯
    adaptor_info = {
        "attempted_packages": attempted_packages,
        "successful_packages": successful_packages,
        "failed_packages": failed_packages,
        "total_attempted": total_attempted,
        "total_successful": total_successful,
        "total_failed": total_failed
    }
    
    # if len(lora_models_path)<1: 
    #     print("No LoRA models loaded successfully, returning empty answer")
    #     return {
    #         "id": data["id"],
    #         "answer": "",
    #         "adaptor_info": adaptor_info,
    #         "merged_adaptor_count": 0
    #     }
    try:
        lora_pred, merged_count = combineLoraWeights_and_predict(data, base_model, tokenizer, lora_models_path, args)
        return {
            "id": data["id"],
            "answer": lora_pred,
            "adaptor_info": adaptor_info,
            "merged_adaptor_count": merged_count
        }
    except Exception as e:
        logging.error(f"é¢„æµ‹è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        traceback.print_exc()
        raise e





def combineLoraWeights_and_predict(data, base_model, tokenizer, lora_models_path, args):
    '''
    1.åˆå¹¶æ¨¡å‹æƒé‡
    2.æ„å»ºè¾“å…¥æç¤º
    3.åŠ è½½åˆå¹¶åçš„æ¨¡å‹è¿›è¡Œæ¨ç†
    4.æ¸…ç†ä¸´æ—¶åˆå¹¶çš„æ¨¡å‹ï¼ŒèŠ‚çœç£ç›˜ç©ºé—´
    Args:
        data: dict,æ•°æ®
        base_model: åŸºç¡€æ¨¡å‹
    constraints:
        å¯¹äºlora_models_pathï¼Œå¦‚æœä¸ºç©ºçš„æƒ…å†µï¼Œä¼šç›´æ¥å¯ç”¨base_modelè¿›è¡Œæ¨ç†
    '''
    try:
        # åˆå¹¶æ¨¡å‹æƒé‡
        # å±•å¹³è·¯å¾„åˆ—è¡¨ï¼Œå› ä¸ºlora_models_path[pkg]æ˜¯ä¸€ä¸ªåŒ…å«è·¯å¾„çš„åˆ—è¡¨
        lora_models_path_list = []
        total_requested_paths = 0
        for pkg, paths in lora_models_path.items():
            if isinstance(paths, list):
                lora_models_path_list.extend(paths)
                total_requested_paths += len(paths)
            else:
                lora_models_path_list.append(paths)
                total_requested_paths += 1
                
        logging.info(f"Attempting to merge {total_requested_paths} LoRA adapters for packages: {list(lora_models_path.keys())}")
 
        # merged_model = merge_lora_weights(base_model, lora_models_path_list)

        # # ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
        # save_path_base = args.loraadaptor_save_path_base
        # merged_save_path = f"{save_path_base}/merged_lora_model"
        # save_lora_model(merged_model, merged_save_path)
        # print(f"åˆå¹¶æ¨¡å‹å·²ä¿å­˜åˆ°: {merged_save_path}")

        save_path_base = args.loraadaptor_save_path_base
        # æ·»åŠ rank_idåˆ°merged modelè·¯å¾„ï¼Œé¿å…å¤šworkerå†²çª
        rank = getattr(args, 'rank', 0)
        if args.tempmerged_adaptor_path is None:
            merged_save_path = f"{save_path_base}/merged_lora_model_rank{rank}"
        else:
            merged_save_path = f"{args.tempmerged_adaptor_path}/merged_lora_model_rank{rank}"
        count = uniform_lora_merging(lora_models_path_list, merged_save_path, 'cuda')
        
        logging.info(f"Successfully merged {count}/{total_requested_paths} LoRA adapters")
        
        if count == 0:
            logging.warning("No adapters were successfully merged, using base model")
        elif count < total_requested_paths:
            logging.warning(f"Only {count} out of {total_requested_paths} adapters were successfully merged")

        # æ„å»ºè¾“å…¥æç¤º
        if args.dataset == 'versicode':
            raise ValueError("versicode not supported")
            # input_prompt = (args.versicode_vscc_prompt 
            #               if args.task == 'vscc' 
            #               else args.versicode_vace_prompt)
            
            # if args.task == 'vscc':
            #     input = input_prompt.format(
            #         description=data["description"],
            #         dependency=data["dependency"],
            #         current_version=data["current_version"]
            #     )
            # else:
            #     input = input_prompt.format(
            #         description=data["description"],
            #         dependency=data["dependency"]
            #     )
        #TODO:promptç›®å‰æ˜¯ä¸´æ—¶ä¹‹ç­–ï¼Œéœ€è¦ç¨ç­‰åæ¢å›ban_deprecationçš„prompt
        elif args.dataset == "versiBCB":
            # if args.Ban_Deprecation:
            #     input_prompt = args.versiBCB_vace_bd_prompt if args.task == "vace" else args.versiBCB_vscc_bd_prompt
            # else:
            input_prompt = args.versiBCB_vace_prompt if args.task == "vace" else args.versiBCB_vscc_prompt

                
            if args.task == "vscc":
                input = input_prompt.format(
                    description=data["description"],
                    dependency=data["dependency"] if "true_dependency" not in data else data["true_dependency"]
                )
            else:
                input = input_prompt.format(
                    description=data["description"],
                    origin_dependency=data["origin_dependency"],
                    origin_code=data["origin_code"],
                    target_dependency=data["target_dependency"] if "true_target_dependency" not in data else data["true_target_dependency"]
                )
        else:
            raise ValueError(f"æ•°æ®é›†ä¸å­˜åœ¨: {args.dataset}")
        
        # åŠ è½½åˆå¹¶åçš„æ¨¡å‹è¿›è¡Œæ¨ç†
        if count == 0:
            loaded_model = base_model
            logging.info("Using base model for inference (no adapters loaded)")
        else:
            try:
                loaded_model = PeftModel.from_pretrained(base_model, merged_save_path)
                logging.info(f"Using merged LoRA model with {count} adapters for inference from {merged_save_path}")
            except Exception as load_error:
                logging.error(f"Error loading merged LoRA model from {merged_save_path}: {load_error}")
                logging.info("Falling back to base model for inference")
                loaded_model = base_model
                count = 0  # Reset count since we're using base model
        
        # ä½¿ç”¨argsä¸­çš„æ¨ç†è®¾ç½®
        result = inference(loaded_model, tokenizer, input, max_new_tokens=1024, 
                         temperature=args.temperature, 
                         top_p=args.top_p)
        
        # æ¸…ç†ä¸´æ—¶åˆå¹¶çš„æ¨¡å‹æ–‡ä»¶ï¼ŒèŠ‚çœç£ç›˜ç©ºé—´
        if count > 0:
            try:
                import shutil
                if os.path.exists(merged_save_path):
                    shutil.rmtree(merged_save_path)
                    logging.info(f"Cleaned up temporary merged model at {merged_save_path}")
            except Exception as cleanup_error:
                logging.warning(f"Warning: Failed to cleanup merged model at {merged_save_path}: {cleanup_error}")
        
        return result, count
        
    except Exception as e:
        logging.error(f"é¢„æµ‹è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        traceback.print_exc()
        raise e

def get_lora_pred_with_cleanup(data, dependencies,args):
    try:
        return get_lora_pred(data, dependencies,args)
    except RuntimeError as e:
        logging.error(f"å‘ç”Ÿé”™è¯¯: {str(e)}")
        traceback.print_exc()
        try:
            # å°è¯•æ¸…ç† GPU ç¼“å­˜
            torch.cuda.empty_cache()
        except:
            # å¦‚æœæ¸…ç†å¤±è´¥ï¼Œå°è¯•é‡ç½® CUDA è®¾å¤‡
            try:
                torch.cuda.reset_device()
            except:
                pass
        # è¿”å›ç©ºç»“æœï¼ŒåŒ…å«adaptorä¿¡æ¯
        return {
            "id": data["id"], 
            "answer": "",
            "adaptor_info": {
                "attempted_packages": [],
                "successful_packages": [],
                "failed_packages": [],
                "total_attempted": 0,
                "total_successful": 0,
                "total_failed": 0,
                "error": str(e)
            },
            "merged_adaptor_count": 0
        }

def load_model_from_ift_meta_config(card_id_or_path, args):
    """
    åŸºäºIFTå…ƒé…ç½®å¡ç‰‡åŠ è½½æ¨¡å‹é…ç½®ï¼Œä½†å…·ä½“çš„é¢„æµ‹æ•°æ®ç”±args.taskç¡®å®š
    
    Args:
        card_id_or_path: str, IFTå…ƒé…ç½®å¡ç‰‡IDæˆ–æ–‡ä»¶è·¯å¾„
        args: argparseå‚æ•°ï¼ŒåŒ…å«taskç­‰ä¿¡æ¯
        
    Returns:
        dict: åŒ…å«æ¨¡å‹é…ç½®çš„å­—å…¸
    """
    try:
        card_manager = get_default_card_manager()
        
        # åŠ è½½IFTå¡ç‰‡
        if card_id_or_path.endswith('.yaml') or card_id_or_path.endswith('.yml'):
            ift_card = card_manager.load_ift_card(card_id_or_path)
        else:
            ift_card = card_manager.get_card_by_id(card_id_or_path)
        
        if not ift_card:
            raise ValueError(f"æ‰¾ä¸åˆ°IFTå¡ç‰‡: {card_id_or_path}")
        
        # éªŒè¯å¡ç‰‡
        if not card_manager.validate_card(ift_card):
            raise ValueError("IFTé…ç½®å¡ç‰‡æ— æ•ˆ")
        
        # æå–é¢„æµ‹é…ç½®
        prediction_config = card_manager.extract_prediction_config(ift_card)
        
        # æ£€æŸ¥ä»»åŠ¡å…¼å®¹æ€§
        compatible_tasks = prediction_config.get("compatible_tasks", ["vace", "vscc"])
        if args.task not in compatible_tasks:
            logging.warning(f"ä»»åŠ¡ {args.task} å¯èƒ½ä¸æ­¤IFTé…ç½®ä¸å…¼å®¹ï¼Œæ”¯æŒçš„ä»»åŠ¡: {compatible_tasks}")
        
        # åº”ç”¨IFTé…ç½®åˆ°argsï¼ˆå…ƒé…ç½®æ–¹å¼ï¼‰
        args.model_name = prediction_config["model_name"]
        if prediction_config.get("tokenizer_name"):
            args.tokenizer_name = prediction_config["tokenizer_name"]
        args.knowledge_type = prediction_config["knowledge_type"]
        args.precision = prediction_config["precision"]
        args.adopt_IFT_checkpoint = True  # å¯ç”¨IFTæ¨¡å¼
        args.ift_type = prediction_config["ift_type"]
        args.ift_data_strategy = prediction_config["data_strategy"]
        args.device_map = prediction_config.get("device_strategy", "auto")
        
        # ä»è·¯å¾„ç­–ç•¥ä¸­æ¨æ–­LoRAåŸºç¡€è·¯å¾„
        path_strategy = prediction_config.get("path_strategy", {})
        lora_base_path = path_strategy.get("lora_base_path_pattern")
        if lora_base_path and (not hasattr(args, 'loraadaptor_save_path_base') or args.loraadaptor_save_path_base is None):
            args.loraadaptor_save_path_base = lora_base_path
        
        # ä¿å­˜IFTé…ç½®ä¿¡æ¯ä¾›åç»­ä½¿ç”¨
        args.ift_card_config = prediction_config
        
        logging.info(f"âœ… å·²åŠ è½½IFTå…ƒé…ç½®: {prediction_config['card_id']}")
        logging.info(f"ğŸ¯ IFTç±»å‹: {prediction_config['ift_type']}")
        logging.info(f"ğŸ“Š æ•°æ®ç­–ç•¥: {prediction_config['data_strategy']}")
        logging.info(f"ğŸ·ï¸ çŸ¥è¯†ç±»å‹: {prediction_config['knowledge_type']}")
        logging.info(f"ğŸ® å½“å‰ä»»åŠ¡: {args.task}")
        
        return prediction_config
        
    except Exception as e:
        logging.error(f"åŠ è½½IFTå…ƒé…ç½®å¤±è´¥: {e}")
        raise


def apply_ift_config_to_lora_training_args(prediction_config, args):
    """
    å°†IFTå…ƒé…ç½®åº”ç”¨åˆ°LoRAè®­ç»ƒå‚æ•°ä¸­
    
    Args:
        prediction_config: dict, ä»IFTå¡ç‰‡æå–çš„é¢„æµ‹é…ç½®
        args: argparseå‚æ•°å¯¹è±¡
    """
    # åº”ç”¨LoRAé…ç½®
    lora_config = prediction_config.get("lora_config", {})
    if lora_config:
        args.lora_r = lora_config.get("r", 64)
        args.lora_alpha = lora_config.get("alpha", 128)
        args.lora_dropout = lora_config.get("dropout", 0.05)
        args.lora_target_modules = lora_config.get("target_modules", [])
        
        logging.info(f"åº”ç”¨LoRAé…ç½®: r={args.lora_r}, alpha={args.lora_alpha}, dropout={args.lora_dropout}")
    
    # åº”ç”¨IFTè®­ç»ƒé…ç½®
    ift_training_config = prediction_config.get("ift_training_config", {})
    if ift_training_config:
        args.ift_learning_rate = ift_training_config.get("learning_rate", 1e-6)
        args.ift_num_epochs = ift_training_config.get("num_epochs", 16)
        args.ift_batch_size = ift_training_config.get("batch_size", 8)
        
        logging.info(f"åº”ç”¨IFTè®­ç»ƒé…ç½®: lr={args.ift_learning_rate}, epochs={args.ift_num_epochs}, batch_size={args.ift_batch_size}")
    
    # åº”ç”¨æ•°æ®æºé…ç½®
    data_sources = prediction_config.get("data_sources", {})
    if data_sources:
        args.ift_data_sources = data_sources
        logging.info(f"åº”ç”¨æ•°æ®æºé…ç½®: {data_sources.keys()}")

def apply_task2maskpacks_filter(dependency, task_id, task2maskpacks):
    """
    æ ¹æ®task2maskpacksé…ç½®è¿‡æ»¤dependency
    
    Args:
        dependency: dict, åŸå§‹ä¾èµ–å­—å…¸ {package: version}
        task_id: str/int, ä»»åŠ¡ID
        task2maskpacks: dict, ä»»åŠ¡IDåˆ°æ©ç åŒ…åˆ—è¡¨çš„æ˜ å°„
        
    Returns:
        dict: è¿‡æ»¤åçš„ä¾èµ–å­—å…¸
    """
    if task2maskpacks is None or not dependency:
        return dependency
    
    # å°†task_idè½¬æ¢ä¸ºå­—ç¬¦ä¸²ä»¥åŒ¹é…JSONæ–‡ä»¶ä¸­çš„æ ¼å¼
    task_id_str = str(task_id)
    
    # å¦‚æœè¯¥task_idä¸åœ¨æ©ç é…ç½®ä¸­ï¼Œè¿”å›åŸå§‹dependency
    if task_id_str not in task2maskpacks:
        logging.debug(f"Task ID {task_id} ä¸åœ¨æ©ç é…ç½®ä¸­ï¼Œä¿æŒåŸå§‹dependency")
        return dependency
    
    # è·å–è¯¥ä»»åŠ¡åº”è¯¥ä¿ç•™çš„åŒ…åˆ—è¡¨
    mask_packages = task2maskpacks[task_id_str]
    
    # å¦‚æœæ©ç åŒ…åˆ—è¡¨ä¸ºç©ºï¼Œè¿”å›ç©ºçš„dependency
    if not mask_packages:
        logging.info(f"Task ID {task_id} çš„æ©ç åŒ…åˆ—è¡¨ä¸ºç©ºï¼Œè¿”å›ç©ºdependency")
        return {}
    
    # è¿‡æ»¤dependencyï¼Œåªä¿ç•™åœ¨æ©ç åˆ—è¡¨ä¸­çš„åŒ…
    filtered_dependency = {}
    for package, version in dependency.items():
        if package in mask_packages:
            filtered_dependency[package] = version
        else:
            logging.debug(f"Task ID {task_id}: è¿‡æ»¤æ‰åŒ… {package} (ä¸åœ¨æ©ç åˆ—è¡¨ä¸­)")
    
    logging.info(f"Task ID {task_id}: åº”ç”¨æ©ç è¿‡æ»¤ï¼Œ{len(dependency)} -> {len(filtered_dependency)} ä¸ªåŒ…")
    logging.debug(f"Task ID {task_id}: åŸå§‹åŒ…: {list(dependency.keys())}")
    logging.debug(f"Task ID {task_id}: æ©ç åŒ…: {mask_packages}")
    logging.debug(f"Task ID {task_id}: è¿‡æ»¤ååŒ…: {list(filtered_dependency.keys())}")
    
    return filtered_dependency


def versiBCB_lora_merge_pred(args, task="vace", removeDeprecationData=False):
    from omegaconf import OmegaConf
    import fcntl
    
    # è·å–rankå’Œworld_sizeå‚æ•°ï¼Œé»˜è®¤ä¸ºå•workeræ¨¡å¼
    rank = getattr(args, 'rank', 0)
    world_size = getattr(args, 'world_size', 1)
    
    logging.info(f"Worker {rank}/{world_size} starting task: {task}, removeDeprecation: {removeDeprecationData}")
    
    # åŠ è½½task2maskpacksæ©ç æ•°æ®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    task2maskpacks = None
    valid_task_ids = None
    task_id_filter_set = None
    
    if getattr(args, 'enable_task2maskpacks', False):
        task2maskpacks_file = getattr(args, 'task2maskpacks_file', 'data/task2maskpacks.json')
        try:
            with open(task2maskpacks_file, 'r', encoding='utf-8') as f:
                task2maskpacks = json.load(f)
            logging.info(f"âœ… æˆåŠŸåŠ è½½task2maskpacksæ•°æ®: {len(task2maskpacks)} ä¸ªä»»åŠ¡æ©ç é…ç½®")
            
            # å¦‚æœå¯ç”¨äº†only_task2maskpacks_idsé€‰é¡¹ï¼Œæå–æœ‰æ•ˆçš„task idåˆ—è¡¨
            if getattr(args, 'only_task2maskpacks_ids', False):
                # å°†å­—ç¬¦ä¸²task idè½¬æ¢ä¸ºæ•´æ•°ï¼Œç”¨äºåç»­è¿‡æ»¤
                valid_task_ids = set()
                for task_id_str in task2maskpacks.keys():
                    try:
                        task_id_int = int(task_id_str)
                        valid_task_ids.add(task_id_int)
                    except ValueError:
                        logging.warning(f"âš ï¸ è·³è¿‡æ— æ•ˆçš„task id: {task_id_str} (æ— æ³•è½¬æ¢ä¸ºæ•´æ•°)")
                
                logging.info(f"ğŸ” æå–åˆ° {len(valid_task_ids)} ä¸ªæœ‰æ•ˆçš„task idsç”¨äºè¿‡æ»¤")
                logging.debug(f"æœ‰æ•ˆtask idsç¤ºä¾‹: {sorted(list(valid_task_ids))[:10]}")
                
        except Exception as e:
            logging.error(f"âŒ åŠ è½½task2maskpacksæ–‡ä»¶å¤±è´¥ {task2maskpacks_file}: {e}")
            logging.info("ğŸ”„ å°†ç»§ç»­æ‰§è¡Œä½†ä¸åº”ç”¨æ©ç ")
            task2maskpacks = None
            valid_task_ids = None
    else:
        logging.info("ğŸ“ task2maskpacksæ©ç åŠŸèƒ½æœªå¯ç”¨")
    
    # åŠ è½½task idè¿‡æ»¤æ–‡ä»¶ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if getattr(args, 'enable_task_id_filter', False):
        task_id_filter_file = getattr(args, 'task_id_filter_file')
        if task_id_filter_file is None:
            logging.error("âŒ --enable_task_id_filter å·²å¯ç”¨ï¼Œä½† --task_id_filter_file æœªæŒ‡å®š")
            raise ValueError("å¿…é¡»æŒ‡å®š --task_id_filter_file è·¯å¾„")
        
        try:
            with open(task_id_filter_file, 'r', encoding='utf-8') as f:
                task_id_filter_data = json.load(f)
            
            # ç¡®ä¿æ•°æ®æ˜¯åˆ—è¡¨æ ¼å¼
            if isinstance(task_id_filter_data, list):
                task_id_filter_set = set(task_id_filter_data)
            else:
                logging.error(f"âŒ task idè¿‡æ»¤æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼ŒæœŸæœ›åˆ—è¡¨æ ¼å¼ï¼Œå¾—åˆ°: {type(task_id_filter_data)}")
                raise ValueError("task idè¿‡æ»¤æ–‡ä»¶å¿…é¡»æ˜¯æ•´æ•°åˆ—è¡¨æ ¼å¼")
            
            logging.info(f"âœ… æˆåŠŸåŠ è½½task idè¿‡æ»¤æ–‡ä»¶: {task_id_filter_file}")
            logging.info(f"ğŸ” è¿‡æ»¤task idsæ•°é‡: {len(task_id_filter_set)}")
            logging.debug(f"è¿‡æ»¤task idsç¤ºä¾‹: {sorted(list(task_id_filter_set))[:10]}")
            
        except Exception as e:
            logging.error(f"âŒ åŠ è½½task idè¿‡æ»¤æ–‡ä»¶å¤±è´¥ {task_id_filter_file}: {e}")
            raise
    else:
        logging.info("ğŸ“ task idè¿‡æ»¤åŠŸèƒ½æœªå¯ç”¨")
    
    # å¦‚æœæŒ‡å®šäº†IFTå…ƒé…ç½®å¡ç‰‡ï¼Œåˆ™ä½¿ç”¨å…ƒé…ç½®æ¨¡å¼
    if hasattr(args, 'ift_card') and args.ift_card:
        logging.info(f"ğŸš€ ä½¿ç”¨IFTå…ƒé…ç½®æ¨¡å¼: {args.ift_card}")
        
        try:
            # åŠ è½½IFTå…ƒé…ç½®
            prediction_config = load_model_from_ift_meta_config(args.ift_card, args)
            
            # åº”ç”¨IFTé…ç½®åˆ°è®­ç»ƒå‚æ•°
            apply_ift_config_to_lora_training_args(prediction_config, args)
            
            # ä»IFTé…ç½®åŠ è½½lora_configï¼ˆå¦‚æœæ²¡æœ‰é€šè¿‡å…¶ä»–æ–¹å¼æŒ‡å®šï¼‰
            if not hasattr(args, 'lora_config') or args.lora_config is None:
                # ğŸ”§ ä¿®å¤ï¼šé¦–å…ˆåŠ è½½åŸºç¡€é…ç½®æ–‡ä»¶ä½œä¸ºé»˜è®¤é…ç½®
                logging.info("ğŸ“ åŠ è½½åŸºç¡€LoRAé…ç½®æ–‡ä»¶ä½œä¸ºé»˜è®¤é…ç½®")
                base_lora_config = OmegaConf.load(LORA_CONFIG_PATH)
                
                # ç„¶åç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–åŸºç¡€é…ç½®
                base_lora_config.temperature = args.temperature
                base_lora_config.top_p = args.top_p
                base_lora_config.max_dependency_num = args.max_dependency_num
                base_lora_config.append_srcDep = args.append_srcDep
                
                # ç”¨IFTå¡ç‰‡é…ç½®è¦†ç›–åŸºç¡€é…ç½®ä¸­çš„ç›¸åº”å­—æ®µ
                if prediction_config.get("data_sources", {}).get("dataset"):
                    base_lora_config.dataset = prediction_config["data_sources"]["dataset"]
                else:
                    base_lora_config.dataset = "versiBCB"
                    
                # ç¡®ä¿model_nameå­—æ®µå­˜åœ¨
                base_lora_config.model_name = args.model_name
                
                # ç¡®ä¿å…³é”®è·¯å¾„é…ç½®å­˜åœ¨
                if args.loraadaptor_save_path_base:
                    base_lora_config.loraadaptor_save_path_base = args.loraadaptor_save_path_base
                if args.device_map:
                    base_lora_config.device_map = args.device_map
                if args.knowledge_type:
                    base_lora_config.knowledge_type = args.knowledge_type
                if args.precision:
                    base_lora_config.precision = args.precision
                # ä»IFTå¡ç‰‡è·å–prompté…ç½®ï¼ˆå¦‚æœå­˜åœ¨çš„è¯ï¼‰
                prompt_configs = {
                    "versiBCB_vace_prompt": prediction_config.get("data_sources", {}).get("versiBCB_vace_prompt", ""),
                    "versiBCB_vscc_prompt": prediction_config.get("data_sources", {}).get("versiBCB_vscc_prompt", ""),
                    "versiBCB_vscc_bd_prompt": prediction_config.get("data_sources", {}).get("versiBCB_vscc_bd_prompt", ""),
                    "versiBCB_vace_bd_prompt": prediction_config.get("data_sources", {}).get("versiBCB_vace_bd_prompt", ""),
                    "versicode_vscc_prompt": prediction_config.get("data_sources", {}).get("versicode_vscc_prompt", ""),
                    "versicode_vace_prompt": prediction_config.get("data_sources", {}).get("versicode_vace_prompt", "")
                }
                
                # åªæœ‰å½“IFTå¡ç‰‡ä¸­æœ‰å…·ä½“çš„prompté…ç½®æ—¶æ‰è¦†ç›–ï¼Œå¦åˆ™ä¿æŒåŸºç¡€é…ç½®
                for prompt_key, prompt_value in prompt_configs.items():
                    if prompt_value:  # åªæœ‰éç©ºå€¼æ‰è¦†ç›–
                        setattr(base_lora_config, prompt_key, prompt_value)
                
                args.lora_config = base_lora_config
                logging.info("âœ… åŸºç¡€é…ç½®ä¸IFTé…ç½®åˆå¹¶å®Œæˆ")
            
            logging.info("âœ… IFTå…ƒé…ç½®åº”ç”¨å®Œæˆ")
            
        except Exception as e:
            logging.error(f"âŒ åº”ç”¨IFTå…ƒé…ç½®å¤±è´¥: {e}")
            logging.info("ğŸ”„ å›é€€åˆ°æ ‡å‡†é…ç½®æ¨¡å¼")
            # å›é€€åˆ°æ ‡å‡†é…ç½®åŠ è½½
            lora_config = OmegaConf.load(LORA_CONFIG_PATH)
            lora_config.temperature = args.temperature
            lora_config.top_p = args.top_p
            lora_config.max_dependency_num = args.max_dependency_num
            lora_config.append_srcDep = args.append_srcDep
            lora_config.model_name = args.model_name
            args.lora_config = lora_config
    else:
        # æ ‡å‡†é…ç½®æ¨¡å¼
        logging.info("ğŸ“ ä½¿ç”¨æ ‡å‡†LoRAé…ç½®æ¨¡å¼")
        
        # åŠ è½½ lora é…ç½®
        lora_config = OmegaConf.load(LORA_CONFIG_PATH)
        
        # å°†å‘½ä»¤è¡Œå‚æ•°åˆå¹¶åˆ°é…ç½®ä¸­
        lora_config.temperature = args.temperature
        lora_config.top_p = args.top_p
        lora_config.max_dependency_num = args.max_dependency_num
        lora_config.append_srcDep = args.append_srcDep
        lora_config.model_name = args.model_name
        # æ·»åŠ  lora_config åˆ° args ä¸­
        args.lora_config = lora_config
    
    # è½¬æ¢ä¸ºæ™®é€šå­—å…¸ç”¨äºåç»­å¤„ç†
    config = OmegaConf.to_container(args.lora_config, resolve=True)
    
    # å°†æ‰€æœ‰å¿…è¦çš„é…ç½®ä¿¡æ¯æ·»åŠ åˆ°argsä¸­ï¼Œä½¿predictionè¿‡ç¨‹å®Œå…¨ä¾èµ–args
    args.dataset = config.get("dataset", "versiBCB")
    args.task = task
    args.Ban_Deprecation = removeDeprecationData
    args.lora_config_path = LORA_CONFIG_PATH
    
    # å¦‚æœargsä¸­æ²¡æœ‰è®¾ç½®loraadaptor_save_path_baseæˆ–ä¸ºNoneï¼Œåˆ™ä½¿ç”¨configä¸­çš„å€¼
    if not hasattr(args, 'loraadaptor_save_path_base') or args.loraadaptor_save_path_base is None:
        args.loraadaptor_save_path_base = config.get("loraadaptor_save_path_base", "/datanfs2/chenrongyi/models/loraadaptors/docstring/")
    
    if not hasattr(args, 'device_map') or args.device_map is None:
        args.device_map = config.get("device_map", "auto")
    
    # å¦‚æœå‘½ä»¤è¡Œæ²¡æœ‰æŒ‡å®šknowledge_typeï¼Œåˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼
    if not hasattr(args, 'knowledge_type') or args.knowledge_type is None:
        args.knowledge_type = config.get("knowledge_type", "doc")
    
    # æ·»åŠ promptä¿¡æ¯åˆ°args
    args.versiBCB_vace_prompt = config.get("versiBCB_vace_prompt", "")
    args.versiBCB_vscc_prompt = config.get("versiBCB_vscc_prompt", "")
    args.versiBCB_vscc_bd_prompt = config.get("versiBCB_vscc_bd_prompt", "")
    args.versiBCB_vace_bd_prompt = config.get("versiBCB_vace_bd_prompt", "")
    args.versicode_vscc_prompt = config.get("versicode_vscc_prompt", "")
    args.versicode_vace_prompt = config.get("versicode_vace_prompt", "")
    
    # æå–LoRA card infoç”¨äºæ–‡ä»¶å
    lora_cardinfo = {
        'r': config.get('r', 64),
        'alpha': config.get('alpha', 128),
        'lr': config.get('learning_rate', 1e-6),
        'bs': config.get('batch_size', 8),
        'epochs': config.get('num_epochs', 4)
    }
    
    # æ ¹æ®args.taskç¡®å®šè¦åŠ è½½çš„æ•°æ®ï¼ˆè€Œä¸æ˜¯ä»IFTå¡ç‰‡ä¸­è·å–ï¼‰
    benchmark = "Versicode_Benchmark" if args.dataset == "versicode" else "VersiBCB_Benchmark"
    filename = 'vscc_datas' if task == "vscc" else 'vace_datas'
    filename = filename + "_for_warning" if removeDeprecationData else filename
    
    if args.specified_bench_path is None:
        with open(f"data/{benchmark}/{filename}.json", "r") as f:
            datas = json.load(f)
    else:
        with open(args.specified_bench_path, "r") as f:
            datas = json.load(f)
    
    logging.info(f"ğŸ“Š åŠ è½½æ•°æ®é›†: {benchmark}/{filename}.json")
    logging.info(f"ğŸ“ˆ æ•°æ®æ¡ç›®æ•°é‡: {len(datas)}")
    
    # åˆ›å»ºè¾“å‡ºç®¡ç†å™¨
    output_manager = OutputManager(args.base_output_dir)
    
    # ç”ŸæˆåŸºç¡€æ–‡ä»¶åï¼ˆåŒ…å«IFTä¿¡æ¯å¦‚æœä½¿ç”¨äº†IFTé…ç½®ï¼‰
    dataset_name = f"{args.dataset}_{task}{'_BD' if removeDeprecationData else ''}"
    model_name = args.model_name.split('/')[-1]
    
    # å¦‚æœä½¿ç”¨äº†IFTé…ç½®ï¼Œåœ¨æ–‡ä»¶åä¸­ä½“ç°
    approach_name = "LoRA"
    if hasattr(args, 'ift_card_config') and args.ift_card_config:
        ift_type = args.ift_card_config.get('ift_type', 'default')
        data_strategy = args.ift_card_config.get('data_strategy', 'default')
        approach_name = f"LoRA_IFT_{ift_type}_{data_strategy}"
        logging.info(f"ğŸ·ï¸ ä½¿ç”¨IFTå¢å¼ºæ–¹æ³•: {approach_name}")
    
    base_filename = output_manager.generate_base_filename(
        dataset=dataset_name,
        model_name=model_name,
        approach=approach_name,
        corpus_type=args.knowledge_type,
        max_dependency_num=args.max_dependency_num,
        append_srcDep=args.append_srcDep,
        lora_cardinfo=lora_cardinfo
    )
    
    # è·å–è¾“å‡ºè·¯å¾„å’Œé…ç½®è·¯å¾„
    output_path, config_path = output_manager.get_output_path_and_config(
        approach="LoRA",
        base_filename=base_filename,
        model_name_or_path=args.model_name,
        newWhenExist=args.newWhenExist
    )
    if args.specified_output_path is not None:
        output_path = args.specified_output_path
    if args.specified_config_path is not None:
        config_path = args.specified_config_path
    
    # å¤„ç†å·²å­˜åœ¨ç»“æœçš„æƒ…å†µ - æ”¹è¿›çš„é€»è¾‘
    processed_ids = set()
    
    # æ­¥éª¤1ï¼šåŠ è½½å·²æœ‰ç»“æœ
    if os.path.exists(output_path) and not args.overwrite:
        logging.info(f"Found existing output file: {output_path}")
        logging.info("Loading existing results to avoid duplicates...")
        
        try:
            with open(output_path, "r", encoding="utf-8") as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if line:
                        try:
                            result = json.loads(line)
                            if "id" in result:
                                processed_ids.add(result["id"])
                        except json.JSONDecodeError as e:
                            logging.warning(f"Warning: Invalid JSON on line {line_num}: {e}")
                            continue
            
            logging.info(f"Found {len(processed_ids)} already processed IDs")
            
        except Exception as e:
            logging.error(f"Error loading existing results: {e}")
            processed_ids = set()
    
    elif args.overwrite and os.path.exists(output_path):
        logging.info(f"Worker {rank}: Overwriting existing file: {output_path}")
        # åªæœ‰rank 0 è´Ÿè´£æ¸…ç©ºæ–‡ä»¶ï¼Œå…¶ä»–workerç­‰å¾…
        if rank == 0:
            open(output_path, 'w').close()
            logging.info(f"Worker {rank}: File cleared")
        else:
            # å…¶ä»–workerç­‰å¾…ä¸€ä¸‹ï¼Œç¡®ä¿æ–‡ä»¶è¢«æ¸…ç©º
            import time
            time.sleep(2)
            logging.info(f"Worker {rank}: Waiting for file to be cleared...")
    
    # æ­¥éª¤2ï¼šè¿‡æ»¤å‡ºéœ€è¦å¤„ç†çš„æ•°æ®ï¼ˆåœ¨VSCCèŒƒå›´å†…ä¸”æœªè¢«å¤„ç†ï¼‰
    unprocessed_data = []
    skipped_by_task2maskpacks_filter = 0
    skipped_by_task_id_filter = 0
    
    for i, data in enumerate(datas):
        if VSCC_LOW_BOUND <= i <= VSCC_HIGH_BOUND:
            if data["id"] not in processed_ids:
                # å¦‚æœå¯ç”¨äº†only_task2maskpacks_idsé€‰é¡¹ï¼Œæ£€æŸ¥è¯¥task idæ˜¯å¦åœ¨æœ‰æ•ˆåˆ—è¡¨ä¸­
                if valid_task_ids is not None:
                    task_id = data.get("id")
                    if task_id not in valid_task_ids:
                        logging.debug(f"æ„å»ºé˜¶æ®µè·³è¿‡Task ID {task_id}: ä¸åœ¨task2maskpacksæ–‡ä»¶ä¸­")
                        skipped_by_task2maskpacks_filter += 1
                        continue
                
                # å¦‚æœå¯ç”¨äº†task idè¿‡æ»¤ï¼Œæ£€æŸ¥è¯¥task idæ˜¯å¦åœ¨è¿‡æ»¤åˆ—è¡¨ä¸­
                if task_id_filter_set is not None:
                    task_id = data.get("id")
                    if task_id not in task_id_filter_set:
                        logging.debug(f"æ„å»ºé˜¶æ®µè·³è¿‡Task ID {task_id}: ä¸åœ¨task idè¿‡æ»¤æ–‡ä»¶ä¸­")
                        skipped_by_task_id_filter += 1
                        continue
                
                unprocessed_data.append((i, data))

    logging.info(f"Total data in range [{VSCC_LOW_BOUND}, {VSCC_HIGH_BOUND}]: {VSCC_HIGH_BOUND - VSCC_LOW_BOUND + 1}")
    logging.info(f"Already processed: {len(processed_ids)}")
    if valid_task_ids is not None:
        logging.info(f"Skipped by task2maskpacks filter: {skipped_by_task2maskpacks_filter}")
        logging.info(f"Valid task2maskpacks IDs count: {len(valid_task_ids)}")
    if task_id_filter_set is not None:
        logging.info(f"Skipped by task id filter: {skipped_by_task_id_filter}")
        logging.info(f"Task id filter count: {len(task_id_filter_set)}")
    logging.info(f"Remaining unprocessed: {len(unprocessed_data)}")
    
    # æ­¥éª¤3ï¼šå¯¹æœªå¤„ç†çš„æ•°æ®è¿›è¡Œå‡åŒ€åˆ†é…
    if world_size > 1 and len(unprocessed_data) > 0:
        # æŒ‰rankåˆ†ç‰‡æœªå¤„ç†çš„æ•°æ®
        total_unprocessed = len(unprocessed_data)
        samples_per_worker = total_unprocessed // world_size
        remainder = total_unprocessed % world_size
        
        # è®¡ç®—å½“å‰workerçš„æ•°æ®èŒƒå›´
        start_idx = rank * samples_per_worker + min(rank, remainder)
        end_idx = start_idx + samples_per_worker + (1 if rank < remainder else 0)
        
        worker_data = unprocessed_data[start_idx:end_idx]
        logging.info(f"Worker {rank} processing {len(worker_data)} unprocessed samples (global indices {start_idx}-{end_idx-1} of unprocessed data)")
        
        if len(worker_data) == 0:
            logging.info(f"Worker {rank} has no data to process, exiting...")
            return 0, 0
    else:
        # å•workeræ¨¡å¼æˆ–æ²¡æœ‰æœªå¤„ç†æ•°æ®
        worker_data = unprocessed_data
        if world_size == 1:
            logging.info(f"Single worker processing {len(worker_data)} unprocessed samples")
        else:
            logging.info(f"No unprocessed data remaining for workers to handle")
            
        if len(worker_data) == 0:
            logging.info(f"No unprocessed data, task completed")
            return 0, 0
    
    # è¾…åŠ©å‡½æ•°ï¼šå®‰å…¨åœ°è¿½åŠ åˆ°æ–‡ä»¶
    def safe_append_to_file(file_path, data):
        """ä½¿ç”¨æ–‡ä»¶é”å®‰å…¨åœ°è¿½åŠ æ•°æ®åˆ°æ–‡ä»¶"""
        try:
            with open(file_path, "a", encoding="utf-8") as f:
                # è·å–æ–‡ä»¶é”
                fcntl.flock(f.fileno(), fcntl.LOCK_EX)
                try:
                    json.dump(data, f, ensure_ascii=False)
                    f.write('\n')
                    f.flush()  # ç¡®ä¿æ•°æ®å†™å…¥ç£ç›˜
                finally:
                    # é‡Šæ”¾æ–‡ä»¶é”
                    fcntl.flock(f.fileno(), fcntl.LOCK_UN)
            return True
        except Exception as e:
            logging.error(f"Worker {rank}: Error writing to file: {e}")
            return False
    
    def append2jsonl(file_path,data):
        try:
            with open(file_path,'a+',encoding='utf-8') as f:
                json.dump(data,f,ensure_ascii=False)
                f.write('\n')
                f.flush()
            logging.info(f"successful write to {file_path}")
            return True
            
        except Exception as e:
            logging.error(f"Worker {rank}: Error writing to file: {e}")
            return False
        
    # å¤„ç†åˆ†é…ç»™å½“å‰workerçš„æ•°æ®
    processed_count = 0
    lora_pred_result = []
    
    for original_idx, data in tqdm(worker_data, desc=f"Worker {rank}"):
        if args.dataset == "versicode":
            pack = data["dependency"]
            version = get_version(data["version"]) if args.task == "vscc" else get_version(data["target_version"])
            dependency = {pack:version}
            if not getDataExistence(dependency):
                continue
            
            # åº”ç”¨task2maskpacksæ©ç è¿‡æ»¤
            if task2maskpacks is not None:
                original_dependency = dependency.copy()
                dependency = apply_task2maskpacks_filter(dependency, data["id"], task2maskpacks)
                
                # è®°å½•æ©ç åº”ç”¨æƒ…å†µ
                if len(dependency) != len(original_dependency):
                    logging.info(f"Task ID {data['id']}: æ©ç è¿‡æ»¤ç”Ÿæ•ˆï¼Œ{len(original_dependency)} -> {len(dependency)} ä¸ªåŒ…")
        elif args.dataset == "versiBCB":
            dependency = data["target_dependency"] if task == "vace" else data["dependency"]
            src_dependency = data["origin_dependency"] if task == "vace" else data["dependency"]
            from utils.getDependencyUtils import getSubsetDep,combineDep
            if args.max_dependency_num is not None:
                dependency = getSubsetDep(dependency, args.max_dependency_num)
                src_dependency = getSubsetDep(src_dependency, args.max_dependency_num)
            if args.append_srcDep:
                if task=='vscc':
                    raise ValueError("vsccä»»åŠ¡ä¸æ”¯æŒæ·»åŠ æºä¾èµ–")
                dependency = combineDep(dependency,src_dependency)
            
            # åº”ç”¨task2maskpacksæ©ç è¿‡æ»¤
            if task2maskpacks is not None:
                original_dependency = dependency.copy()
                dependency = apply_task2maskpacks_filter(dependency, data["id"], task2maskpacks)
                
                # è®°å½•æ©ç åº”ç”¨æƒ…å†µ
                if len(dependency) != len(original_dependency):
                    logging.info(f"Task ID {data['id']}: æ©ç è¿‡æ»¤ç”Ÿæ•ˆï¼Œ{len(original_dependency)} -> {len(dependency)} ä¸ªåŒ…")
        else:
            raise ValueError(f"æ•°æ®é›†ä¸å­˜åœ¨: {args.dataset}")
            
        try:
            lora_pred = get_lora_pred_with_cleanup(data, dependency, args)
            processed_count += 1
            
            # å®‰å…¨åœ°è¿½åŠ åˆ°å…±äº«æ–‡ä»¶
            if append2jsonl(output_path, lora_pred):
                # åŠ å…¥åˆ°å†…å­˜åˆ—è¡¨ç”¨äºç»Ÿè®¡
                lora_pred_result.append(lora_pred)
                # æ›´æ–°å·²å¤„ç†IDé›†åˆ
                processed_ids.add(data["id"])
            else:
                logging.error(f"Worker {rank}: Failed to write result for ID {data['id']}")
                
            # æ¯å¤„ç†10ä¸ªæ ·æœ¬æŠ¥å‘Šä¸€æ¬¡è¿›åº¦
            if processed_count % 10 == 0:
                logging.info(f"Worker {rank}: Processed {processed_count}/{len(worker_data)} samples")
                
        except Exception as e:
            logging.error(f"Worker {rank}: å¤„ç† ID {data['id']} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            traceback.print_exc()
            error_result = {
                "id": data["id"], 
                "answer": "",
                "adaptor_info": {
                    "attempted_packages": [],
                    "successful_packages": [],
                    "failed_packages": [],
                    "total_attempted": 0,
                    "total_successful": 0,
                    "total_failed": 0,
                    "error": f"Processing error: {str(e)}"
                },
                "merged_adaptor_count": 0
            }
            processed_count += 1
            continue
    
    # è®¡ç®—å½“å‰workerçš„adaptorä½¿ç”¨ç»Ÿè®¡
    total_samples = len(lora_pred_result)
    samples_with_adaptors = 0
    total_successful_adaptors = 0
    total_attempted_adaptors = 0
    samples_with_full_success = 0
    
    for result in lora_pred_result:
        if "adaptor_info" in result:
            adaptor_info = result["adaptor_info"]
            total_attempted_adaptors += adaptor_info.get("total_attempted", 0)
            total_successful_adaptors += adaptor_info.get("total_successful", 0)
            if adaptor_info.get("total_successful", 0) > 0:
                samples_with_adaptors += 1
            if (adaptor_info.get("total_attempted", 0) > 0 and 
                adaptor_info.get("total_successful", 0) == adaptor_info.get("total_attempted", 0)):
                samples_with_full_success += 1
    
    logging.info(f"\n=== Worker {rank} LoRA Adaptor Usage Statistics ===")
    logging.info(f"Worker {rank} processed samples: {total_samples}")
    logging.info(f"Samples with at least 1 adaptor loaded: {samples_with_adaptors}/{total_samples}")
    logging.info(f"Samples with all adaptors loaded successfully: {samples_with_full_success}/{total_samples}")
    logging.info(f"Total adaptors attempted: {total_attempted_adaptors}")
    logging.info(f"Total adaptors loaded successfully: {total_successful_adaptors}/{total_attempted_adaptors}")
    if total_attempted_adaptors > 0:
        success_rate = (total_successful_adaptors / total_attempted_adaptors) * 100
        logging.info(f"Adaptor loading success rate: {success_rate:.2f}%")
    logging.info(f"===============================================")
    
    # åªæœ‰rank 0è´Ÿè´£ä¿å­˜é…ç½®æ–‡ä»¶ï¼Œé¿å…å¹¶å‘å†™å…¥å†²çª
    if rank == 0:
        logging.info(f"Worker {rank}: Saving configuration file...")
        try:
            # ç”Ÿæˆå¹¶ä¿å­˜é…ç½®æ–‡ä»¶
            config_data = output_manager.generate_config(
                approach="LoRA",
                args=args
            )
            # æ·»åŠ å¤šworkerä¿¡æ¯åˆ°é…ç½®ä¸­
            if world_size > 1:
                config_data["experiment_info"]["multi_worker"] = {
                    "world_size": world_size,
                    "data_parallel": True,
                    "unprocessed_data_distribution": True
                }
            output_manager.save_config(config_path, config_data)
            logging.info(f"Worker {rank}: Config saved to: {config_path}")
        except Exception as e:
            logging.error(f"Worker {rank}: Error saving config: {e}")
    else:
        logging.info(f"Worker {rank}: Skipping config save (handled by rank 0)")
    
    logging.info(f"Worker {rank}: Results appended to: {output_path}")
    logging.info(f"Worker {rank}: Task {task} completed with {processed_count} processed samples")
    
    # è¿”å›å¤„ç†çš„æ ·æœ¬æ•°é‡ï¼Œç”¨äºåç»­ç»Ÿè®¡
    return processed_count, 0  # ç¬¬äºŒä¸ªè¿”å›å€¼æ”¹ä¸º0ï¼Œå› ä¸ºæ²¡æœ‰è·³è¿‡æ ·æœ¬çš„æ¦‚å¿µäº†


if __name__ == "__main__":
    # trainLoraModels()
    # trainNormalLoraModels()
    
    # å¯ä»¥ç›´æ¥è°ƒç”¨æ— å‚å‡½æ•°ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
    # loadModelandPredict()
    args = argparse.ArgumentParser()
    args.add_argument("--max_dependency_num", type=int, default=10,help="æœ€å¤§ä¾èµ–æ•°é‡")
    args.add_argument("--append_srcDep", action="store_true", help="æ˜¯å¦æ·»åŠ æºä¾èµ–")
    args.add_argument("--temperature", type=float, default=0.2, help="Temperature for sampling")
    args.add_argument("--top_p", type=float, default=0.95, help="Top-p for nucleus sampling")
    args.add_argument("--precision", type=str, default="fp16", help="Precision for model prediction")
    args.add_argument("--loraadaptor_save_path_base", type=str, default="/datanfs2/chenrongyi/models/loraadaptors/", help="adaptor_base_path")
    args.add_argument("--tempmerged_adaptor_path", type=str, default=None, help="å­˜æ”¾ä¸´æ—¶mergeçš„adaptorç‚¹")
    args.add_argument("--model_name", type=str, default="/datanfs2/chenrongyi/hf/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/3ad372fc79158a2148299e3318516c786aeded6c", help="model_name")
    args.add_argument("--knowledge_type", type=str, default="docstring", help="knowledge_type,overrideå¯¹åº”çš„conf") # å¯¹åº”åå­—ä¸­éœ€åŒ…å«å¯¹åº”çš„knowledge_typeæ‰ä¼šæˆåŠŸåŠ è½½
    args.add_argument("--newWhenExist", action="store_true", help="å½“åŒåè¾“å‡ºæ–‡ä»¶å­˜åœ¨æ—¶ï¼Œæ˜¯å¦åˆ›å»ºæ–°æ–‡ä»¶")
    args.add_argument("--overwrite", action="store_true", help="å½“åŒåè¾“å‡ºæ–‡ä»¶å­˜åœ¨æ—¶ï¼Œæ˜¯å¦æ¸…ç©ºï¼Œè¿˜æ˜¯loadåŸæœ‰ç»“æœå¹¶ç»§ç»­æœªè¿›è¡Œçš„æ¨ç†")
    args.add_argument("--rank", type=int, default=0, help="rank")
    args.add_argument("--world_size", type=int, default=1, help="world_size")
    args.add_argument("--task", type=str, default="vace", help="task")
    args.add_argument("--specified_bench_path", type=str, default=None, help="æŒ‡å®šè¦é¢„æµ‹çš„benchmarkè·¯å¾„")
    args.add_argument("--Ban_Deprecation", action="store_true", help="æ˜¯å¦ç¦ç”¨deprecation")
    args.add_argument("--force_gpu", action="store_true", help="æ˜¯å¦å¼ºåˆ¶ä½¿ç”¨GPU")
    args.add_argument("--base_output_dir", type=str, default="output/approach_eval", help="Base output directory")
    args.add_argument("--adopt_IFT_checkpoint", action="store_true", help="æ˜¯å¦ä½¿ç”¨å¯¹åº”ç›®å½•ä¸‹IFTçš„checkpoint")
    args.add_argument("--ift_enabled_packages", type=str, nargs='+', default=None,
                     help="æŒ‡å®šå¯ç”¨IFTçš„åŒ…ååˆ—è¡¨ï¼Œåªæœ‰åˆ—è¡¨ä¸­çš„åŒ…ä¼šä½¿ç”¨IFT checkpointï¼Œå…¶ä»–åŒ…ä½¿ç”¨æ™®é€šLoRAæ¨¡å‹ã€‚ä¾‹å¦‚ï¼š--ift_enabled_packages matplotlib numpy pandas")
    args.add_argument("--ift_data_strategy", type=str, default=None, 
                     choices=["same_minor_version", "all_versions", "closest_n"],
                     help="åå¥½çš„IFTæ•°æ®ç­–ç•¥")
    args.add_argument("--ift_type", type=str, default=None,
                     help="åå¥½çš„IFTç±»å‹æ ‡è¯†")
    args.add_argument("--list_ift_models", action="store_true", help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„IFTæ¨¡å‹ç„¶åé€€å‡º")
    args.add_argument("--ift_model_id", type=str, default=None, help="ç›´æ¥æŒ‡å®šIFTæ¨¡å‹ID")
    args.add_argument("--ift_card", type=str, default=None, help="æŒ‡å®šIFTé…ç½®å¡ç‰‡æ–‡ä»¶è·¯å¾„")
    args.add_argument("--list_ift_cards", action="store_true", help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„IFTé…ç½®å¡ç‰‡ç„¶åé€€å‡º")
    args.add_argument("--enable_task2maskpacks", action="store_true", help="å¯ç”¨task2maskpacksæ©ç åŠŸèƒ½ï¼Œæ ¹æ®taskidæ©ç å¯¹åº”dependencyçš„pack")
    args.add_argument("--task2maskpacks_file", type=str, default="data/task2maskpacks.json", help="task2maskpacksæ©ç æ–‡ä»¶è·¯å¾„")
    args.add_argument("--only_task2maskpacks_ids", action="store_true", help="ä»…å¤„ç†task2maskpacksæ–‡ä»¶ä¸­å®šä¹‰çš„task ids")
    args.add_argument("--enable_task_id_filter", action="store_true", help="å¯ç”¨task idè¿‡æ»¤åŠŸèƒ½ï¼Œä»…å¤„ç†æŒ‡å®šæ–‡ä»¶ä¸­çš„task ids")
    args.add_argument("--task_id_filter_file", type=str, default=None, help="task idè¿‡æ»¤æ–‡ä»¶è·¯å¾„ï¼Œåº”ä¸ºåŒ…å«æ•´æ•°åˆ—è¡¨çš„JSONæ–‡ä»¶")
    # è‡ªå®šä¹‰è¾“å‡ºæ–‡ä»¶
    args.add_argument("--specified_output_path", type=str, default=None, help="æŒ‡å®šè¾“å‡ºæ–‡ä»¶è·¯å¾„")
    args.add_argument("--specified_config_path", type=str, default=None, help="æŒ‡å®šé…ç½®æ–‡ä»¶è·¯å¾„")
    args = args.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    log_dir = setup_logging(args)
    
    # æ–°å¢ï¼štask2maskpacksåŠŸèƒ½çŠ¶æ€æ—¥å¿—
    if getattr(args, 'enable_task2maskpacks', False):
        logging.info("=== Task2MaskPacks æ©ç åŠŸèƒ½å·²å¯ç”¨ ===")
        logging.info(f"ğŸ¯ æ©ç æ–‡ä»¶è·¯å¾„: {getattr(args, 'task2maskpacks_file', 'data/task2maskpacks.json')}")
        logging.info("ğŸ“ å°†æ ¹æ®taskidæ©ç å¯¹åº”dependencyçš„pack")
        if getattr(args, 'only_task2maskpacks_ids', False):
            logging.info("ğŸ” ä»…å¤„ç†task2maskpacksæ–‡ä»¶ä¸­å®šä¹‰çš„task ids")
    else:
        logging.info("=== Task2MaskPacks æ©ç åŠŸèƒ½æœªå¯ç”¨ ===")
        logging.info("ğŸ“ å°†ä½¿ç”¨åŸå§‹dependencyé…ç½®")
        if getattr(args, 'only_task2maskpacks_ids', False):
            logging.warning("âš ï¸ --only_task2maskpacks_ids å·²è®¾ç½®ï¼Œä½† --enable_task2maskpacks æœªå¯ç”¨ï¼Œè¯¥é€‰é¡¹å°†è¢«å¿½ç•¥")
    
    # æ–°å¢ï¼štask idè¿‡æ»¤åŠŸèƒ½çŠ¶æ€æ—¥å¿—
    if getattr(args, 'enable_task_id_filter', False):
        logging.info("=== Task ID è¿‡æ»¤åŠŸèƒ½å·²å¯ç”¨ ===")
        logging.info(f"ğŸ¯ è¿‡æ»¤æ–‡ä»¶è·¯å¾„: {getattr(args, 'task_id_filter_file', 'æœªæŒ‡å®š')}")
        logging.info("ğŸ“ å°†ä»…å¤„ç†è¿‡æ»¤æ–‡ä»¶ä¸­æŒ‡å®šçš„task ids")
    else:
        logging.info("=== Task ID è¿‡æ»¤åŠŸèƒ½æœªå¯ç”¨ ===")
    
    # ğŸ¯ æ–°å¢ï¼šåŒ…çº§åˆ«IFTæ§åˆ¶çš„æ—¥å¿—è®°å½•å’ŒéªŒè¯
    if hasattr(args, 'ift_enabled_packages') and args.ift_enabled_packages:
        logging.info("=== åŒ…çº§åˆ«IFTæ§åˆ¶å·²å¯ç”¨ ===")
        logging.info(f"ğŸ“¦ æŒ‡å®šçš„IFTå¯ç”¨åŒ…åˆ—è¡¨: {args.ift_enabled_packages}")
        logging.info(f"ğŸ’¡ åªæœ‰ä»¥ä¸‹åŒ…ä¼šå°è¯•åŠ è½½IFT checkpoint:")
        for pkg in args.ift_enabled_packages:
            logging.info(f"   - {pkg}")
        logging.info(f"ğŸ“ å…¶ä»–åŒ…å°†ç›´æ¥ä½¿ç”¨æ™®é€šLoRAæ¨¡å‹")
        
        # å¦‚æœåŒæ—¶è®¾ç½®äº†adopt_IFT_checkpointï¼Œç»™å‡ºæç¤º
        if args.adopt_IFT_checkpoint:
            logging.info(f"âš ï¸ æ³¨æ„: åŒæ—¶è®¾ç½®äº†--adopt_IFT_checkpointå’Œ--ift_enabled_packages")
            logging.info(f"   å®é™…è¡Œä¸º: æŒ‰åŒ…çº§åˆ«æ§åˆ¶ï¼Œåªå¯¹æŒ‡å®šåŒ…å¯ç”¨IFT")
    elif args.adopt_IFT_checkpoint:
        logging.info("=== å…¨å±€IFTæ¨¡å¼å·²å¯ç”¨ ===")
        logging.info("ğŸš€ æ‰€æœ‰åŒ…éƒ½å°†å°è¯•åŠ è½½IFT checkpoint")
    else:
        logging.info("=== æ ‡å‡†LoRAæ¨¡å¼ ===")
        logging.info("ğŸ“ æ‰€æœ‰åŒ…éƒ½å°†ä½¿ç”¨æ™®é€šLoRAæ¨¡å‹")
    
    # å¦‚æœè¯·æ±‚åˆ—å‡ºIFTæ¨¡å‹ï¼Œåˆ™æ‰§è¡Œå¹¶é€€å‡º
    if args.list_ift_models:
        ift_manager = get_ift_manager()
        models = ift_manager.list_models()
        
        if not models:
            print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ³¨å†Œçš„IFTæ¨¡å‹")
        else:
            print(f"æ‰¾åˆ° {len(models)} ä¸ªå·²æ³¨å†Œçš„IFTæ¨¡å‹:")
            print()
            for model in models:
                print(f"æ¨¡å‹ID: {model['model_id']}")
                print(f"  åŒ…-ç‰ˆæœ¬: {model['pkg']}-{model['version']}")
                print(f"  åŸºç¡€æ¨¡å‹: {model['base_model'].split('/')[-1]}")
                print(f"  çŸ¥è¯†ç±»å‹: {model['knowledge_type']}")
                print(f"  æ•°æ®ç­–ç•¥: {model['data_strategy']}")
                print(f"  IFTç±»å‹: {model['ift_type'] or 'default'}")
                print(f"  æ¨¡å‹è·¯å¾„: {model['ift_model_path']}")
                print(f"  å­˜åœ¨æ€§: {'âœ“' if model.get('model_exists', False) else 'âœ—'}")
                print(f"  æ³¨å†Œæ—¶é—´: {model['registered_at']}")
                print("-" * 60)
        print("é€€å‡ºç¨‹åº")
        sys.exit(0)
    
    # å¦‚æœè¯·æ±‚åˆ—å‡ºIFTé…ç½®å¡ç‰‡ï¼Œåˆ™æ‰§è¡Œå¹¶é€€å‡º
    if args.list_ift_cards:
        card_manager = get_default_card_manager()
        cards = card_manager.list_ift_cards()
        
        if not cards:
            print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•IFTé…ç½®å¡ç‰‡")
        else:
            print(f"æ‰¾åˆ° {len(cards)} ä¸ªIFTé…ç½®å¡ç‰‡:")
            print()
            for card in cards:
                print(f"å¡ç‰‡ID: {card['card_id']}")
                print(f"  æ–‡ä»¶å: {card['filename']}")
                print(f"  åŒ…-ç‰ˆæœ¬: {card['pkg']}-{card['version']}")
                print(f"  IFTç±»å‹: {card['ift_type']}")
                print(f"  æ•°æ®ç­–ç•¥: {card['data_strategy']}")
                print(f"  åˆ›å»ºæ—¶é—´: {card['created_at']}")
                print(f"  æè¿°: {card['description']}")
                print(f"  ä½¿ç”¨æ–¹å¼: python benchmark/pred_lora.py --ift_card {card['filename']}")
                print("-" * 80)
        print("é€€å‡ºç¨‹åº")
        sys.exit(0)
    
    # å¦‚æœæŒ‡å®šäº†IFTé…ç½®å¡ç‰‡ï¼Œåˆ™åŠ è½½å¹¶åº”ç”¨é…ç½®
    if args.ift_card:
        try:
            card_manager = get_default_card_manager()
            ift_card = card_manager.load_ift_card(args.ift_card)
            
            # éªŒè¯é…ç½®å¡ç‰‡
            if not card_manager.validate_card(ift_card):
                logging.error(f"IFTé…ç½®å¡ç‰‡æ— æ•ˆ: {args.ift_card}")
                sys.exit(1)
            
            # ä»é…ç½®å¡ç‰‡ä¸­æå–é…ç½®
            prediction_config = card_manager.extract_prediction_config(ift_card)
            
            # åº”ç”¨é…ç½®å¡ç‰‡ä¸­çš„è®¾ç½®åˆ°args
            args.model_name = prediction_config["model_name"]
            args.knowledge_type = prediction_config["knowledge_type"]
            args.precision = prediction_config["precision"]
            args.adopt_IFT_checkpoint = True  # å¼ºåˆ¶å¯ç”¨IFTæ¨¡å¼
            args.ift_type = prediction_config["ift_type"]
            args.ift_data_strategy = prediction_config["data_strategy"]
            
            # å¦‚æœæ²¡æœ‰æŒ‡å®šloraadaptor_save_path_baseï¼Œä»å¡ç‰‡ä¸­æ¨æ–­
            if not hasattr(args, 'loraadaptor_save_path_base') or args.loraadaptor_save_path_base is None:
                # ä»base_lora_pathæ¨æ–­åŸºç¡€è·¯å¾„
                base_lora_path = prediction_config["base_lora_path"]
                # å‡è®¾è·¯å¾„ç»“æ„æ˜¯ .../base_path/pkg/version/model_name/knowledge_type/
                import os
                parts = base_lora_path.split(os.sep)
                if len(parts) >= 4:
                    # æ‰¾åˆ°knowledge_typeä¹‹å‰çš„è·¯å¾„ä½œä¸ºåŸºç¡€è·¯å¾„
                    knowledge_type = prediction_config["knowledge_type"]
                    try:
                        if knowledge_type in parts:
                            kt_index = parts.index(knowledge_type)
                            base_path = os.sep.join(parts[:kt_index])
                            args.loraadaptor_save_path_base = base_path
                            logging.info(f"ä»IFTé…ç½®å¡ç‰‡æ¨æ–­åŸºç¡€è·¯å¾„: {base_path}")
                    except:
                        # å¦‚æœæ¨æ–­å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„
                        pass
            
            logging.info(f"âœ… å·²åŠ è½½IFTé…ç½®å¡ç‰‡: {args.ift_card}")
            logging.info(f"ğŸ¯ ç›®æ ‡åŒ…-ç‰ˆæœ¬: {prediction_config['pkg']}-{prediction_config['version']}")
            logging.info(f"ğŸ“‹ IFTç±»å‹: {prediction_config['ift_type']}")
            logging.info(f"ğŸ“Š æ•°æ®ç­–ç•¥: {prediction_config['data_strategy']}")
            logging.info(f"ğŸ·ï¸ å¡ç‰‡ID: {prediction_config['card_id']}")
            
        except Exception as e:
            logging.error(f"åŠ è½½IFTé…ç½®å¡ç‰‡å¤±è´¥: {e}")
            sys.exit(1)
    
    # è¯¦ç»†çš„CUDAç¯å¢ƒæ£€æµ‹å’Œè¯Šæ–­
    logging.info("=== CUDAç¯å¢ƒè¯Šæ–­ ===")
    cuda_devices = os.environ.get('CUDA_VISIBLE_DEVICES', 'Not set')
    logging.info(f"CUDA_VISIBLE_DEVICES={cuda_devices}")
    
    # æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
    logging.info(f"torch.cuda.is_available(): {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        visible_gpu_count = torch.cuda.device_count()
        logging.info(f"PyTorchå¯è§GPUæ•°é‡: {visible_gpu_count}")
        
        if visible_gpu_count > 0:
            for i in range(visible_gpu_count):
                gpu_name = torch.cuda.get_device_properties(i).name
                total_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)
                logging.info(f"GPU {i}: {gpu_name} ({total_memory:.1f}GB)")
        else:
            logging.warning("PyTorchæ£€æµ‹åˆ°CUDAå¯ç”¨ä½†GPUæ•°é‡ä¸º0")
    else:
        logging.warning("PyTorchæ£€æµ‹CUDAä¸å¯ç”¨")
        
        # å¦‚æœè®¾ç½®äº†CUDA_VISIBLE_DEVICESä½†CUDAä¸å¯ç”¨ï¼Œç»™å‡ºè¯¦ç»†è¯Šæ–­
        if cuda_devices != 'Not set':
            logging.error(f"ç¯å¢ƒå˜é‡CUDA_VISIBLE_DEVICES={cuda_devices}ï¼Œä½†PyTorchæ— æ³•ä½¿ç”¨CUDA")
            logging.error("å¯èƒ½çš„åŸå› ï¼š")
            logging.error("1. PyTorchæ²¡æœ‰CUDAæ”¯æŒ - æ£€æŸ¥ï¼špython -c 'import torch; print(torch.version.cuda)'")
            logging.error("2. CUDAé©±åŠ¨/è¿è¡Œæ—¶ç‰ˆæœ¬ä¸åŒ¹é…")
            logging.error("3. GPUè®¾å¤‡ç¼–å·ä¸å­˜åœ¨")
            logging.error("4. GPUè¢«å…¶ä»–è¿›ç¨‹å ç”¨")
            
            # å°è¯•è¿è¡Œnvidia-smiæ£€æŸ¥GPUçŠ¶æ€
            try:
                import subprocess
                result = subprocess.run(['nvidia-smi', '--query-gpu=index,name,memory.used,memory.total', 
                                       '--format=csv,noheader,nounits'], 
                                      capture_output=True, text=True, timeout=10)
                if result.returncode == 0:
                    logging.info("nvidia-smiè¾“å‡º:")
                    for line in result.stdout.strip().split('\n'):
                        logging.info(f"  {line}")
                else:
                    logging.error(f"nvidia-smiæ‰§è¡Œå¤±è´¥: {result.stderr}")
            except Exception as e:
                logging.error(f"æ— æ³•è¿è¡Œnvidia-smi: {e}")
    
    logging.info("=== å¼€å§‹LoRAé¢„æµ‹ä»»åŠ¡ ===")
    logging.info(f"æ—¥å¿—ä¿å­˜ç›®å½•: {log_dir}")
    
    versiBCB_lora_merge_pred(args, task=args.task, removeDeprecationData=args.Ban_Deprecation)
    # versiBCB_lora_merge_pred(args, task="vscc", removeDeprecationData=False)
    # versiBCB_lora_merge_pred(args, task="vace", removeDeprecationData=True)
    # versiBCB_lora_merge_pred(args, task="vscc", removeDeprecationData=True)
    
    logging.info("=== LoRAé¢„æµ‹ä»»åŠ¡å®Œæˆ ===")
    print("-" * 60)
    sys.exit(0)