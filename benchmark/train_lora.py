import torch
import json
import os
import shutil
import argparse
import deepspeed
import logging
import sys
from datetime import datetime
from peft import PeftModel,TaskType
from torch.utils.data import DataLoader
from utils.loraTrain.buildandloadData import DocstringDataset, collate_fn, LazyDocstringDataset,QADataset,TextDataset,DocstringDataset1,SrccodeDataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from benchmark.config.code.config_lora import LORA_CONFIG_PATH, load_config
from utils.loraTrain.getVersicodeData import getPkgDocstringItems,GetQAPairsFromBenchData,GetQAPairsFromFlatIFTData
from utils.getDatasetPacks import getPackVersions
from peft import get_peft_model, LoraConfig
from utils.loraPathConfigure import pathConfigurator
from utils.loraTrain.dataset_loader import load_dataset
from utils.loraTrain.loraTrainUtils import getEquipAdaptorModel
from utils.loraTrain.loraTrainUtils import loraModelExists,get_dataloader,buildandTrainLoraModel,getDataExistence,load_lora_model,load_config,create_lora_config,save_lora_model,load_lora_model_withPeft,load_base_model,merge_lora_weights,inference,train_lora_model_withPEFT
from utils.data_statistics.getStatistics import load_and_aggregate_package_versions
from utils.loraTrain.log import setup_logging
import traceback

# å¯¼å…¥å†…å­˜è°ƒè¯•æ¨¡å—
try:
    from utils.memoryDebug.memoryCheck import GPUMemoryProfiler
    from utils.memoryDebug.trainWithMemoryDebug import debug_lora_training, TrainingMemoryProfiler
    from utils.memoryDebug.quickMemoryDebug import QuickMemoryDebugger
    MEMORY_DEBUG_AVAILABLE = True
except ImportError as e:
    logging.warning(f"å†…å­˜è°ƒè¯•æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    MEMORY_DEBUG_AVAILABLE = False
profiler = GPUMemoryProfiler()
# def trainNormalLoraModel(config,tokenizer):
#     # ç”±åŸå§‹æ•°æ®æºæ„å»ºdataset
#     corpus_path = ''
#     dataset = getDataset(corpus_path,tokenizer)
#     # æ„å»ºdataloader
#     dataloader = DataLoader(dataset, batch_size=config["batch_size"], shuffle=True,collate_fn=collate_fn)
#     # ä½¿ç”¨dataloaderè®­ç»ƒæ¨¡å‹ï¼Œåº”ç”¨é…ç½®ä¸­çš„ç²¾åº¦
#     precision = config.get("precision", "fp16")
#     lora_model = buildandTrainLoraModel(config, dataloader, precision)
#     return lora_model
def trainLoraModelForPack(config,pkg,version,tokenizer,corpus_path=None,dataset_type='docstring',precision='fp16',enable_memory_debug=False,memory_debug_log_dir=None):
    files_info = []
    data_file_path = f"{corpus_path}/{pkg}/{version}.jsonl"
    
    logging.info(f"æ­£åœ¨åŠ è½½åŒ… {pkg}-{version} çš„è®­ç»ƒæ•°æ®: {data_file_path}")
    
    try:
        with open(data_file_path, "r", encoding="utf-8") as f:
            for line in f:
                line_data = json.loads(line)
                files_info.append(line_data)
    except FileNotFoundError:
        logging.error(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file_path}")
        return None
    except Exception as e:
        logging.error(f"è¯»å–æ•°æ®æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return None
        
    # æ ¹æ®é…ç½®çš„è®­ç»ƒæ•°æ®ç™¾åˆ†æ¯”æˆªå–æ•°æ®
    original_count = len(files_info)
    if config["override_data_percentage"] is not None:
        files_info = files_info[:int(len(files_info)*float(config["override_data_percentage"]))]
    else:
        files_info = files_info[:int(len(files_info)*config["traindata_percentage"])]
    used_count = len(files_info)
    
    logging.info(f"åŒ… {pkg}-{version}: æ€»æ•°æ®é‡={original_count}, ä½¿ç”¨æ•°æ®é‡={used_count} ({config['traindata_percentage']*100:.1f}%)")
    
    if len(files_info) == 0:
        logging.warning(f"åŒ… {pkg}-{version} æ²¡æœ‰å¯ç”¨çš„è®­ç»ƒæ•°æ®")
        return None
        
    # æ ¹æ®æ•°æ®é›†ç±»å‹åˆ›å»ºå¯¹åº”çš„æ•°æ®é›†
    if dataset_type == 'docstring':
        dataset = DocstringDataset1(files_info, tokenizer, block_size=128,pkg=pkg,version=version)
    elif dataset_type == 'docs':
        dataset = DocstringDataset1(files_info, tokenizer, block_size=128,pkg=pkg,version=version)
    elif dataset_type == 'srccodes':
        dataset = SrccodeDataset(files_info, tokenizer, block_size=128,pkg=pkg,version=version)
    else:
        logging.error(f"ä¸æ”¯æŒçš„æ•°æ®é›†ç±»å‹: {dataset_type}")
        raise ValueError(f"Invalid dataset type: {dataset_type}")
        
    logging.info(f"åˆ›å»ºæ•°æ®é›†å®Œæˆï¼Œæ•°æ®é›†å¤§å°: {len(dataset)}")
    
    dataloader = DataLoader(dataset, batch_size=config["batch_size"], shuffle=True,collate_fn=lambda batch: collate_fn(batch, tokenizer))
    logging.info(f"åˆ›å»ºDataLoaderå®Œæˆï¼Œbatch_size={config['batch_size']}")

    logging.info(f"å¼€å§‹è®­ç»ƒåŒ… {pkg}-{version} çš„LoRAæ¨¡å‹ï¼Œç²¾åº¦: {precision}")
    
    # æ ¹æ®æ˜¯å¦å¯ç”¨å†…å­˜è°ƒè¯•é€‰æ‹©ä¸åŒçš„è®­ç»ƒæ–¹å¼
    if enable_memory_debug and MEMORY_DEBUG_AVAILABLE:
        logging.info(f"ğŸ” å¯ç”¨å†…å­˜è°ƒè¯•æ¨¡å¼è®­ç»ƒåŒ… {pkg}-{version}")
        
        # è®¾ç½®å†…å­˜è°ƒè¯•æ—¥å¿—ç›®å½•
        if memory_debug_log_dir is None:
            memory_debug_log_dir = f"logs/memory_debug/{pkg}_{version}"
        
        # ä½¿ç”¨å†…å­˜è°ƒè¯•è®­ç»ƒ
        lora_model, debug_results = debug_lora_training(
            config, dataloader, precision, pkg, version, dataset_type, memory_debug_log_dir
        )
        
        # è®°å½•å†…å­˜è°ƒè¯•ç»“æœåˆ°æ—¥å¿—
        logging.info("="*80)
        logging.info(f"ğŸ“Š åŒ… {pkg}-{version} å†…å­˜è°ƒè¯•ç»“æœ:")
        logging.info("="*80)
        
        # ğŸ†• é¦–å…ˆæ˜¾ç¤ºç®€æ´çš„å†…å­˜æŠ¥å‘Šæ‘˜è¦
        if 'json_reports' in debug_results:
            logging.info(f"\nğŸ“„ ç®€æ´å†…å­˜æŠ¥å‘Šæ‘˜è¦:")
            for stage, report in debug_results['json_reports'].items():
                if 'brief' in stage and 'memory_breakdown' in report:
                    logging.info(f"\n  ğŸ” é˜¶æ®µ: {stage}")
                    
                    breakdown = report['memory_breakdown']
                    total_memory = report['total_memory_mb']
                    gpu_utilization = report['gpu_utilization_percentage']
                    
                    logging.info(f"    æ€»å†…å­˜å ç”¨: {total_memory:.2f} MB")
                    logging.info(f"    GPUåˆ©ç”¨ç‡: {gpu_utilization:.2f}%")
                    logging.info(f"    å„ç»„ä»¶å†…å­˜å ç”¨:")
                    
                    # æŒ‰å†…å­˜å ç”¨æ’åº
                    sorted_components = sorted(breakdown.items(), key=lambda x: x[1]["memory_mb"], reverse=True)
                    
                    for i, (component, info) in enumerate(sorted_components):
                        memory_mb = info["memory_mb"]
                        percentage = info["percentage"]
                        
                        # ä¸­æ–‡ç»„ä»¶åæ˜ å°„
                        component_names = {
                            "base_model": "åŸºç¡€æ¨¡å‹",
                            "lora_parameters": "LoRAå‚æ•°",
                            "gradients": "æ¢¯åº¦",
                            "optimizer_states": "ä¼˜åŒ–å™¨çŠ¶æ€",
                            "activations": "æ¿€æ´»å€¼"
                        }
                        
                        display_name = component_names.get(component, component)
                        logging.info(f"      {i+1}. {display_name}: {memory_mb:.2f} MB ({percentage:.1f}%)")
        
        # è®°å½•ç»¼åˆåˆ†ææ‘˜è¦
        if 'summary' in debug_results:
            summary = debug_results['summary']
            logging.info(f"\nğŸ¯ ç»¼åˆåˆ†ææ‘˜è¦:")
            logging.info(f"  åˆ†æé˜¶æ®µæ•°: {summary.get('total_stages_analyzed', 0)}")
            logging.info(f"  åˆ†æçš„é˜¶æ®µ: {', '.join(summary.get('stages', []))}")
            logging.info(f"  å³°å€¼å†…å­˜ä½¿ç”¨é˜¶æ®µ: {summary.get('peak_memory_stage', 'N/A')}")
            logging.info(f"  å³°å€¼å†…å­˜ä½¿ç”¨é‡: {summary.get('peak_memory_mb', 0):.2f} MB")
            
        # è®°å½•è¯¦ç»†çš„å†…å­˜ä½¿ç”¨æƒ…å†µï¼ˆä»…æ˜¾ç¤ºå…³é”®ä¿¡æ¯ï¼‰
        if 'json_reports' in debug_results:
            logging.info(f"\nğŸ“Š è¯¦ç»†å†…å­˜ä½¿ç”¨æƒ…å†µ:")
            for stage, report in debug_results['json_reports'].items():
                if 'brief' not in stage and 'memory_summary' in report:
                    memory_summary = report['memory_summary']
                    occupancy = report.get('occupancy_percentages', {})
                    
                    logging.info(f"\n  ğŸ” é˜¶æ®µ: {stage}")
                    logging.info(f"    æ€»å†…å­˜ä½¿ç”¨: {memory_summary['total_memory_mb']:.2f} MB")
                    logging.info(f"    GPUå ç”¨ç‡: {occupancy.get('total_used_percentage', 0):.2f}%")
                    
                    # åªæ˜¾ç¤ºå†…å­˜å ç”¨æœ€é«˜çš„3ä¸ªç»„ä»¶
                    component_breakdown = memory_summary.get('component_breakdown', {})
                    if component_breakdown:
                        sorted_components = sorted(component_breakdown.items(), key=lambda x: x[1], reverse=True)
                        logging.info(f"    ä¸»è¦ç»„ä»¶å†…å­˜å ç”¨:")
                        for i, (component, memory_mb) in enumerate(sorted_components[:3]):
                            component_percentage = occupancy.get('component_percentages', {}).get(component, 0)
                            logging.info(f"      {i+1}. {component}: {memory_mb:.2f} MB ({component_percentage:.2f}%)")
        
        # è®°å½•å†…å­˜é—®é¢˜æ£€æµ‹ç»“æœ
        if 'memory_issues' in debug_results:
            memory_issues = debug_results['memory_issues']
            logging.info(f"\nğŸš¨ å†…å­˜é—®é¢˜æ£€æµ‹:")
            has_issues = False
            for issue_type, details in memory_issues.items():
                if details:
                    has_issues = True
                    logging.warning(f"  âš ï¸  {issue_type}: {details}")
            if not has_issues:
                logging.info("  âœ… æœªæ£€æµ‹åˆ°å†…å­˜é—®é¢˜")
        
        # ç”Ÿæˆå†…å­˜ä¼˜åŒ–å»ºè®®
        if 'json_reports' in debug_results:
            try:
                from utils.memoryDebug.trainWithMemoryDebug import generate_memory_optimization_suggestions
                suggestions = generate_memory_optimization_suggestions(debug_results['json_reports'])
                
                if any(suggestion_list for suggestion_list in suggestions.values()):
                    logging.info(f"\nğŸ’¡ å†…å­˜ä¼˜åŒ–å»ºè®®:")
                    for category, suggestion_list in suggestions.items():
                        if suggestion_list:
                            category_names = {
                                "general_suggestions": "é€šç”¨å»ºè®®",
                                "parameter_optimization": "å‚æ•°ä¼˜åŒ–",
                                "gradient_optimization": "æ¢¯åº¦ä¼˜åŒ–",
                                "optimizer_optimization": "ä¼˜åŒ–å™¨ä¼˜åŒ–",
                                "training_optimization": "è®­ç»ƒä¼˜åŒ–"
                            }
                            display_category = category_names.get(category, category)
                            logging.info(f"  ğŸ“ {display_category}:")
                            for suggestion in suggestion_list[:2]:  # åªæ˜¾ç¤ºå‰2ä¸ªå»ºè®®
                                logging.info(f"    â€¢ {suggestion}")
                else:
                    logging.info(f"\nğŸ’¡ å†…å­˜ä¼˜åŒ–å»ºè®®: æš‚æ— ç‰¹åˆ«å»ºè®®ï¼Œå†…å­˜ä½¿ç”¨æ­£å¸¸")
            except Exception as e:
                logging.warning(f"ç”Ÿæˆä¼˜åŒ–å»ºè®®æ—¶å‡ºé”™: {e}")
        
        logging.info(f"\nğŸ“‚ è¯¦ç»†JSONæŠ¥å‘Šå·²ä¿å­˜åˆ°: {memory_debug_log_dir}")
        logging.info("="*80)
        
    else:
        # ä½¿ç”¨æ ‡å‡†è®­ç»ƒæ–¹å¼
        if enable_memory_debug and not MEMORY_DEBUG_AVAILABLE:
            logging.warning("âš ï¸  å†…å­˜è°ƒè¯•æ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨æ ‡å‡†è®­ç»ƒæ¨¡å¼")
            
        lora_model=buildandTrainLoraModel(config,dataloader,precision,pkg,version,knowledge_type=dataset_type)
    
    logging.info(f"åŒ… {pkg}-{version} çš„LoRAæ¨¡å‹è®­ç»ƒå®Œæˆ")
    
    return lora_model



def trainLoraModelsForVersiBCB(benchmark_data_path=None,corpus_path="/datanfs2/chenrongyi/data/docs",knowledge_type='docstring',model_config=None,precision='fp16',pack_versions=None,pre_filtered=False,enable_memory_debug=False,memory_debug_log_dir=None):
    """
    è®­ç»ƒLoRAæ¨¡å‹for VersiBCB
    
    Args:
        benchmark_data_path: str, å•ä¸ªbenchmarkæ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºå‘åå…¼å®¹ï¼‰
        corpus_path: str, è¯­æ–™åº“è·¯å¾„
        knowledge_type: str, çŸ¥è¯†ç±»å‹
        model_config: dict, æ¨¡å‹é…ç½®
        precision: str, ç²¾åº¦
        pack_versions: dict, é¢„å…ˆæ±‡æ€»çš„åŒ…ç‰ˆæœ¬ä¿¡æ¯ {pkg: [versions]}ï¼Œå¦‚æœæä¾›åˆ™å¿½ç•¥benchmark_data_path
        pre_filtered: bool, åŒ…ç‰ˆæœ¬æ˜¯å¦å·²ç»é¢„è¿‡æ»¤ï¼ˆå¤šworkeræ¨¡å¼ä¸‹ä¸ºTrueï¼‰
        enable_memory_debug: bool, æ˜¯å¦å¯ç”¨å†…å­˜è°ƒè¯•
        memory_debug_log_dir: str, å†…å­˜è°ƒè¯•æ—¥å¿—ç›®å½•
    """
    if pack_versions is not None:
        # ä½¿ç”¨é¢„å…ˆæ±‡æ€»çš„åŒ…ç‰ˆæœ¬ä¿¡æ¯
        packVersions = pack_versions
        logging.info(f"ä½¿ç”¨é¢„å…ˆæ±‡æ€»çš„åŒ…ç‰ˆæœ¬ä¿¡æ¯: {len(packVersions)} ä¸ªåŒ…")
    else:
        # å‘åå…¼å®¹ï¼šä½¿ç”¨å•ä¸ªbenchmarkæ–‡ä»¶
        if benchmark_data_path is None:
            raise ValueError("å¿…é¡»æä¾› benchmark_data_path æˆ– pack_versions å…¶ä¸­ä¹‹ä¸€")
        
        logging.info(f"ä»å•ä¸ªbenchmarkæ–‡ä»¶åŠ è½½åŒ…ç‰ˆæœ¬ä¿¡æ¯: {benchmark_data_path}")
        with open(benchmark_data_path, "r") as f:
            datas = json.load(f)
        packVersions = getPackVersions(datas)
    # base_model, tokenizer = load_base_model(config.get("model_name"), config.get("device_map"))
    tokenizer = AutoTokenizer.from_pretrained(model_config.get("model_name"))
    model_name = model_config.get("model_name").split("/")[-1]
    
    # å†…å­˜è°ƒè¯•ç›¸å…³æ—¥å¿—
    if enable_memory_debug:
        if MEMORY_DEBUG_AVAILABLE:
            logging.info("ğŸ” å¯ç”¨å†…å­˜è°ƒè¯•æ¨¡å¼")
            if memory_debug_log_dir:
                logging.info(f"ğŸ“‚ å†…å­˜è°ƒè¯•æ—¥å¿—ç›®å½•: {memory_debug_log_dir}")
        else:
            logging.warning("âš ï¸  å†…å­˜è°ƒè¯•æ¨¡å—ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ ‡å‡†è®­ç»ƒæ¨¡å¼")
    
    logging.info(f"å¼€å§‹è®­ç»ƒLoRAæ¨¡å‹ï¼Œæ•°æ®é›†ç±»å‹: {knowledge_type}")
    logging.info(f"æ€»è®¡ {len(packVersions)} ä¸ªåŒ…éœ€è¦å¤„ç†")
    
    trained_count = 0
    skipped_count = 0
    error_count = 0
    
    for pkg,versions in packVersions.items():
        for version in versions:
            if pre_filtered:
                # å¤šworkeræ¨¡å¼ä¸‹ï¼ŒåŒ…ç‰ˆæœ¬å·²ç»é¢„è¿‡æ»¤ï¼Œè·³è¿‡å­˜åœ¨æ€§æ£€æŸ¥
                logging.info(f"å¼€å§‹è®­ç»ƒåŒ… {pkg}-{version} çš„LoRAæ¨¡å‹ï¼ˆé¢„è¿‡æ»¤æ¨¡å¼ï¼‰")
            else:
                # ä¼ ç»Ÿæ¨¡å¼ï¼Œéœ€è¦æ£€æŸ¥LoRAæ¨¡å‹æ˜¯å¦å­˜åœ¨
                logging.info(f"æ£€æŸ¥åŒ… {pkg}-{version} çš„LoRAæ¨¡å‹æ˜¯å¦å­˜åœ¨")
                if loraModelExists(pkg,version,model_name,model_config,knowledge_type=knowledge_type):
                    logging.info(f"åŒ… {pkg}-{version} çš„LoRAæ¨¡å‹å·²å­˜åœ¨ï¼Œè·³è¿‡è®­ç»ƒ")
                    skipped_count += 1
                    continue
                    
            try:
                if not pre_filtered:
                    logging.info(f"å¼€å§‹è®­ç»ƒåŒ… {pkg}-{version} çš„LoRAæ¨¡å‹")
                    
                # è®¾ç½®æ¯ä¸ªåŒ…çš„å†…å­˜è°ƒè¯•æ—¥å¿—ç›®å½•
                pkg_memory_debug_log_dir = None
                if enable_memory_debug and memory_debug_log_dir:
                    pkg_memory_debug_log_dir = os.path.join(memory_debug_log_dir, f"{pkg}_{version}")
                    
                lora_model=trainLoraModelForPack(
                    model_config,pkg,version,tokenizer,corpus_path,
                    dataset_type=knowledge_type,precision=precision,
                    enable_memory_debug=enable_memory_debug,
                    memory_debug_log_dir=pkg_memory_debug_log_dir
                )
                lora_save_path = pathConfigurator().getPath(model_config,pkg,version,model_name,knowledge_type=knowledge_type)
                lora_model.save_pretrained(lora_save_path)
                logging.info(f"æˆåŠŸè®­ç»ƒå¹¶ä¿å­˜åŒ… {pkg}-{version} çš„LoRAæ¨¡å‹åˆ°: {lora_save_path}")
                trained_count += 1
            except Exception as e:
                logging.error(f"è®­ç»ƒåŒ… {pkg}-{version} æ—¶å‡ºé”™: {e}")
                logging.error(f"é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
                error_count += 1
                continue
    
    logging.info(f"è®­ç»ƒå®Œæˆç»Ÿè®¡: è®­ç»ƒ={trained_count}, è·³è¿‡={skipped_count}, é”™è¯¯={error_count}")
    
    # è¿”å›ç»Ÿè®¡ä¿¡æ¯ä»¥ä¾›å¤šworkeræ¨¡å¼ä½¿ç”¨
    return {
        'trained': trained_count,
        'skipped': skipped_count,
        'failed': error_count,
        'total': trained_count + skipped_count + error_count
    }

if __name__ == "__main__":
    model_config = load_config(LORA_CONFIG_PATH)
    args = argparse.ArgumentParser()
    # ï¼Ÿä¼¼ä¹æé”™äº†ï¼Œargs.precisionä¼šè¦†ç›–config_loraä¸­çš„precision
    args.add_argument("--precision", type=str, default="bf16",help="precision of the model,ä½†ä¼šè¢«config_loraä¸­çš„precisionè¦†ç›–",choices=["fp16","fp32","bf16"])
    args.add_argument("--dataset_type", type=str, default="docstring",help="dataset type, docstring or srccodes,ç”¨äºå­˜å‚¨æ–‡ä»¶çš„æœ€ç»ˆå‘½å",choices=["docstring","srccodes"])
    args.add_argument("--corpus_path", type=str, default="/datanfs4/chenrongyi/data/docs",help="corpus pathï¼Œå¿…é¡»ä¸dataset_typeä¸€è‡´ï¼Œä¸ç„¶ä¼šè®­ç»ƒå‡ºé”™è¯¯çš„å¯¹è±¡")
    args.add_argument("--benchmark_data_path", type=str, default="data/VersiBCB_Benchmark/vace_datas.json",help="benchmark data path (single file, for backward compatibility)")
    args.add_argument("--benchmark_paths", type=str, nargs='+', default=None, help="multiple benchmark data paths (will override benchmark_data_path if provided)")
    args.add_argument("--loraadaptor_save_path_base", type=str, default="/datanfs2/chenrongyi/models/loraadaptors/",help="lora adaptor save path base")
    args.add_argument("--model_name", type=str, default="/datanfs2/chenrongyi/models/Llama-3.1-8B",help="model name")
    args.add_argument("--log_dir", type=str, default=None, help="æŒ‡å®šæ—¥å¿—ç›®å½•ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™è‡ªåŠ¨ç”Ÿæˆ")
    args.add_argument("--override_data_percentage", type=str, default=None, help="override the data percentage,ä½†æ˜¯ä¸ä¿®æ”¹æ–‡ä»¶å")
    # å‡è¡¡è®¾å¤‡æ˜ å°„å‚æ•°
    args.add_argument("--use_balanced_device_map", type=bool, default=True, help="æ˜¯å¦ä½¿ç”¨å‡è¡¡è®¾å¤‡æ˜ å°„")
    args.add_argument("--force_balance", type=bool, default=True, help="æ˜¯å¦å¼ºåˆ¶å‡è¡¡åˆ†é…")
    args.add_argument("--exclude_cpu", type=bool, default=True, help="æ˜¯å¦æ’é™¤CPUè®¾å¤‡")
    args.add_argument("--check_r_consistency", type=bool, default=True, help="æ˜¯å¦æ£€æŸ¥rå€¼ä¸€è‡´æ€§")
    args.add_argument("--strict_r_check", type=bool, default=False, help="æ˜¯å¦ä¸¥æ ¼æ£€æŸ¥rå€¼ä¸€è‡´æ€§") 
    # åŠ¨æ€è®¾å¤‡æ˜ å°„å‚æ•°
    args.add_argument("--use_dynamic_device_map", type=bool, default=False, help="æ˜¯å¦ä½¿ç”¨åŠ¨æ€è®¾å¤‡æ˜ å°„ç­–ç•¥")
    args.add_argument("--balance_threshold", type=float, default=0.3, help="åŠ¨æ€æ˜ å°„çš„å‡è¡¡é˜ˆå€¼(0.0-1.0)")
    # è®¾å¤‡ç®¡ç†overrideå‚æ•°ï¼Œç”¨äºoverrideå‰éƒ¨è®¾å¤‡æ˜ å°„é…ç½®
    args.add_argument("--device_map_strategy", type=str, default="balanced", 
                     choices=["auto", "balanced", "dynamic"], help="è®¾å¤‡æ˜ å°„ç­–ç•¥é€‰æ‹©")
    
    # å¤šworkerå‚æ•°ï¼ˆæ–°å¢ï¼Œæ¨¡ä»¿train_lora_ift.pyï¼‰
    args.add_argument("--rank", type=int, default=0,
                     help="å½“å‰workerçš„rankï¼ˆç”¨äºå¤šworkerè®­ç»ƒï¼‰")
    args.add_argument("--world_size", type=int, default=1,
                     help="æ€»workeræ•°é‡ï¼ˆç”¨äºå¤šworkerè®­ç»ƒï¼‰")
    
    # å†…å­˜è°ƒè¯•å‚æ•°
    args.add_argument("--enable_memory_debug", action="store_true", default=False,
                     help="å¯ç”¨å†…å­˜è°ƒè¯•æ¨¡å¼ï¼Œè¾“å‡ºè¯¦ç»†çš„å‚æ•°ä¸æ˜¾å­˜å ç”¨å¯¹ç…§è¡¨")
    args.add_argument("--memory_debug_log_dir", type=str, default=None,
                     help="å†…å­˜è°ƒè¯•æ—¥å¿—ç›®å½•ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™è‡ªåŠ¨ç”Ÿæˆ")
    
    args = args.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    log_dir = setup_logging(args)
    
    # è¯¦ç»†çš„CUDAç¯å¢ƒæ£€æµ‹å’Œè¯Šæ–­
    logging.info("=== CUDAç¯å¢ƒè¯Šæ–­ ===")
    cuda_devices = os.environ.get('CUDA_VISIBLE_DEVICES', 'Not set')
    logging.info(f"CUDA_VISIBLE_DEVICES={cuda_devices}")
    
    # æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
    logging.info(f"torch.cuda.is_available(): {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        visible_gpu_count = torch.cuda.device_count()
        logging.info(f"PyTorchå¯è§GPUæ•°é‡: {visible_gpu_count}")
        
        if visible_gpu_count > 0:
            for i in range(visible_gpu_count):
                gpu_name = torch.cuda.get_device_properties(i).name
                total_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)
                logging.info(f"GPU {i}: {gpu_name} ({total_memory:.1f}GB)")
            
            # å½“æœ‰GPUå¯ç”¨æ—¶ï¼Œæ›´æ–°é…ç½®ä½¿ç”¨GPU
            logging.info("æ£€æµ‹åˆ°GPUï¼Œè®¾ç½®æ¨¡å‹ä½¿ç”¨GPU")
            model_config["device_map"] = "auto"
        else:
            logging.warning("PyTorchæ£€æµ‹åˆ°CUDAå¯ç”¨ä½†GPUæ•°é‡ä¸º0")
    else:
        logging.warning("PyTorchæ£€æµ‹CUDAä¸å¯ç”¨")
        
        # å¦‚æœè®¾ç½®äº†CUDA_VISIBLE_DEVICESä½†CUDAä¸å¯ç”¨ï¼Œç»™å‡ºè¯¦ç»†è¯Šæ–­
        if cuda_devices != 'Not set':
            logging.error(f"ç¯å¢ƒå˜é‡CUDA_VISIBLE_DEVICES={cuda_devices}ï¼Œä½†PyTorchæ— æ³•ä½¿ç”¨CUDA")
            logging.error("å¯èƒ½çš„åŸå› ï¼š")
            logging.error("1. PyTorchæ²¡æœ‰CUDAæ”¯æŒ - æ£€æŸ¥ï¼špython -c 'import torch; print(torch.version.cuda)'")
            logging.error("2. CUDAé©±åŠ¨/è¿è¡Œæ—¶ç‰ˆæœ¬ä¸åŒ¹é…")
            logging.error("3. GPUè®¾å¤‡ç¼–å·ä¸å­˜åœ¨")
            logging.error("4. GPUè¢«å…¶ä»–è¿›ç¨‹å ç”¨")
            
            # å°è¯•è¿è¡Œnvidia-smiæ£€æŸ¥GPUçŠ¶æ€
            try:
                import subprocess
                result = subprocess.run(['nvidia-smi', '--query-gpu=index,name,memory.used,memory.total', 
                                       '--format=csv,noheader,nounits'], 
                                      capture_output=True, text=True, timeout=10)
                if result.returncode == 0:
                    logging.info("nvidia-smiè¾“å‡º:")
                    for line in result.stdout.strip().split('\n'):
                        logging.info(f"  {line}")
                else:
                    logging.error(f"nvidia-smiæ‰§è¡Œå¤±è´¥: {result.stderr}")
            except Exception as e:
                logging.error(f"æ— æ³•è¿è¡Œnvidia-smi: {e}")
    
    logging.info("=== é…ç½®ä¿¡æ¯ ===")
    logging.info(f"æœ€ç»ˆdevice_map: {model_config.get('device_map', 'auto')}")
    
    # override base_configs using args
    model_config["precision"] = args.precision
    model_config["knowledge_type"] = args.dataset_type
    model_config["corpus_path"] = args.corpus_path
    model_config["benchmark_data_path"] = args.benchmark_data_path
    model_config["loraadaptor_save_path_base"] = args.loraadaptor_save_path_base
    model_config["model_name"] = args.model_name
    model_config["override_data_percentage"] = args.override_data_percentage
    model_config["use_balanced_device_map"] = args.use_balanced_device_map
    model_config["force_balance"] = args.force_balance
    model_config["exclude_cpu"] = args.exclude_cpu
    model_config["check_r_consistency"] = args.check_r_consistency
    model_config["strict_r_check"] = args.strict_r_check
    
    # åŠ¨æ€è®¾å¤‡æ˜ å°„å‚æ•°
    model_config["use_dynamic_device_map"] = args.use_dynamic_device_map
    model_config["balance_threshold"] = args.balance_threshold
    
    # æ ¹æ®device_map_strategyè‡ªåŠ¨è®¾ç½®æ˜ å°„å‚æ•°
    if args.device_map_strategy == "dynamic":
        model_config["use_dynamic_device_map"] = True
        model_config["use_balanced_device_map"] = False
    elif args.device_map_strategy == "balanced":
        model_config["use_dynamic_device_map"] = False
        model_config["use_balanced_device_map"] = True
    elif args.device_map_strategy == "auto":
        model_config["use_dynamic_device_map"] = False
        model_config["use_balanced_device_map"] = False
        model_config["device_map"] = "auto"
    
    # å¤„ç†å†…å­˜è°ƒè¯•å‚æ•°
    if args.enable_memory_debug:
        if not MEMORY_DEBUG_AVAILABLE:
            logging.error("âŒ å†…å­˜è°ƒè¯•æ¨¡å—ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥utils.memoryDebugæ¨¡å—æ˜¯å¦æ­£ç¡®å®‰è£…")
            logging.error("å†…å­˜è°ƒè¯•åŠŸèƒ½å°†è¢«ç¦ç”¨")
            args.enable_memory_debug = False
        else:
            logging.info("ğŸ” å¯ç”¨å†…å­˜è°ƒè¯•æ¨¡å¼")
            
            # è®¾ç½®å†…å­˜è°ƒè¯•æ—¥å¿—ç›®å½•
            if args.memory_debug_log_dir is None:
                # è‡ªåŠ¨ç”Ÿæˆå†…å­˜è°ƒè¯•æ—¥å¿—ç›®å½•
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                args.memory_debug_log_dir = os.path.join(log_dir, f"memory_debug_{timestamp}")
                
            # ç¡®ä¿å†…å­˜è°ƒè¯•æ—¥å¿—ç›®å½•å­˜åœ¨
            os.makedirs(args.memory_debug_log_dir, exist_ok=True)
            logging.info(f"ğŸ“‚ å†…å­˜è°ƒè¯•æ—¥å¿—å°†ä¿å­˜åˆ°: {args.memory_debug_log_dir}")
            
            # æ£€æŸ¥CUDAå¯ç”¨æ€§ï¼ˆå†…å­˜è°ƒè¯•éœ€è¦GPUï¼‰
            if not torch.cuda.is_available():
                logging.warning("âš ï¸  CUDAä¸å¯ç”¨ï¼Œå†…å­˜è°ƒè¯•åŠŸèƒ½å¯èƒ½å—é™")
            else:
                logging.info(f"âœ… æ£€æµ‹åˆ° {torch.cuda.device_count()} ä¸ªGPUï¼Œå†…å­˜è°ƒè¯•åŠŸèƒ½å·²å°±ç»ª")
    # ç¡®å®šä½¿ç”¨å“ªç§æ–¹å¼åŠ è½½åŒ…ç‰ˆæœ¬ä¿¡æ¯
    pack_versions = None
    if args.benchmark_paths is not None:
        # ä½¿ç”¨å¤šä¸ªbenchmarkæ–‡ä»¶
        logging.info("=== ä½¿ç”¨å¤šä¸ªbenchmarkæ–‡ä»¶æ¨¡å¼ ===")
        logging.info(f"Benchmarkæ–‡ä»¶åˆ—è¡¨: {args.benchmark_paths}")
        pack_versions = load_and_aggregate_package_versions(args.benchmark_paths)
        benchmark_data_path = None  # ä¸ä¼ é€’å•ä¸ªæ–‡ä»¶è·¯å¾„
    else:
        # ä½¿ç”¨å•ä¸ªbenchmarkæ–‡ä»¶ï¼ˆå‘åå…¼å®¹ï¼‰
        logging.info("=== ä½¿ç”¨å•ä¸ªbenchmarkæ–‡ä»¶æ¨¡å¼ ===") 
        logging.info(f"Benchmarkæ–‡ä»¶: {args.benchmark_data_path}")
        benchmark_data_path = args.benchmark_data_path
    
    # ğŸš€ å¤šworkeræ¨¡å¼çš„åŒ…ç‰ˆæœ¬åˆ†é…ï¼ˆæ–°å¢ï¼‰
    if pack_versions is not None and args.world_size > 1:
        logging.info(f"ğŸš€ å¤šworkeræ¨¡å¼: rank={args.rank}, world_size={args.world_size}")
        
        # ğŸ” ç¬¬ä¸€æ­¥ï¼šé¢„è¿‡æ»¤å‡ºçœŸæ­£éœ€è¦è®­ç»ƒçš„åŒ…ç‰ˆæœ¬ç»„åˆ
        logging.info("ğŸ“‹ ç¬¬ä¸€æ­¥ï¼šé¢„è¿‡æ»¤çœŸæ­£éœ€è¦è®­ç»ƒçš„åŒ…ç‰ˆæœ¬...")
        
        # å‡†å¤‡æ¨¡å‹ç›¸å…³ä¿¡æ¯
        model_name = model_config.get("model_name").split("/")[-1] if model_config.get("model_name") else "unknown"
        knowledge_type = model_config.get("knowledge_type", "docstring")
        
        # æ”¶é›†æ‰€æœ‰åŒ…ç‰ˆæœ¬ç»„åˆ
        all_pkg_versions = []
        for pkg, versions in pack_versions.items():
            for version in versions:
                all_pkg_versions.append((pkg, version))
        
        total_combinations = len(all_pkg_versions)
        logging.info(f"æ€»è®¡ {total_combinations} ä¸ªåŒ…ç‰ˆæœ¬ç»„åˆå¾…æ£€æŸ¥")
        
        # è¿‡æ»¤å‡ºçœŸæ­£éœ€è¦è®­ç»ƒçš„ç»„åˆ
        need_training_combinations = []
        already_trained_count = 0
        no_data_count = 0
        
        logging.info("ğŸ” å¼€å§‹æ£€æŸ¥æ¯ä¸ªåŒ…ç‰ˆæœ¬çš„è®­ç»ƒçŠ¶æ€...")
        for i, (pkg, version) in enumerate(all_pkg_versions):
            if i % 20 == 0 and i > 0:  # æ¯20ä¸ªæ‰“å°ä¸€æ¬¡è¿›åº¦
                logging.info(f"æ£€æŸ¥è¿›åº¦: {i}/{total_combinations} ({i/total_combinations*100:.1f}%)")
            
            # æ£€æŸ¥LoRAæ¨¡å‹æ˜¯å¦å·²å­˜åœ¨ï¼ˆä½¿ç”¨ç›¸åŒçš„é€»è¾‘ï¼‰
            try:
                from utils.loraTrain.loraTrainUtils import loraModelExists
                if loraModelExists(pkg, version, model_name, model_config, knowledge_type=knowledge_type):
                    already_trained_count += 1
                    logging.debug(f"è·³è¿‡å·²è®­ç»ƒ: {pkg}-{version}")
                    continue
            except Exception as e:
                logging.debug(f"æ£€æŸ¥LoRAæ¨¡å‹å­˜åœ¨æ€§æ—¶å‡ºé”™ {pkg}-{version}: {e}")
                # å¦‚æœæ£€æŸ¥å¤±è´¥ï¼Œä¿å®ˆåœ°åŠ å…¥è®­ç»ƒåˆ—è¡¨
                pass
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è®­ç»ƒæ•°æ®ï¼ˆç®€åŒ–æ£€æŸ¥ï¼ŒåªéªŒè¯æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼‰
            try:
                corpus_path = model_config.get("corpus_path", "/datanfs2/chenrongyi/data/docs")
                data_file_path = f"{corpus_path}/{pkg}/{version}.jsonl"
                if not os.path.exists(data_file_path):
                    no_data_count += 1
                    logging.debug(f"è·³è¿‡æ— æ•°æ®: {pkg}-{version}")
                    continue
                
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºç©º
                if os.path.getsize(data_file_path) == 0:
                    no_data_count += 1
                    logging.debug(f"è·³è¿‡ç©ºæ•°æ®: {pkg}-{version}")
                    continue
                    
            except Exception as e:
                logging.debug(f"æ£€æŸ¥è®­ç»ƒæ•°æ®æ—¶å‡ºé”™ {pkg}-{version}: {e}")
                # å¦‚æœæ£€æŸ¥å¤±è´¥ï¼Œä¿å®ˆåœ°åŠ å…¥è®­ç»ƒåˆ—è¡¨
                pass
            
            # å¦‚æœé€šè¿‡æ‰€æœ‰æ£€æŸ¥ï¼ŒåŠ å…¥éœ€è¦è®­ç»ƒçš„åˆ—è¡¨
            need_training_combinations.append((pkg, version))
        
        logging.info(f"ğŸ“Š é¢„è¿‡æ»¤ç»Ÿè®¡:")
        logging.info(f"  åŸå§‹ç»„åˆæ•°: {total_combinations}")
        logging.info(f"  å·²è®­ç»ƒè·³è¿‡: {already_trained_count}")
        logging.info(f"  æ— æ•°æ®è·³è¿‡: {no_data_count}")
        logging.info(f"  éœ€è¦è®­ç»ƒ: {len(need_training_combinations)}")
        logging.info(f"  è¿‡æ»¤ç‡: {(already_trained_count + no_data_count)/total_combinations*100:.1f}%")
        
        if len(need_training_combinations) == 0:
            logging.info(f"Worker {args.rank}: æ²¡æœ‰éœ€è¦è®­ç»ƒçš„åŒ…ç‰ˆæœ¬ç»„åˆï¼Œé€€å‡º")
            sys.exit(0)
        
        # ğŸ¯ ç¬¬äºŒæ­¥ï¼šå°†çœŸæ­£éœ€è¦è®­ç»ƒçš„ç»„åˆåˆ†é…ç»™workers
        logging.info(f"ğŸ¯ ç¬¬äºŒæ­¥ï¼šåˆ†é… {len(need_training_combinations)} ä¸ªçœŸæ­£éœ€è¦è®­ç»ƒçš„ç»„åˆ...")
        
        # è®¡ç®—å½“å‰workerè´Ÿè´£çš„åŒ…ç‰ˆæœ¬èŒƒå›´ï¼ˆåŸºäºéœ€è¦è®­ç»ƒçš„ç»„åˆï¼‰
        real_combinations = len(need_training_combinations)
        combinations_per_worker = real_combinations // args.world_size
        remainder = real_combinations % args.world_size
        
        # è®¡ç®—å½“å‰workerçš„èµ·å§‹å’Œç»“æŸç´¢å¼•
        start_idx = args.rank * combinations_per_worker + min(args.rank, remainder)
        end_idx = start_idx + combinations_per_worker + (1 if args.rank < remainder else 0)
        
        # åˆ†é…ç»™å½“å‰workerçš„åŒ…ç‰ˆæœ¬ç»„åˆï¼ˆåªåŒ…å«éœ€è¦è®­ç»ƒçš„ï¼‰
        worker_pkg_versions = need_training_combinations[start_idx:end_idx]
        
        logging.info(f"Worker {args.rank} åˆ†é…åˆ° {len(worker_pkg_versions)} ä¸ªéœ€è¦è®­ç»ƒçš„åŒ…ç‰ˆæœ¬ç»„åˆ:")
        logging.info(f"  å…¨å±€ç´¢å¼•èŒƒå›´: [{start_idx}, {end_idx}) (åŸºäºéœ€è¦è®­ç»ƒçš„ç»„åˆ)")
        logging.info(f"  é¢„æœŸè®­ç»ƒå·¥ä½œé‡å‡è¡¡: âœ…")
        
        # æ˜¾ç¤ºåˆ†é…è¯¦æƒ…ï¼ˆå‰5ä¸ªï¼‰
        for i, (pkg, version) in enumerate(worker_pkg_versions[:5]):
            logging.info(f"    {i+1}. {pkg}-{version}")
        if len(worker_pkg_versions) > 5:
            logging.info(f"    ... è¿˜æœ‰ {len(worker_pkg_versions) - 5} ä¸ª")
        
        # é‡æ–°æ„å»ºå½“å‰workerçš„pack_versionså­—å…¸
        worker_pack_versions = {}
        for pkg, version in worker_pkg_versions:
            if pkg not in worker_pack_versions:
                worker_pack_versions[pkg] = []
            worker_pack_versions[pkg].append(version)
        
        # ä½¿ç”¨åˆ†é…åçš„åŒ…ç‰ˆæœ¬ä¿¡æ¯
        pack_versions = worker_pack_versions
        
        if len(pack_versions) == 0:
            logging.info(f"Worker {args.rank} æ²¡æœ‰åˆ†é…åˆ°ä»»ä½•éœ€è¦è®­ç»ƒçš„åŒ…ç‰ˆæœ¬ï¼Œé€€å‡º")
            sys.exit(0)
        
        # æ˜¾ç¤ºæœ€ç»ˆçš„è´Ÿè½½åˆ†é…ä¿¡æ¯
        logging.info(f"ğŸ’ª Worker {args.rank} æœ€ç»ˆåˆ†é…:")
        logging.info(f"  åŒ…æ•°é‡: {len(worker_pack_versions)}")
        logging.info(f"  åŒ…ç‰ˆæœ¬ç»„åˆæ•°: {len(worker_pkg_versions)}")
        logging.info(f"  é¢„æœŸéƒ½éœ€è¦å®é™…è®­ç»ƒ: âœ…")
            
    elif pack_versions is not None:
        logging.info("å•workeræ¨¡å¼")
    elif args.world_size > 1:
        logging.info(f"ğŸš€ å¤šworkeræ¨¡å¼: rank={args.rank}, world_size={args.world_size}")
        logging.info("æ³¨æ„: å¤šworkeræ¨¡å¼å½“å‰ä»…æ”¯æŒä½¿ç”¨--benchmark_pathså‚æ•°çš„æ‰¹é‡æ¨¡å¼")
        logging.info("å•ä¸ªbenchmarkæ–‡ä»¶æ¨¡å¼å°†åœ¨æ‰€æœ‰workerä¸Šè¿è¡Œç›¸åŒçš„è®­ç»ƒä»»åŠ¡")
    
    if pack_versions is not None:
        logging.info(f"å½“å‰workerå°†å¤„ç† {len(pack_versions)} ä¸ªåŒ…ï¼Œå…± {sum(len(versions) for versions in pack_versions.values())} ä¸ªåŒ…ç‰ˆæœ¬ç»„åˆ")
    
    logging.info("å¼€å§‹LoRAæ¨¡å‹è®­ç»ƒ...")
    try:
        stats = trainLoraModelsForVersiBCB(
            benchmark_data_path=benchmark_data_path,
            model_config=model_config,
            precision=args.precision,
            knowledge_type=args.dataset_type,
            corpus_path=args.corpus_path,
            pack_versions=pack_versions,
            pre_filtered=pack_versions is not None and args.world_size > 1,
            enable_memory_debug=args.enable_memory_debug,
            memory_debug_log_dir=args.memory_debug_log_dir
        )
        if args.world_size > 1:
            logging.info(f"Worker {args.rank} LoRAæ¨¡å‹è®­ç»ƒä»»åŠ¡å®Œæˆ!")
            logging.info(f"Worker {args.rank} ç»Ÿè®¡: è®­ç»ƒ={stats['trained']}, è·³è¿‡={stats['skipped']}, é”™è¯¯={stats['failed']}, æ€»è®¡={stats['total']}")
        else:
            logging.info("LoRAæ¨¡å‹è®­ç»ƒæˆåŠŸå®Œæˆ!")
            logging.info(f"æœ€ç»ˆç»Ÿè®¡: è®­ç»ƒ={stats['trained']}, è·³è¿‡={stats['skipped']}, é”™è¯¯={stats['failed']}, æ€»è®¡={stats['total']}")
    except Exception as e:
        if args.world_size > 1:
            logging.error(f"Worker {args.rank} è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        else:
            logging.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        raise
    
    logging.info(f"æ‰€æœ‰æ—¥å¿—å·²ä¿å­˜åˆ°: {log_dir}")
    pass

# =============================================================================
# ä½¿ç”¨ç¤ºä¾‹ (Usage Examples)
# =============================================================================
#
# 1. ä½¿ç”¨å•ä¸ªbenchmarkæ–‡ä»¶ï¼ˆå‘åå…¼å®¹ï¼‰:
# python benchmark/train_lora.py \
#     --precision fp16 \
#     --dataset_type docstring \
#     --corpus_path /datanfs2/chenrongyi/data/docs \
#     --benchmark_data_path benchmark/data/VersiBCB_Benchmark/vace_datas.json \
#     --model_name /datanfs2/chenrongyi/models/Llama-3.1-8B
#
# 2. ä½¿ç”¨å¤šä¸ªbenchmarkæ–‡ä»¶ï¼ˆæ–°åŠŸèƒ½ï¼‰:
# python benchmark/train_lora.py \
#     --precision fp16 \
#     --dataset_type docstring \
#     --corpus_path /datanfs2/chenrongyi/data/docs \
#     --benchmark_paths \
#         benchmark/data/VersiBCB_Benchmark/vace_datas.json \
#         benchmark/data/VersiBCB_Benchmark/vscc_datas.json \
#         benchmark/data/VersiBCB_Benchmark/vace_datas_for_warning.json \
#     --model_name /datanfs2/chenrongyi/models/Llama-3.1-8B
#
# 3. å¯ç”¨å†…å­˜è°ƒè¯•æ¨¡å¼ï¼ˆæ–°åŠŸèƒ½ï¼‰:
# python benchmark/train_lora.py \
#     --precision fp16 \
#     --dataset_type docstring \
#     --corpus_path /datanfs2/chenrongyi/data/docs \
#     --benchmark_data_path benchmark/data/VersiBCB_Benchmark/vace_datas.json \
#     --model_name /datanfs2/chenrongyi/models/Llama-3.1-8B \
#     --enable_memory_debug \
#     --memory_debug_log_dir logs/memory_debug_custom
#
# 4. å†…å­˜è°ƒè¯• + å¤šæ–‡ä»¶æ‰¹é‡è®­ç»ƒ:
# python benchmark/train_lora.py \
#     --precision fp16 \
#     --dataset_type docstring \
#     --corpus_path /datanfs2/chenrongyi/data/docs \
#     --benchmark_paths \
#         benchmark/data/VersiBCB_Benchmark/vace_datas.json \
#         benchmark/data/VersiBCB_Benchmark/vscc_datas.json \
#     --model_name /datanfs2/chenrongyi/models/Llama-3.1-8B \
#     --enable_memory_debug
#
# æ³¨æ„: 
# - å¦‚æœåŒæ—¶æä¾› --benchmark_paths å’Œ --benchmark_data_pathï¼Œ
#   --benchmark_paths ä¼šè¦†ç›– --benchmark_data_path
# - å†…å­˜è°ƒè¯•æ¨¡å¼ä¼šè¾“å‡ºè¯¦ç»†çš„å‚æ•°ä¸æ˜¾å­˜å ç”¨å¯¹ç…§è¡¨åˆ°æ—¥å¿—æ–‡ä»¶
# - å¦‚æœä¸æŒ‡å®š --memory_debug_log_dirï¼Œä¼šè‡ªåŠ¨ç”Ÿæˆæ—¶é—´æˆ³ç›®å½•
# ============================================================================= 
