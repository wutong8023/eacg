import torch
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
import json
import os
import shutil
import argparse
import logging
import sys
from datetime import datetime
from peft import PeftModel, TaskType
from torch.utils.data import DataLoader
from utils.loraTrain.buildandloadData import DocstringDataset, collate_fn, LazyDocstringDataset, QADataset, TextDataset, DocstringDataset1, SrccodeDataset
from benchmark.config.code.config_lora import MODEL_SOURCE
if MODEL_SOURCE == "MODELSCOPE":
    from modelscope import AutoTokenizer
else:
    from transformers import AutoTokenizer
from benchmark.config.code.config_lora import LORA_CONFIG_PATH, load_config
from utils.loraTrain.getVersicodeData import getPkgDocstringItems, GetQAPairsFromBenchData, GetQAPairsFromFlatIFTData
from utils.getDatasetPacks import getPackVersions
from peft import get_peft_model, LoraConfig
from utils.loraPathConfigure import pathConfigurator
from utils.loraTrain.dataset_loader import load_dataset
from utils.loraTrain.loraTrainUtils import getEquipAdaptorModel, loraModelExists, get_dataloader, buildandTrainLoraModel, getDataExistence, load_lora_model, load_config, create_lora_config, save_lora_model, load_lora_model_withPeft, load_base_model, merge_lora_weights, inference, train_lora_model_withPEFT
from utils.data_statistics.getStatistics import load_and_aggregate_package_versions
from utils.loraTrain.log import setup_logging
from utils.data_distribution_checker import create_distributed_dataloader_with_checking, check_batch_distribution
from utils.clean_resource import log_gpu_memory_usage, clear_model_outputs_and_cache, clear_optimizer_states, force_clear_cuda_context, comprehensive_memory_cleanup, force_memory_reset_device, force_cleanup_memory,  find_and_clear_lingering_tensors,log_detailed_gpu_memory_report
from utils.loraTrain.dataFilter import getTrainDataItems, apply_prefilter_to_package_versions
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))





import matplotlib
from utils.plotting.training_plots import save_training_plots, create_loss_summary, print_loss_summary


# è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ç¦ç”¨tokenizerså¹¶è¡ŒåŒ–ï¼Œé¿å…åˆ†å¸ƒå¼è®­ç»ƒä¸­çš„æ­»é”è­¦å‘Š

os.environ["TOKENIZERS_PARALLELISM"] = "false"


def init_distributed_training():
    """
    åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ
    """
    # æ£€æŸ¥æ˜¯å¦åœ¨torchrunç¯å¢ƒä¸­
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        # ä»ç¯å¢ƒå˜é‡ä¸­è·å–rankå’Œworld_size
        rank = int(os.environ['RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        local_rank = int(os.environ['LOCAL_RANK'])
        
        # è®¾ç½®CUDAè®¾å¤‡
        torch.cuda.set_device(local_rank)
        
        # åˆå§‹åŒ–åˆ†å¸ƒå¼è¿›ç¨‹ç»„
        dist.init_process_group(backend='nccl', init_method='env://')
        
        logging.info(f"åˆ†å¸ƒå¼è®­ç»ƒåˆå§‹åŒ–å®Œæˆ - Rank: {rank}, World Size: {world_size}, Local Rank: {local_rank}")
        
        return rank, world_size, local_rank
    else:
        # éåˆ†å¸ƒå¼ç¯å¢ƒ
        logging.info("éåˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ")
        return 0, 1, 0

import torch
from torch.utils.data import DataLoader
from torch.utils.data.dataset import Subset
import numpy as np
import torch.distributed as dist
from utils.loraTrain.buildandloadData import collate_fn

def create_kfold_dataloader(dataset, batch_size, rank, world_size, shuffle=True):
    """
    åˆ›å»ºK-Foldåˆ†å¸ƒå¼æ•°æ®åŠ è½½å™¨ï¼Œç¡®ä¿æ¯ä¸ªrankæ•°æ®æ€»é‡ä¸€è‡´ä¸”æ­£äº¤
    
    å‚æ•°:
        dataset: å®Œæ•´æ•°æ®é›†,ç›®å‰çš„æ ¼å¼æ˜¯DocstringDataset1ï¼Œæ¯æ¡æ•°æ®æ˜¯strï¼Œé€šè¿‡collate_fnè½¬æ¢ä¸ºdict
        batch_size: æ¯ä¸ªbatchçš„å¤§å°
        rank: å½“å‰è¿›ç¨‹çš„rank (0åˆ°world_size-1)
        world_size: æ€»è¿›ç¨‹æ•°(å³Kå€¼)
        shuffle: æ˜¯å¦åœ¨åˆ’åˆ†foldå‰æ‰“ä¹±æ•°æ®é¡ºåº
    
    è¿”å›:
        DataLoader: ä¸ºå½“å‰rankåˆ›å»ºçš„DataLoader
    """
    # ç¡®ä¿world_sizeç­‰äºè¿›ç¨‹æ•°ä¸”å¤§äº1
    if world_size < 2:
        # å•è¿›ç¨‹æƒ…å†µï¼Œç›´æ¥è¿”å›å®Œæ•´æ•°æ®é›†
        dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=2,
            pin_memory=True,
            drop_last=True,
            collate_fn=lambda batch: collate_fn(batch, dataset.tokenizer if hasattr(dataset, 'tokenizer') else None)
        )
        return dataloader
    
    # 1. åˆ›å»ºå…¨å±€ç´¢å¼•
    indices = np.arange(len(dataset))
    # æ‰“ä¹±çš„é€‰é¡¹
    # if shuffle:
    #     rng = np.random.RandomState(42)  # å›ºå®šéšæœºç§å­ä¿è¯å„rankä¸€è‡´
    #     rng.shuffle(indices)
    
    # 2. å°†æ•°æ®åˆ’åˆ†ä¸ºKä¸ªfoldï¼Œç¡®ä¿æ¯ä¸ªfoldå¤§å°ç›¸ç­‰
    fold_size = len(dataset) // world_size
    fold_indices = []
    
    for i in range(world_size):
        start = i * fold_size
        end = (i + 1) * fold_size
        fold_indices.append(indices[start:end].tolist())
        logging.info(f"fold_indices length for fold {i}: {len(fold_indices[i])}")
    # logging.info(f"fold_indices: {fold_indices}")
    # logging.info(f"fold_indices length: {len(fold_indices)}")
    
    # 3. ä¸ºå½“å‰rankåˆ›å»ºå­é›†
    current_fold_indices = fold_indices[rank]
    subset = Subset(dataset, current_fold_indices)

    # 5. éªŒè¯æ•°æ®åˆ†é…å¹¶æ‰“å°ç´¢å¼•ä¿¡æ¯
    logging.info(f"Rank {rank}: åˆ†é…åˆ° {len(subset)} ä¸ªæ ·æœ¬")
    if world_size > 1:
        all_subsets = [None] * world_size
        dist.gather_object(
            current_fold_indices,
            all_subsets if rank == 0 else None,
            dst=0
        )
        for i in range(world_size):
            if rank == 0:
                logging.info(f"Rank {i}: æ ¹æ®subsetåˆ†é…åˆ° {len(all_subsets[i])} ä¸ªæ ·æœ¬")

    
    # logging.info(f"Rank {rank}: ç´¢å¼•èŒƒå›´ {min(current_fold_indices)}-{max(current_fold_indices)}")
    # logging.info(f"Rank {rank}: å‰10ä¸ªç´¢å¼•: {current_fold_indices[:10]}")
    # logging.info(f"Rank {rank}: å10ä¸ªç´¢å¼•: {current_fold_indices[-10:]}")
        
    # 4. åˆ›å»ºDataLoader
    dataloader = DataLoader(
        subset,
        batch_size=batch_size,
        shuffle=False,  
        pin_memory=True,
        drop_last=False,  # ç¡®ä¿æ‰€æœ‰rankçš„batchæ•°ä¸€è‡´
        collate_fn=lambda batch: collate_fn(batch, dataset.tokenizer if hasattr(dataset, 'tokenizer') else None)
    )
    

    # 6. æ”¶é›†æ‰€æœ‰rankçš„ç´¢å¼•å¹¶éªŒè¯æ­£äº¤æ€§
    if world_size > 1:
        # æ”¶é›†æ‰€æœ‰rankçš„ç´¢å¼•
        all_indices = [None] * world_size
        dist.gather_object(
            current_fold_indices,
            all_indices if rank == 0 else None,
            dst=0
        )
        
        # åœ¨rank 0ä¸ŠéªŒè¯æ­£äº¤æ€§
        if rank == 0:
            logging.info(f"\n=== K-Foldæ•°æ®åˆ†é…éªŒè¯ ===")
            logging.info(f"æ€»æ•°æ®é›†å¤§å°: {len(dataset)}")
            
            # æ£€æŸ¥æ¯ä¸ªfoldçš„å¤§å°
            for i, fold_idx in enumerate(all_indices):
                logging.info(f"Fold {i}: {len(fold_idx)} ä¸ªæ ·æœ¬, æœ€å°ç´¢å¼•: {min(fold_idx)}, æœ€å¤§ç´¢å¼•: {max(fold_idx)}")
            
            # æ£€æŸ¥æ­£äº¤æ€§ï¼ˆæ— é‡å ï¼‰
            overlap_found = False
            for i in range(world_size):
                for j in range(i + 1, world_size):
                    set_i = set(all_indices[i])
                    set_j = set(all_indices[j])
                    overlap = set_i & set_j
                    if overlap:
                        logging.info(f"âŒ Fold {i} å’Œ Fold {j} æœ‰ {len(overlap)} ä¸ªé‡å ç´¢å¼•")
                        overlap_found = True
            
            if not overlap_found:
                logging.info("âœ… æ‰€æœ‰foldä¹‹é—´å®Œå…¨æ­£äº¤ï¼ˆæ— é‡å ï¼‰")
            
            # æ£€æŸ¥è¦†ç›–ç‡
            all_used_indices = set()
            for fold_idx in all_indices:
                all_used_indices.update(fold_idx)
            
            total_used = len(all_used_indices)
            expected_used = world_size * fold_size
            logging.info(f"æ•°æ®è¦†ç›–æƒ…å†µ: ä½¿ç”¨äº† {total_used} / {len(dataset)} ä¸ªæ ·æœ¬")
            logging.info(f"é¢„æœŸä½¿ç”¨: {expected_used} ä¸ªæ ·æœ¬")
            
            if total_used == expected_used:
                logging.info("âœ… æ•°æ®è¦†ç›–ç‡æ­£ç¡®")
            else:
                logging.info(f"âš ï¸  æ•°æ®è¦†ç›–ç‡é—®é¢˜: ä¸¢å¤±äº† {len(dataset) - total_used} ä¸ªæ ·æœ¬")
    
    return dataloader


def verify_dataloader_indices(dataloader, rank, world_size):
    """
    éªŒè¯DataLoaderå®é™…åŠ è½½çš„æ•°æ®ç´¢å¼•
    """
    print(f"\n=== Rank {rank} DataLoaderç´¢å¼•éªŒè¯ ===")
    
    # æ”¶é›†å‰å‡ ä¸ªbatchçš„å®é™…ç´¢å¼•
    batch_indices = []
    for i, batch in enumerate(dataloader):
        if i >= 3:  # åªæ£€æŸ¥å‰3ä¸ªbatch
            break
        
        # å°è¯•ä»batchä¸­æå–åŸå§‹ç´¢å¼•ä¿¡æ¯
        if hasattr(batch, 'indices'):
            indices = batch.indices
        elif isinstance(batch, dict) and 'indices' in batch:
            indices = batch['indices']
        else:
            # å¦‚æœæ²¡æœ‰ç›´æ¥çš„ç´¢å¼•ä¿¡æ¯ï¼Œä½¿ç”¨batchçš„é¡ºåºæ¨æ–­
            batch_size = len(batch['input_ids']) if isinstance(batch, dict) and 'input_ids' in batch else len(batch)
            start_idx = i * batch_size
            indices = list(range(start_idx, start_idx + batch_size))
        
        batch_indices.extend(indices)
        print(f"Rank {rank}, Batch {i}: ç´¢å¼• {indices[:5]}...{indices[-5:] if len(indices) > 5 else indices}")
    
    print(f"Rank {rank}: å‰3ä¸ªbatchæ€»å…±æ¶‰åŠ {len(batch_indices)} ä¸ªæ ·æœ¬")
    
    # æ”¶é›†æ‰€æœ‰rankçš„batchç´¢å¼•
    if world_size > 1:
        all_batch_indices = [None] * world_size
        dist.gather_object(
            batch_indices,
            all_batch_indices if rank == 0 else None,
            dst=0
        )
        
        # åœ¨rank 0ä¸ŠéªŒè¯batchçº§åˆ«çš„æ­£äº¤æ€§
        if rank == 0:
            print(f"\n=== Batchçº§åˆ«æ­£äº¤æ€§éªŒè¯ ===")
            overlap_found = False
            for i in range(world_size):
                for j in range(i + 1, world_size):
                    set_i = set(all_batch_indices[i])
                    set_j = set(all_batch_indices[j])
                    overlap = set_i & set_j
                    if overlap:
                        print(f"âŒ Rank {i} å’Œ Rank {j} çš„batchæœ‰ {len(overlap)} ä¸ªé‡å ç´¢å¼•")
                        overlap_found = True
            
            if not overlap_found:
                print("âœ… æ‰€æœ‰rankçš„batchæ•°æ®å®Œå…¨æ­£äº¤")


def trainLoraModelForPack_DDP(config, pkg, version, tokenizer, corpus_path=None, dataset_type='docstring', precision='fp16', rank=0, world_size=1, local_rank=0, gpus_per_process=1, enable_data_checking=True,no_save=False):
    """
    åˆ†å¸ƒå¼è®­ç»ƒå•ä¸ªåŒ…çš„LoRAæ¨¡å‹ï¼ˆå¸¦æ•°æ®åˆ†å¸ƒæ£€æŸ¥ï¼‰
    """
    if rank == 0:
        logging.info(f"æ­£åœ¨åŠ è½½åŒ… {pkg}-{version} çš„åŸå§‹è®­ç»ƒæ•°æ®")
        
    # files_info = []
    # data_file_path = f"{corpus_path}/{pkg}/{version}.jsonl"
    # try:
    #     with open(data_file_path, "r", encoding="utf-8") as f:
    #         for line in f:
    #             files_info.append(json.loads(line))
    # except FileNotFoundError:
    #     if rank == 0:
    #         logging.error(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file_path}")
    #     return None
        
    # original_count = len(files_info)
    # if config["override_data_percentage"] is not None:
    #     files_info = files_info[:int(len(files_info)*float(config["override_data_percentage"]))]
    # else:
    #     files_info = files_info[:int(len(files_info)*config["traindata_percentage"])]
    original_count,files_info = getTrainDataItems(corpus_path,pkg,version,config)
    if rank == 0:
        logging.info(f"åŒ… {pkg}-{version}: æ€»æ•°æ®é‡={original_count}, ä½¿ç”¨æ•°æ®é‡={len(files_info)} ({config['traindata_percentage']*100:.1f}%)")

    # --- æ•°æ®é›†å•ç‚¹ç”Ÿæˆä¸åˆ†å‘ ---
    processed_input_ids = None
    
    if rank == 0:
        # åªæœ‰ rank 0 è¿›è¡Œæ•°æ®é¢„å¤„ç†
        logging.info("Rank 0: æ­£åœ¨è¿›è¡Œåˆ†è¯å’Œåˆ‡å—...")
        try:
            temp_dataset = DocstringDataset1(files_info, tokenizer, block_size=128, pkg=pkg, version=version)
            processed_input_ids = temp_dataset.input_ids
            logging.info(f"Rank 0: æ•°æ®å¤„ç†å®Œæˆï¼Œç”Ÿæˆ {len(processed_input_ids)} æ¡æ ·æœ¬")
            
            if len(processed_input_ids) == 0:
                logging.warning("Rank 0: è­¦å‘Š - å¤„ç†åçš„æ•°æ®é›†ä¸ºç©º")
        except Exception as e:
            logging.error(f"Rank 0: æ•°æ®å¤„ç†å¤±è´¥: {e}")
            processed_input_ids = []
    
    # ä½¿ç”¨ broadcast_object_list åˆ†å‘å¤„ç†å¥½çš„æ•°æ®
    if world_size > 1:
        logging.info(f"Rank {rank}: å¼€å§‹æ•°æ®åˆ†å‘...")
        
        # åˆ›å»ºè¦å¹¿æ’­çš„å¯¹è±¡åˆ—è¡¨
        object_list = [processed_input_ids] if rank == 0 else [None]
        
        try:
            # å¹¿æ’­æ•°æ®
            dist.broadcast_object_list(object_list, src=0)
            
            # æ‰€æœ‰è¿›ç¨‹éƒ½ä»å¹¿æ’­åçš„æ•°æ®ä¸­è·å– input_ids
            final_input_ids = object_list[0]
            
            if final_input_ids is None:
                logging.error(f"Rank {rank}: æ¥æ”¶åˆ°ç©ºæ•°æ®")
                return None
            
            logging.info(f"Rank {rank}: æ•°æ®åˆ†å‘å®Œæˆï¼Œæ¥æ”¶åˆ° {len(final_input_ids)} æ¡æ ·æœ¬")
        except Exception as e:
            logging.error(f"Rank {rank}: æ•°æ®åˆ†å‘å¤±è´¥: {e}")
            return None
    else:
        # å•è¿›ç¨‹æƒ…å†µ
        final_input_ids = processed_input_ids
        if final_input_ids is None or len(final_input_ids) == 0:
            logging.warning(f"Rank {rank}: å•è¿›ç¨‹æ¨¡å¼ï¼Œæ•°æ®ä¸ºç©º")
            return None
        logging.info(f"Rank {rank}: å•è¿›ç¨‹æ¨¡å¼ï¼Œæ•°æ®é‡ {len(final_input_ids)} æ¡æ ·æœ¬")
    
    # æ‰€æœ‰è¿›ç¨‹ä½¿ç”¨ç›¸åŒçš„æ•°æ®åˆ›å»ºæ•°æ®é›†å®ä¾‹
    dataset = DocstringDataset1(
        items=None,  # ä¸éœ€è¦åŸå§‹æ•°æ®
        tokenizer=tokenizer,
        block_size=128,
        pkg=pkg,
        version=version,
        input_ids=final_input_ids  # ä½¿ç”¨åˆ†å‘çš„æ•°æ®
    )
    
    # éªŒè¯æ‰€æœ‰rankçš„æ•°æ®é›†å¤§å°ä¸€è‡´
    if world_size > 1:
        dataset_sizes = [None] * world_size
        dist.gather_object(
            len(dataset),
            dataset_sizes if rank == 0 else None,
            dst=0
        )
        
        if rank == 0:
            logging.info("=== æ•°æ®åˆ†å‘åçš„Dataseté•¿åº¦éªŒè¯ ===")
            for i, size in enumerate(dataset_sizes):
                logging.info(f"Rank {i}: æœ€ç»ˆæ•°æ®é›†å¤§å° {size} æ¡æ ·æœ¬")
            
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰rankçš„æ•°æ®é›†å¤§å°ä¸€è‡´
            unique_sizes = set(dataset_sizes)
            if len(unique_sizes) == 1:
                logging.info("âœ… æ‰€æœ‰rankçš„æ•°æ®é›†å¤§å°å®Œå…¨ä¸€è‡´")
            else:
                logging.error(f"âŒ æ•°æ®åˆ†å‘åä»ç„¶ä¸ä¸€è‡´: {unique_sizes}")
                
            logging.info("=== æ•°æ®åˆ†å‘éªŒè¯å®Œæˆ ===")
    else:
        logging.info(f"Rank {rank}: å•è¿›ç¨‹æ¨¡å¼ï¼Œæœ€ç»ˆæ•°æ®é›†å¤§å° {len(dataset)} æ¡æ ·æœ¬")

    if rank == 0:
        logging.info(f"æ‰€æœ‰è¿›ç¨‹å‡å·²åŒæ­¥æ•°æ®é›†ï¼Œæ€»å¤§å°: {len(dataset)}")
    
    # åˆ›å»ºK-foldåˆ†å¸ƒå¼æ•°æ®åŠ è½½å™¨
    dataloader = create_kfold_dataloader(dataset, config["batch_size"], rank, world_size, shuffle=True)
    
    if rank == 0:
        logging.info(f"åˆ›å»ºK-fold DataLoaderå®Œæˆï¼Œbatch_size={config['batch_size']}, world_size={world_size}")
        if enable_data_checking:
            logging.info("æ•°æ®åˆ†å¸ƒæ£€æŸ¥åŠŸèƒ½å·²å¯ç”¨")
    
    # éªŒè¯DataLoaderçš„å®é™…ç´¢å¼•åˆ†å¸ƒ
    if enable_data_checking:
        verify_dataloader_indices(dataloader, rank, world_size)
    
    if rank == 0:
        logging.info(f"å¼€å§‹è®­ç»ƒåŒ… {pkg}-{version} çš„LoRAæ¨¡å‹ï¼Œç²¾åº¦: {precision}")
    
    # è°ƒç”¨åˆ†å¸ƒå¼è®­ç»ƒå‡½æ•°
    lora_model = buildandTrainLoraModel_DDP(
        config, dataloader, precision, pkg, version, 
        knowledge_type=dataset_type, rank=rank, world_size=world_size, 
        local_rank=local_rank, gpus_per_process=gpus_per_process,
        no_save=no_save
    )
    
    if rank == 0:
        logging.info(f"åŒ… {pkg}-{version} çš„LoRAæ¨¡å‹è®­ç»ƒå®Œæˆ")
    
    # æ¸…ç†æ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    if 'dataset' in locals():
        del dataset
    if 'dataloader' in locals():
        del dataloader
    if 'final_input_ids' in locals():
        del final_input_ids
    if 'processed_input_ids' in locals():
        del processed_input_ids
    if 'files_info' in locals():
        del files_info
    
    # ä½¿ç”¨æ–°çš„æ¸…ç†å‡½æ•°
    # comprehensive_memory_cleanup(rank, "åŒ…è®­ç»ƒç»“æŸ", f"{pkg}-{version}")
    
    return lora_model





def trainLoraModelsForVersiBCB_DDP(benchmark_data_path=None, corpus_path="/datanfs2/chenrongyi/data/docs", knowledge_type='docstring', model_config=None, precision='fp16', pack_versions=None, pre_filtered=False, rank=0, world_size=1, local_rank=0, gpus_per_process=1, enable_data_checking=True, no_save=False, enable_prefilter=True):
    """
    åˆ†å¸ƒå¼è®­ç»ƒLoRAæ¨¡å‹for VersiBCB,æŒ‡å‘æ‰€æœ‰åŒ…ç‰ˆæœ¬
    
    Args:
        enable_prefilter: æ˜¯å¦å¯ç”¨é¢„è¿‡æ»¤åŠŸèƒ½ï¼Œæå‰è¿‡æ»¤æ‰ä¸éœ€è¦è®­ç»ƒçš„åŒ…ç‰ˆæœ¬
    """
    if pack_versions is not None:
        # ä½¿ç”¨é¢„å…ˆæ±‡æ€»çš„åŒ…ç‰ˆæœ¬ä¿¡æ¯
        packVersions = pack_versions
        if rank == 0:
            logging.info(f"ä½¿ç”¨é¢„å…ˆæ±‡æ€»çš„åŒ…ç‰ˆæœ¬ä¿¡æ¯: {len(packVersions)} ä¸ªåŒ…")
    else:
        # å‘åå…¼å®¹ï¼šä½¿ç”¨å•ä¸ªbenchmarkæ–‡ä»¶
        if benchmark_data_path is None:
            raise ValueError("å¿…é¡»æä¾› benchmark_data_path æˆ– pack_versions å…¶ä¸­ä¹‹ä¸€")
        
        if rank == 0:
            logging.info(f"ä»å•ä¸ªbenchmarkæ–‡ä»¶åŠ è½½åŒ…ç‰ˆæœ¬ä¿¡æ¯: {benchmark_data_path}")
        
        with open(benchmark_data_path, "r") as f:
            datas = json.load(f)
        packVersions = getPackVersions(datas)
    
    # åªåœ¨rank 0ä¸ŠåŠ è½½tokenizer
    if rank == 0:
        tokenizer = AutoTokenizer.from_pretrained(model_config.get("model_name"))
        model_name = model_config.get("model_name").split("/")[-1]
        
        logging.info(f"å¼€å§‹åˆ†å¸ƒå¼è®­ç»ƒLoRAæ¨¡å‹ï¼Œæ•°æ®é›†ç±»å‹: {knowledge_type}")
        
        # é¢„è¿‡æ»¤åŠŸèƒ½ï¼ˆä»…åœ¨rank 0ä¸Šæ‰§è¡Œï¼‰
        if enable_prefilter and not pre_filtered:
            logging.info("ğŸ” å¯ç”¨é¢„è¿‡æ»¤åŠŸèƒ½ï¼Œæ£€æŸ¥è®­ç»ƒéœ€æ±‚...")
            
            original_total = sum(len(versions) for versions in packVersions.values())
            
            filtered_packVersions, prefilter_stats = apply_prefilter_to_package_versions(
                packVersions, model_config, corpus_path, knowledge_type, log_details=True
            )
            
            # æ›´æ–°åŒ…ç‰ˆæœ¬åˆ—è¡¨ä¸ºè¿‡æ»¤åçš„ç‰ˆæœ¬
            packVersions = filtered_packVersions
            
            new_total = sum(len(versions) for versions in packVersions.values())
            logging.info(f"é¢„è¿‡æ»¤å®Œæˆ: {original_total} -> {new_total} ä¸ªåŒ…ç‰ˆæœ¬éœ€è¦è®­ç»ƒ")
            
            if new_total == 0:
                logging.info("ğŸ‰ é¢„è¿‡æ»¤åæ²¡æœ‰åŒ…ç‰ˆæœ¬éœ€è¦è®­ç»ƒï¼Œæ‰€æœ‰ä»»åŠ¡å·²å®Œæˆ!")
                return {
                    'trained': 0,
                    'skipped': prefilter_stats['lora_exists'] + prefilter_stats['no_data'],
                    'failed': prefilter_stats['errors'],
                    'total': prefilter_stats['total'],
                    'prefilter_stats': prefilter_stats
                }
        else:
            if pre_filtered:
                logging.info("ä½¿ç”¨é¢„è¿‡æ»¤çš„åŒ…ç‰ˆæœ¬åˆ—è¡¨ï¼Œè·³è¿‡é¢„è¿‡æ»¤æ­¥éª¤")
            else:
                logging.info("é¢„è¿‡æ»¤åŠŸèƒ½å·²ç¦ç”¨")
        
        logging.info(f"æ€»è®¡ {len(packVersions)} ä¸ªåŒ…éœ€è¦å¤„ç†")
    else:
        tokenizer = AutoTokenizer.from_pretrained(model_config.get("model_name"))
        model_name = model_config.get("model_name").split("/")[-1]
    
    trained_count = 0
    skipped_count = 0
    error_count = 0
    
    for pkg, versions in packVersions.items():
        for version in versions:
            if pre_filtered or enable_prefilter:
                # é¢„è¿‡æ»¤æ¨¡å¼æˆ–å·²è¿‡æ»¤æ¨¡å¼ä¸‹ï¼Œè·³è¿‡å­˜åœ¨æ€§æ£€æŸ¥ï¼ˆå·²åœ¨é¢„è¿‡æ»¤ä¸­å®Œæˆï¼‰
                if rank == 0:
                    logging.info(f"å¼€å§‹è®­ç»ƒåŒ… {pkg}-{version} çš„LoRAæ¨¡å‹ï¼ˆé¢„è¿‡æ»¤æ¨¡å¼ï¼‰")
            else:
                # ä¼ ç»Ÿæ¨¡å¼ï¼Œéœ€è¦æ£€æŸ¥LoRAæ¨¡å‹æ˜¯å¦å­˜åœ¨
                if rank == 0:
                    logging.info(f"æ£€æŸ¥åŒ… {pkg}-{version} çš„LoRAæ¨¡å‹æ˜¯å¦å­˜åœ¨")
                
                if loraModelExists(pkg, version, model_name, model_config, knowledge_type=knowledge_type):
                    if rank == 0:
                        logging.info(f"åŒ… {pkg}-{version} çš„LoRAæ¨¡å‹å·²å­˜åœ¨ï¼Œè·³è¿‡è®­ç»ƒ")
                    skipped_count += 1
                    continue
            
            # è·å–è®­ç»ƒæ•°æ®ï¼ˆæ ¹æ®æ˜¯å¦å¯ç”¨é¢„è¿‡æ»¤é€‰æ‹©ä¸åŒçš„å¤„ç†æ–¹å¼ï¼‰
            if enable_prefilter:
                # é¢„è¿‡æ»¤æ¨¡å¼ï¼šç®€åŒ–çš„æ•°æ®æ£€æŸ¥ï¼Œå› ä¸ºé¢„è¿‡æ»¤å·²ç»éªŒè¯è¿‡æ•°æ®å­˜åœ¨æ€§
                try:
                    original_count, files_info = getTrainDataItems(corpus_path, pkg, version, model_config)
                    if rank == 0:
                        logging.info(f"åŒ… {pkg}-{version}: ä½¿ç”¨æ•°æ®é‡={len(files_info)} (é¢„è¿‡æ»¤æ¨¡å¼)")
                except Exception as e:
                    if rank == 0:
                        logging.error(f"è·å–è®­ç»ƒæ•°æ®å¤±è´¥ {pkg}-{version}: {e}")
                    error_count += 1
                    continue
            else:
                # ä¼ ç»Ÿæ¨¡å¼ï¼šå®Œæ•´çš„æ•°æ®æ£€æŸ¥
                original_count, files_info = getTrainDataItems(corpus_path, pkg, version, model_config)
                
                if rank == 0:
                    logging.info(f"åŒ… {pkg}-{version}: æ€»æ•°æ®é‡={original_count}, ä½¿ç”¨æ•°æ®é‡={len(files_info)} ({model_config['traindata_percentage']*100:.1f}%)")
                if len(files_info) == 0:
                    if rank == 0:
                        logging.info(f"åŒ… {pkg}-{version} çš„è®­ç»ƒæ•°æ®ä¸ºç©ºï¼Œè·³è¿‡è®­ç»ƒ")
                    skipped_count += 1
                    continue
            
            try:
                if not pre_filtered and rank == 0:
                    logging.info(f"å¼€å§‹è®­ç»ƒåŒ… {pkg}-{version} çš„LoRAæ¨¡å‹")
                
                lora_model = trainLoraModelForPack_DDP(
                    model_config, pkg, version, tokenizer, corpus_path, 
                    dataset_type=knowledge_type, precision=precision,
                    rank=rank, world_size=world_size, local_rank=local_rank,
                    gpus_per_process=gpus_per_process,
                    enable_data_checking=enable_data_checking,
                    no_save=no_save
                )
                
                # åªåœ¨rank 0ä¸Šä¿å­˜æ¨¡å‹ï¼ˆé™¤éæŒ‡å®šä¸ä¿å­˜ï¼‰
                if rank == 0 and lora_model is not None and not no_save:
                    lora_save_path = pathConfigurator().getPath(model_config, pkg, version, model_name, knowledge_type=knowledge_type)
                    lora_model.save_pretrained(lora_save_path)
                    logging.info(f"æˆåŠŸè®­ç»ƒå¹¶ä¿å­˜åŒ… {pkg}-{version} çš„LoRAæ¨¡å‹åˆ°: {lora_save_path}")
                elif rank == 0 and lora_model is not None and no_save:
                    logging.info(f"æˆåŠŸè®­ç»ƒåŒ… {pkg}-{version} çš„LoRAæ¨¡å‹ï¼ˆæœªä¿å­˜ï¼Œno-saveæ¨¡å¼ï¼‰")
                
                # æ¸…ç†æ¨¡å‹å¯¹è±¡
                if lora_model is not None:
                    del lora_model
                
                trained_count += 1
                
                # æ¯ä¸ªpackageè®­ç»ƒå®Œæˆåå¼ºåˆ¶æ¸…ç†å†…å­˜
                # comprehensive_memory_cleanup(rank, "packageå®Œæˆ", f"{pkg}-{version}")
                
                # å¦‚æœå†…å­˜ä»ç„¶æ²¡æœ‰å®Œå…¨é‡Šæ”¾ï¼Œä½¿ç”¨è®¾å¤‡é‡ç½®
                if rank == 0 and torch.cuda.is_available():
                    allocated = torch.cuda.memory_allocated(0) / (1024**3)
                    if allocated > 0.1:  # å¦‚æœè¿˜æœ‰è¶…è¿‡100MBçš„åˆ†é…å†…å­˜
                        logging.warning(f"å†…å­˜ä»æœ‰ {allocated:.2f}GB æœªé‡Šæ”¾ï¼Œæ‰§è¡Œè®¾å¤‡é‡ç½®")
                        force_memory_reset_device(rank, "packageå®Œæˆ_è®¾å¤‡é‡ç½®", f"{pkg}-{version}")
                
                # ---- NEW DIAGNOSTICS ----
                if rank == 0:
                    logging.info("--- Running Post-Cleanup Diagnostics ---")
                    # find_and_clear_lingering_tensors(rank, f"after training {pkg}-{version}")
                    # log_detailed_gpu_memory_report(rank, f"after training {pkg}-{version}")

            except Exception as e:
                if rank == 0:
                    logging.error(f"è®­ç»ƒåŒ… {pkg}-{version} æ—¶å‡ºé”™: {e}")
                error_count += 1
                
                # å³ä½¿å‡ºé”™ä¹Ÿè¦æ¸…ç†å†…å­˜
                # comprehensive_memory_cleanup(rank, "packageå‡ºé”™", f"{pkg}-{version}")
                
                # å‡ºé”™åä¹Ÿæ‰§è¡Œè®¾å¤‡é‡ç½®ï¼Œç¡®ä¿ä¸‹ä¸€ä¸ªåŒ…èƒ½æ­£å¸¸è®­ç»ƒ
                # force_memory_reset_device(rank, "packageå‡ºé”™_è®¾å¤‡é‡ç½®", f"{pkg}-{version}")

                # ---- NEW DIAGNOSTICS ----
                if rank == 0:
                    logging.info("--- Running Post-Error Diagnostics ---")
                    # find_and_clear_lingering_tensors(rank, f"after error in {pkg}-{version}")
                    log_detailed_gpu_memory_report(rank, f"after error in {pkg}-{version}")
                
                continue
    
    if rank == 0:
        logging.info(f"è®­ç»ƒå®Œæˆç»Ÿè®¡: è®­ç»ƒ={trained_count}, è·³è¿‡={skipped_count}, é”™è¯¯={error_count}")
    
    # è¿”å›ç»Ÿè®¡ä¿¡æ¯ä»¥ä¾›å¤šworkeræ¨¡å¼ä½¿ç”¨
    return {
        'trained': trained_count,
        'skipped': skipped_count,
        'failed': error_count,
        'total': trained_count + skipped_count + error_count
    }


def buildandTrainLoraModel_DDP(config, dataloader, precision='fp16', pkg=None, version=None, knowledge_type=None, rank=0, world_size=1, local_rank=0, gpus_per_process=1,no_save=False):
    """
    åˆ†å¸ƒå¼è®­ç»ƒLoRAæ¨¡å‹
    """
    # è®°å½•è®­ç»ƒå¼€å§‹å’Œé…ç½®ä¿¡æ¯
    if rank == 0:
        logging.info("=" * 60)
        logging.info(f"buildandTrainLoraModel_DDP - å¼€å§‹è®­ç»ƒ: {pkg}-{version}")
        logging.info(f"åˆ†å¸ƒå¼è®­ç»ƒå‚æ•°: rank={rank}, world_size={world_size}, local_rank={local_rank}")
        logging.info("=" * 60)
    
    model_name = config["model_name"].split("/")[-1]
    if pkg and version:
        pathConfig = pathConfigurator()
        output_adaptor_path = pathConfig.getPath(config, pkg, version, model_name, knowledge_type=knowledge_type)
        if rank == 0:
            logging.info(f"è¾“å‡ºè·¯å¾„: {output_adaptor_path}")
    else:
        if rank == 0:
            logging.error("pkgå’Œversionå¿…é¡»æä¾›")
        raise ValueError("pkg and version must be provided")
    
    # è®°å½•è®­ç»ƒå¼€å§‹å‰çš„å†…å­˜ä½¿ç”¨æƒ…å†µ
    log_gpu_memory_usage(rank, "æ–°ä¸€ä¸ªpkgVersionè®­ç»ƒå¼€å§‹å‰", f"{pkg}-{version}")
    
    try:
        # æ¸…ç†ç°æœ‰çš„GPUç¼“å­˜
        torch.cuda.empty_cache()
        log_gpu_memory_usage(rank, "æ–°ä¸€ä¸ªpkgVersionè®­ç»ƒå¼€å§‹å‰å¯¹äºGPUç¼“å­˜åå ç”¨,æœŸæœ›ä¸º0", f"{pkg}-{version}")
        # è®¡ç®—æ¯ä¸ªè¿›ç¨‹åº”è¯¥ä½¿ç”¨çš„GPUèŒƒå›´
        gpu_start = rank * gpus_per_process
        gpu_end = (rank + 1) * gpus_per_process
        process_gpu_list = list(range(gpu_start, gpu_end))
        
        if rank == 0:
            logging.info(f"è¿›ç¨‹GPUåˆ†é…: rank={rank}, gpu_start={gpu_start}, gpu_end={gpu_end}, process_gpu_list={process_gpu_list}")
        
        # æ ¹æ®æ¯ä¸ªè¿›ç¨‹çš„GPUæ•°é‡å†³å®šè®¾å¤‡æ˜ å°„ç­–ç•¥
        if gpus_per_process == 1:
            # æ¯ä¸ªè¿›ç¨‹åªä½¿ç”¨ä¸€ä¸ªGPUï¼Œä½¿ç”¨ç®€å•çš„è®¾å¤‡æ˜ å°„
            device_map = gpu_start  # ä½¿ç”¨åˆ†é…ç»™å½“å‰è¿›ç¨‹çš„GPU
            if rank == 0:
                logging.info(f"å•GPUæ¨¡å¼ï¼Œä½¿ç”¨è®¾å¤‡: cuda:{gpu_start}")
        else:
            # æ¯ä¸ªè¿›ç¨‹ä½¿ç”¨å¤šä¸ªGPUï¼Œéœ€è¦ä½¿ç”¨å¤æ‚çš„è®¾å¤‡æ˜ å°„ç­–ç•¥
            if rank == 0:
                logging.info(f"å¤šGPUæ¨¡å¼ï¼Œæ¯ä¸ªè¿›ç¨‹ä½¿ç”¨ {gpus_per_process} ä¸ªGPU: {process_gpu_list}")
            
            # è®¾ç½®CUDA_VISIBLE_DEVICESç¯å¢ƒå˜é‡ï¼Œé™åˆ¶å½“å‰è¿›ç¨‹åªèƒ½çœ‹åˆ°åˆ†é…ç»™å®ƒçš„GPU
            original_cuda_visible_devices = os.environ.get('CUDA_VISIBLE_DEVICES', '')
            gpu_ids_str = ','.join(str(gpu_id) for gpu_id in process_gpu_list)
            os.environ['CUDA_VISIBLE_DEVICES'] = gpu_ids_str
            
            if rank == 0:
                logging.info(f"è®¾ç½®CUDA_VISIBLE_DEVICES={gpu_ids_str}")
            
            use_balanced_device_map = config.get("use_balanced_device_map", False)
            
            if use_balanced_device_map:
                from utils.loraTrain.loraTrainUtils import create_balanced_device_map
                force_balance = config.get("force_balance", False)
                exclude_cpu = config.get("exclude_cpu", True)
                if rank == 0:
                    logging.info(f"å‡è¡¡è®¾å¤‡æ˜ å°„å‚æ•°: force_balance={force_balance}, exclude_cpu={exclude_cpu}")
                    print("å¯ç”¨å‡è¡¡è®¾å¤‡æ˜ å°„...")
                
                device_map = create_balanced_device_map(
                    config["model_name"],
                    force_balance=force_balance,
                    exclude_cpu=exclude_cpu
                )
                if device_map is None:
                    if rank == 0:
                        logging.warning("å‡è¡¡è®¾å¤‡æ˜ å°„å¤±è´¥ï¼Œå›é€€åˆ°autoæ¨¡å¼")
                        print("âš ï¸  å‡è¡¡è®¾å¤‡æ˜ å°„å¤±è´¥ï¼Œå›é€€åˆ°autoæ¨¡å¼")
                    device_map = "auto"
                else:
                    if rank == 0:
                        logging.info(f"å‡è¡¡è®¾å¤‡æ˜ å°„åˆ›å»ºæˆåŠŸ: {type(device_map)}")
            else:
                # ä½¿ç”¨é…ç½®ä¸­çš„è®¾å¤‡æ˜ å°„ï¼Œæ”¯æŒå¤šGPUåˆ†å¸ƒ
                device_map = config.get("device_map", "auto")
                if rank == 0:
                    logging.info(f"ä½¿ç”¨æ ‡å‡†è®¾å¤‡æ˜ å°„: {device_map}")
        
        if rank == 0:
            print(f"ä½¿ç”¨è®¾å¤‡æ˜ å°„: {device_map}")
        
        # åŠ è½½åŸºç¡€æ¨¡å‹å’Œtokenizerï¼Œä½¿ç”¨æŒ‡å®šçš„ç²¾åº¦å’Œè®¾å¤‡æ˜ å°„
        if rank == 0:
            logging.info(f"å¼€å§‹åŠ è½½åŸºç¡€æ¨¡å‹: {config['model_name']}")
            logging.info(f"ä½¿ç”¨ç²¾åº¦: {precision}")
        
        base_model, tokenizer = load_base_model(config["model_name"], device_map, precision)
        
        if rank == 0:
            logging.info("åˆ›å»ºLoRAé…ç½®...")
            logging.info(f"LoRAé…ç½®å‚æ•°: target_modules={config['target_modules']}, target_layers={config['target_layers']}, r={config['r']}, alpha={config['alpha']}")
        
        lora_config = create_lora_config(
            config["target_modules"], 
            config["target_layers"], 
            config["r"], 
            config["alpha"]
        )
        
        # åˆ›å»ºLoRAæ¨¡å‹
        if rank == 0:
            logging.info("åˆ›å»ºLoRAæ¨¡å‹...")
        
        lora_model = get_peft_model(base_model, lora_config)
        
        # å¯¹äºçœŸæ­£çš„åˆ†å¸ƒå¼è®­ç»ƒï¼ˆå¤šè¿›ç¨‹ï¼‰ï¼Œä½¿ç”¨DDPåŒ…è£…æ¨¡å‹
        if world_size > 1:
            if gpus_per_process == 1:
                # æ¯ä¸ªè¿›ç¨‹ä¸€ä¸ªGPUï¼Œä½¿ç”¨æ ‡å‡†DDPï¼Œå°†æ¨¡å‹ç§»åŠ¨åˆ°åˆ†é…ç»™å®ƒçš„GPU
                lora_model = lora_model.to(f'cuda:{gpu_start}')
                
                lora_model = DDP(
                    lora_model,
                    device_ids=[gpu_start],
                    output_device=gpu_start,
                    find_unused_parameters=False
                )
                
                if rank == 0:
                    logging.info(f"æ¨¡å‹å·²ç”¨DDPåŒ…è£…ï¼Œè®¾å¤‡: cuda:{gpu_start}")
            else:
                # æ¯ä¸ªè¿›ç¨‹å¤šä¸ªGPUï¼Œæ¨¡å‹å·²ç»é€šè¿‡device_mapåˆ†å¸ƒåˆ°å¤šä¸ªGPU
                # ä¸ä½¿ç”¨DDPï¼Œå› ä¸ºæ¨¡å‹å·²ç»åœ¨å¤šä¸ªGPUä¸Šåˆ†å¸ƒ
                if rank == 0:
                    logging.info(f"å¤šGPUæ¨¡å¼ï¼Œæ¨¡å‹å·²é€šè¿‡device_mapåˆ†å¸ƒåˆ°GPU {process_gpu_list}ï¼Œä¸ä½¿ç”¨DDPåŒ…è£…")
        else:
            # å•è¿›ç¨‹è®­ç»ƒï¼Œä¸éœ€è¦DDPåŒ…è£…
            if rank == 0:
                logging.info("å•è¿›ç¨‹è®­ç»ƒï¼Œä¸ä½¿ç”¨DDPåŒ…è£…")
        
        # åˆ†ææ¨¡å‹å‚æ•°åˆ†å¸ƒï¼ˆä»…åœ¨rank 0ä¸Šï¼‰
        if rank == 0:
            devices_found = {}
            model_for_analysis = lora_model.module if hasattr(lora_model, 'module') else lora_model
            
            for name, param in model_for_analysis.named_parameters():
                device = param.device
                if device not in devices_found:
                    devices_found[device] = []
                devices_found[device].append(name)
            
            logging.info("è®­ç»ƒæ¨¡å‹å‚æ•°åˆ†å¸ƒ:")
            print("è®­ç»ƒæ¨¡å‹å‚æ•°åˆ†å¸ƒ:")
            for device, params in devices_found.items():
                device_info = f"{device}: {len(params)} parameters"
                logging.info(f"- {device_info}")
                print(f"- {device_info}")
        
        # è®­ç»ƒæ¨¡å‹
        if rank == 0:
            logging.info("å¼€å§‹åˆ†å¸ƒå¼è®­ç»ƒæ¨¡å‹")
            logging.info(f"è®­ç»ƒå‚æ•°: epochs={config['num_epochs']}, lr={config['learning_rate']}")
            print(f"å¼€å§‹åˆ†å¸ƒå¼è®­ç»ƒæ¨¡å‹")
        
        lora_model = train_lora_model_DDP(
            lora_model, 
            dataloader, 
            config["num_epochs"], 
            config["learning_rate"],
            config,
            output_adaptor_path,
            rank=rank,
            world_size=world_size,
            local_rank=local_rank,
            gpus_per_process=gpus_per_process,
            process_gpu_list=process_gpu_list,
            no_save=no_save
        )
        
        # æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒèµ„æº
        if world_size > 1:
            dist.barrier()  # ç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½å®Œæˆè®­ç»ƒ
            
            # æå–åŸå§‹æ¨¡å‹ï¼ˆå»æ‰DDPåŒ…è£…ï¼‰
            if hasattr(lora_model, 'module'):
                lora_model = lora_model.module
        
        # å°†æ¨¡å‹ç§»åˆ°CPUå¹¶åˆ†ç¦»è®¡ç®—å›¾ï¼ˆä»…åœ¨rank 0ä¸Šï¼‰
        if rank == 0:
            logging.info("è®­ç»ƒå®Œæˆï¼Œå°†æ¨¡å‹ç§»åŠ¨åˆ°CPU...")
            lora_model = lora_model.cpu()
            for param in lora_model.parameters():
                param.requires_grad = False
        
        # æ¢å¤åŸå§‹çš„CUDA_VISIBLE_DEVICESç¯å¢ƒå˜é‡
        if gpus_per_process > 1:
            if original_cuda_visible_devices:
                os.environ['CUDA_VISIBLE_DEVICES'] = original_cuda_visible_devices
            else:
                del os.environ['CUDA_VISIBLE_DEVICES']
        
        # æ¸…ç†ä¸éœ€è¦çš„å¯¹è±¡
        if rank == 0:
            logging.info("æ¸…ç†è®­ç»ƒèµ„æº...")
        
        del base_model
        del tokenizer
        
        # ä½¿ç”¨æ–°çš„æ¸…ç†å‡½æ•°
        # comprehensive_memory_cleanup(rank, "è®­ç»ƒå®Œæˆå", f"{pkg}-{version}")
        
        if rank == 0:
            logging.info(f"è®­ç»ƒå®Œæˆ: {pkg}-{version}")
            logging.info("=" * 60)
        
        return lora_model if rank == 0 else None
        
    except Exception as e:
        if rank == 0:
            logging.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
            logging.error("é”™è¯¯è¯¦æƒ…:")
            import traceback
            logging.error(traceback.format_exc())
            
            print(f"åˆ›å»ºLoRAæ¨¡å‹æ—¶å‡ºé”™: {str(e)}")
            traceback.print_exc()
        
        # ç¡®ä¿æ¸…ç†æ‰€æœ‰å¯èƒ½çš„èµ„æº
        if 'base_model' in locals():
            del base_model
        if 'tokenizer' in locals():
            del tokenizer
        if 'lora_model' in locals():
            del lora_model
        
        # ä½¿ç”¨æ–°çš„æ¸…ç†å‡½æ•°
        comprehensive_memory_cleanup(rank, "å¼‚å¸¸å¤„ç†", f"{pkg}-{version}")
        
        # å¼‚å¸¸æƒ…å†µä¸‹ä¹Ÿæ‰§è¡Œè®¾å¤‡é‡ç½®
        force_memory_reset_device(rank, "å¼‚å¸¸å¤„ç†_è®¾å¤‡é‡ç½®", f"{pkg}-{version}")
        
        raise e


def train_lora_model_DDP(lora_model, dataloader, num_epochs=10, learning_rate=1e-3, train_config=None, output_adaptor_path=None, rank=0, world_size=1, local_rank=0, gpus_per_process=1, process_gpu_list=None,no_save=False):
    """
    åˆ†å¸ƒå¼è®­ç»ƒLoRAæ¨¡å‹ï¼ˆå¸¦æ•°æ®åˆ†å¸ƒæ£€æŸ¥ï¼‰
    """
    from llmfoundry.optim import DecoupledLionW
    import os
    
    # å¦‚æœæ²¡æœ‰æä¾›process_gpu_listï¼Œåˆ™æ ¹æ®rankå’Œgpus_per_processè®¡ç®—
    if process_gpu_list is None:
        gpu_start = rank * gpus_per_process
        gpu_end = (rank + 1) * gpus_per_process
        process_gpu_list = list(range(gpu_start, gpu_end))
        
        if rank == 0:
            print(f"train_lora_model_DDP: è®¡ç®—è¿›ç¨‹GPUåˆ—è¡¨: {process_gpu_list}")
    
    if rank == 0:
        print(f"train_lora_model_DDP: ä½¿ç”¨GPUåˆ—è¡¨: {process_gpu_list}")
    
    
    # å°è¯•è®¾ç½®matplotlibåç«¯ï¼Œå¦‚æœå¤±è´¥åˆ™ç¦ç”¨ç»˜å›¾åŠŸèƒ½
    try:

        matplotlib.use('Agg')  # è®¾ç½®ä¸ºéäº¤äº’å¼åç«¯
        plotting_enabled = True
    except Exception:
        if rank == 0:
            print("è­¦å‘Šï¼šæ— æ³•è®¾ç½®matplotlibåç«¯ï¼ŒæŸå¤±æ›²çº¿ç»˜å›¾åŠŸèƒ½å°†è¢«ç¦ç”¨ã€‚")
        plotting_enabled = False
    
    # ä½¿ç”¨ DecoupledLionW ä¼˜åŒ–å™¨
    optimizer = DecoupledLionW(
        lora_model.parameters(),
        lr=float(learning_rate),
        betas=(0.9, 0.95),
        weight_decay=1e-6
    )
    
    # æ¢¯åº¦ç´¯ç§¯è®¾ç½®
    actual_batch_size = dataloader.batch_size
    target_batch_size = 16  # é»˜è®¤ç›®æ ‡æ‰¹æ¬¡å¤§å°
    
    # å¦‚æœæä¾›äº†configï¼Œä»ä¸­è¯»å–ç›®æ ‡æ‰¹æ¬¡å¤§å°
    if train_config and "target_batch_size" in train_config:
        target_batch_size = train_config["target_batch_size"]
        if rank == 0:
            print(f"ä»é…ç½®ä¸­è¯»å–ç›®æ ‡æ‰¹æ¬¡å¤§å°: {target_batch_size}")
    
    accumulation_steps = max(1, target_batch_size // actual_batch_size)
    if rank == 0:
        print(f"ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯: å®é™…æ‰¹æ¬¡å¤§å°={actual_batch_size}, ç›®æ ‡æ‰¹æ¬¡å¤§å°={target_batch_size}, ç´¯ç§¯æ­¥æ•°={accumulation_steps}")
    
    # åˆ›å»ºä¿å­˜æ—¥å¿—å’Œæ¨¡å‹çš„ç›®å½•ï¼ˆä»…åœ¨rank 0ä¸Šï¼‰
    if rank == 0:
        save_path_base = train_config.get("save_path_base", "/datanfs2/chenrongyi/models/versiBCB") if train_config else "/datanfs2/chenrongyi/models/versiBCB"
        log_dir = os.path.join(output_adaptor_path, "training_logs")
        checkpoint_dir = os.path.join(output_adaptor_path, "checkpoints")
        os.makedirs(log_dir, exist_ok=True)
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        # ä¸ºæœ¬æ¬¡è®­ç»ƒåˆ›å»ºå”¯ä¸€çš„æ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # åˆ›å»ºæ—¥å¿—æ–‡ä»¶è·¯å¾„å’Œcheckpointè·¯å¾„åŸºç¡€
        log_file = os.path.join(log_dir, f"training_log_{timestamp}_rank{rank}.csv")
        log_plot = os.path.join(log_dir, f"loss_curve_{timestamp}_rank{rank}.png")
        checkpoint_base = os.path.join(checkpoint_dir, f"checkpoint_{timestamp}")
        
        # åˆå§‹åŒ–æŸå¤±è®°å½•
        epoch_losses = []
        step_losses = []
        batch_losses = []
        
        # å†™å…¥æ—¥å¿—æ–‡ä»¶å¤´
        try:
            with open(log_file, "w") as f:
                f.write("epoch,step,batch,loss\n")
        except Exception as e:
            if rank == 0:
                print(f"Error writing to log file: {e}")
    
    # è·å–ç²¾åº¦è®¾ç½®
    
    for epoch in range(num_epochs):
        total_loss = 0
        batch_count = 0
        valid_batch_count = 0
        step_count = 0
        
        # è®¾ç½®åˆ†å¸ƒå¼é‡‡æ ·å™¨çš„epochï¼ˆå¯¹äºshuffleï¼‰
        if hasattr(dataloader, 'sampler') and hasattr(dataloader.sampler, 'set_epoch'):
            dataloader.sampler.set_epoch(epoch)
            
            # æ£€æŸ¥é‡‡æ ·å™¨ç´¢å¼•åˆ†å¸ƒ
            # if data_checker: # This line was removed as per the new_code, as data_checker is no longer passed
            #     data_checker.check_sampler_indices(dataloader.sampler, epoch + 1, batch_count)
        
        # åœ¨æ¯ä¸ªepochå¼€å§‹å‰æ¸…é›¶æ¢¯åº¦
        optimizer.zero_grad()
        
        epoch_batch_count = 0
        for batch in dataloader:
            epoch_batch_count += 1
            
            # æå–batchæ•°æ®
            inputs = batch["input_ids"]
            labels = batch["labels"]
            attention_mask = batch["attention_mask"]
            
            # å°†æ•°æ®ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
            if gpus_per_process == 1:
                # å•GPUæ¨¡å¼ï¼Œç§»åŠ¨åˆ°åˆ†é…ç»™å½“å‰è¿›ç¨‹çš„GPU
                target_device = f'cuda:{process_gpu_list[0]}' if process_gpu_list else f'cuda:{local_rank}'
                inputs = inputs.to(target_device)
                labels = labels.to(target_device)
                attention_mask = attention_mask.to(target_device)
            else:
                # å¤šGPUæ¨¡å¼ï¼Œç§»åŠ¨åˆ°è¿›ç¨‹çš„ç¬¬ä¸€ä¸ªGPU
                # åœ¨è®¾ç½®äº†CUDA_VISIBLE_DEVICESåï¼Œè¿™é‡Œä½¿ç”¨cuda:0æŒ‡å‘è¿›ç¨‹çš„ç¬¬ä¸€ä¸ªGPU
                target_device = 'cuda:0'
                inputs = inputs.to(target_device)
                labels = labels.to(target_device)
                attention_mask = attention_mask.to(target_device)
            
            try:
                # å‰å‘ä¼ æ’­
                # with torch.amp.autocast(device_type='cuda', dtype=torch_dtype, enabled=torch.cuda.is_available()):
                outputs = lora_model(
                    input_ids=inputs,
                    attention_mask=attention_mask,
                    labels=labels
                )
                loss = outputs.loss
                loss = loss / accumulation_steps
                
                # ç«‹å³æå–losså€¼ï¼Œç„¶åæ¸…ç†outputså¯¹è±¡
                current_loss = loss.item() * accumulation_steps
                
                # æ¸…ç†æ¨¡å‹è¾“å‡ºå¯¹è±¡ï¼Œé¿å…æ®‹ç•™
                if hasattr(outputs, 'logits'):
                    outputs.logits = None
                if hasattr(outputs, 'past_key_values'):
                    outputs.past_key_values = None
                if hasattr(outputs, 'hidden_states'):
                    outputs.hidden_states = None
                if hasattr(outputs, 'attentions'):
                    outputs.attentions = None
                del outputs
                
                # åå‘ä¼ æ’­
                loss.backward()
                
                # æ£€æŸ¥lossæ˜¯å¦ä¸ºNaNï¼Œåªæœ‰éNaNå€¼æ‰è®¡å…¥å¹³å‡å€¼
                if not torch.isnan(loss) and not torch.isinf(loss):
                    total_loss += current_loss
                    valid_batch_count += 1
                    
                    # åªåœ¨rank 0ä¸Šè®°å½•æŸå¤±
                    if rank == 0:
                        batch_losses.append(current_loss)
                else:
                    if rank == 0:
                        print(f"Warning: NaN or Inf loss detected in Epoch {epoch+1}, Batch {batch_count+1}. Ignoring this batch for average calculation.")
                
                batch_count += 1
                step_count += 1
                
                # æ¯ä¸ªbatchéƒ½è¾“å‡ºå½“å‰batchçš„lossï¼ˆä»…åœ¨rank 0ä¸Šï¼‰
                if rank == 0 and batch_count % 10 == 0:
                    print(f"Epoch {epoch+1}/{num_epochs}, Batch {batch_count}/{len(dataloader)}, Current Batch Loss: {current_loss:.4f}")
                
                # æ¯accumulation_stepsæ­¥æ›´æ–°ä¸€æ¬¡å‚æ•°
                if step_count % accumulation_steps == 0 or batch_count == len(dataloader):
                    # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
                    torch.nn.utils.clip_grad_norm_(lora_model.parameters(), max_norm=1.0)
                    
                    # å‚æ•°æ›´æ–°
                    optimizer.step()
                    optimizer.zero_grad()
                    
                    # æ¯éš”å‡ æ­¥æ¸…ç†ä¸€æ¬¡ç¼“å­˜ï¼Œé¿å…attentionç¼“å­˜ç§¯ç´¯
                    if step_count % (accumulation_steps * 10) == 0:
                        torch.cuda.empty_cache()
                    
                    # è®¡ç®—å½“å‰æ­¥éª¤çš„å¹³å‡æŸå¤±ï¼Œåªä½¿ç”¨æœ‰æ•ˆçš„batch
                    if valid_batch_count > 0:
                        avg_loss = total_loss / valid_batch_count
                    else:
                        avg_loss = float('nan')
                    
                    # åªåœ¨rank 0ä¸Šè®°å½•å’Œè¾“å‡º
                    if rank == 0:
                        if not torch.isnan(torch.tensor(avg_loss)) and not torch.isinf(torch.tensor(avg_loss)):
                            step_losses.append(avg_loss)
                        
                        # è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶
                        with open(log_file, "a") as f:
                            f.write(f"{epoch+1},{step_count // accumulation_steps},{batch_count},{avg_loss:.6f}\n")
                        
                        # æ¯2ä¸ªæ›´æ–°æ­¥éª¤è¾“å‡ºä¸€æ¬¡å½“å‰avg loss
                        if (step_count // accumulation_steps) % 2 == 0:
                            print(f"Epoch {epoch+1}/{num_epochs}, Step {step_count // accumulation_steps}, Batch {batch_count}/{len(dataloader)}, Avg Loss: {avg_loss:.4f}")
                
            except RuntimeError as e:
                if "expected all tensors to be on the same device" in str(e).lower():
                    if rank == 0:
                        print(f"è®¾å¤‡ä¸ä¸€è‡´é”™è¯¯: {e}")
                        print("è·³è¿‡æ­¤batchå¹¶ç»§ç»­è®­ç»ƒ...")
                    
                    batch_count += 1
                    step_count += 1
                    optimizer.zero_grad()
                    continue
                elif "out of memory" in str(e).lower():
                    if rank == 0:
                        print(f"GPUå†…å­˜ä¸è¶³: {e}")
                        print("æ¸…ç†ç¼“å­˜å¹¶è·³è¿‡æ­¤batch...")
                    torch.cuda.empty_cache()
                    
                    batch_count += 1
                    step_count += 1
                    optimizer.zero_grad()
                    continue
                else:
                    if rank == 0:
                        print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°RuntimeError: {e}")
                        import traceback
                        traceback.print_exc()
                    raise e
        
        # åœ¨epochç»“æŸæ—¶æ£€æŸ¥batchå¤„ç†æ•°é‡
        if rank == 0:
            print(f"Rank {rank}: Epoch {epoch + 1} å¤„ç†äº† {epoch_batch_count} ä¸ªbatches")
        
        # æ”¶é›†æ‰€æœ‰rankçš„batchå¤„ç†æ•°é‡
        if world_size > 1:
            batch_counts = torch.tensor([epoch_batch_count], dtype=torch.long).cuda()
            all_batch_counts = [torch.zeros(1, dtype=torch.long).cuda() for _ in range(world_size)]
            dist.all_gather(all_batch_counts, batch_counts)
            
            if rank == 0:
                # ä»å¼ é‡åˆ—è¡¨ä¸­æå–æ•°å€¼
                batch_counts_list = [tensor.cpu().item() for tensor in all_batch_counts]
                print(f"å„rankæœ¬epochå¤„ç†çš„batchæ•°: {batch_counts_list}")
                
                if len(set(batch_counts_list)) == 1:
                    print(f"âœ… æ‰€æœ‰rankå¤„ç†äº†ç›¸åŒæ•°é‡çš„batch: {batch_counts_list[0]}")
                else:
                    print(f"âŒ ä¸åŒrankå¤„ç†çš„batchæ•°é‡ä¸åŒ: {batch_counts_list}")
        
        # ä½¿ç”¨æ›´å¥å£®çš„åŒæ­¥ï¼Œç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½åˆ°è¾¾è¿™ä¸ªç‚¹
        print(f"rank {rank} è®­ç»ƒè¿›å…¥åŒæ­¥ç‚¹å‰")
        if world_size > 1:
            if rank == 0:
                print("æ‰€æœ‰rankçš„epochè®­ç»ƒå¾ªç¯å®Œæˆï¼Œå‡†å¤‡è¿›å…¥åŒæ­¥ç‚¹...")
            dist.barrier()
            if rank == 0:
                print("æ‰€æœ‰rankå·²æˆåŠŸåŒæ­¥ã€‚")
        
        # è®¡ç®—å¹¶å­˜å‚¨æ­¤epochçš„å¹³å‡æŸå¤±ï¼ˆä»…åœ¨rank 0ä¸Šï¼‰
        if rank == 0:
            avg_epoch_loss = total_loss / max(1, valid_batch_count)
            epoch_losses.append(avg_epoch_loss)
            print(f"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_epoch_loss:.4f} (åŸºäº {valid_batch_count} ä¸ªæœ‰æ•ˆbatch)")
            
            # æ¯2ä¸ªepochä¿å­˜ä¸€æ¬¡æ¨¡å‹
            if not no_save:
                if (epoch + 1) % 2 == 0 or epoch == num_epochs - 1:
                    checkpoint_path = f"{checkpoint_base}_epoch{epoch+1}"
                    os.makedirs(checkpoint_path, exist_ok=True)
                    print(f"ä¿å­˜ä¸­é—´æ£€æŸ¥ç‚¹åˆ° {checkpoint_path}")
                    
                    # ä»DDPä¸­æå–åŸå§‹æ¨¡å‹è¿›è¡Œä¿å­˜
                    model_to_save = lora_model.module if hasattr(lora_model, 'module') else lora_model
                    model_to_save.save_pretrained(checkpoint_path)
                    
                    # ä¿å­˜æœ€æ–°çš„æŸå¤±æ›²çº¿å›¾
                    if plotting_enabled:
                        save_training_plots(epoch_losses, step_losses, batch_losses, log_plot, is_final=False)
        
    # è®­ç»ƒç»“æŸï¼Œä¿å­˜æœ€ç»ˆçš„æŸå¤±æ›²çº¿å’Œç»Ÿè®¡æ‘˜è¦ï¼ˆä»…åœ¨rank 0ä¸Šï¼‰
    if rank == 0:
        if plotting_enabled:
            save_training_plots(epoch_losses, step_losses, batch_losses, log_plot, is_final=True)
        else:
            print("è®­ç»ƒå®Œæˆã€‚ç”±äºmatplotlibä¸å¯ç”¨ï¼ŒæŸå¤±æ›²çº¿æœªç»˜åˆ¶ã€‚")
        
        # æ‰“å°è®­ç»ƒæŸå¤±ç»Ÿè®¡æ‘˜è¦
        if epoch_losses or step_losses or batch_losses:
            summary = create_loss_summary(epoch_losses, step_losses, batch_losses)
            print_loss_summary(summary,"è®­ç»ƒå®Œæˆ")
    
    # æ¸…ç†è®­ç»ƒèµ„æº
    if rank == 0:
        logging.info("æ¸…ç†è®­ç»ƒèµ„æº...")
    
    # æ¸…ç†ä¼˜åŒ–å™¨çŠ¶æ€
    if 'optimizer' in locals():
        clear_optimizer_states(optimizer, rank)
        del optimizer
    
    # æ¸…ç†æ•°æ®åŠ è½½å™¨
    if 'dataloader' in locals():
        del dataloader
    
    # æ¸…ç†æŸå¤±è®°å½•
    if 'epoch_losses' in locals():
        del epoch_losses
    if 'step_losses' in locals():
        del step_losses
    if 'batch_losses' in locals():
        del batch_losses
    
    # ä½¿ç”¨æ–°çš„æ¸…ç†å‡½æ•°
    comprehensive_memory_cleanup(rank, "è®­ç»ƒå‡½æ•°ç»“æŸ", "")
    
    return lora_model


def main():
    """ä¸»å‡½æ•°"""
    # åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ
    rank, world_size, local_rank = init_distributed_training()
    
    model_config = load_config(LORA_CONFIG_PATH)
    args = argparse.ArgumentParser()
    
    # åŸæœ‰å‚æ•°
    args.add_argument("--precision", type=str, default="bf16", help="precision of the model", choices=["fp16", "fp32", "bf16"])
    args.add_argument("--dataset_type", type=str, default="docstring", help="dataset type", choices=["docstring", "srccodes"])
    args.add_argument("--corpus_path", type=str, default="/datanfs4/chenrongyi/data/docs", help="corpus path")
    args.add_argument("--benchmark_data_path", type=str, default="data/VersiBCB_Benchmark/vace_datas.json", help="benchmark data path")
    args.add_argument("--benchmark_paths", type=str, nargs='+', default=None, help="multiple benchmark data paths")
    args.add_argument("--loraadaptor_save_path_base", type=str, default="/datanfs4/chenrongyi/models/loraadaptors/", help="lora adaptor save path base")
    args.add_argument("--model_name", type=str, default="/datanfs2/chenrongyi/models/Llama-3.1-8B-Instruct", help="model name")
    args.add_argument("--log_dir", type=str, default=None, help="æŒ‡å®šæ—¥å¿—ç›®å½•")
    args.add_argument("--override_data_percentage", type=str, default=None, help="override the data percentage")
    
    # è®¾å¤‡æ˜ å°„å‚æ•°
    args.add_argument("--use_balanced_device_map", type=bool, default=False, help="æ˜¯å¦ä½¿ç”¨å‡è¡¡è®¾å¤‡æ˜ å°„ï¼ˆåˆ†å¸ƒå¼è®­ç»ƒæ—¶å»ºè®®å…³é—­ï¼‰")
    args.add_argument("--force_balance", type=bool, default=True, help="æ˜¯å¦å¼ºåˆ¶å‡è¡¡åˆ†é…")
    args.add_argument("--exclude_cpu", type=bool, default=True, help="æ˜¯å¦æ’é™¤CPUè®¾å¤‡")
    args.add_argument("--check_r_consistency", type=bool, default=True, help="æ˜¯å¦æ£€æŸ¥rå€¼ä¸€è‡´æ€§")
    args.add_argument("--strict_r_check", type=bool, default=False, help="æ˜¯å¦ä¸¥æ ¼æ£€æŸ¥rå€¼ä¸€è‡´æ€§")
    args.add_argument("--use_dynamic_device_map", type=bool, default=False, help="æ˜¯å¦ä½¿ç”¨åŠ¨æ€è®¾å¤‡æ˜ å°„ç­–ç•¥")
    args.add_argument("--balance_threshold", type=float, default=0.3, help="åŠ¨æ€æ˜ å°„çš„å‡è¡¡é˜ˆå€¼")
    args.add_argument("--device_map_strategy", type=str, default="auto", choices=["auto", "balanced", "dynamic"], help="è®¾å¤‡æ˜ å°„ç­–ç•¥é€‰æ‹©")
    args.add_argument("--gpus_per_process", type=int, default=1, help="æ¯ä¸ªè¿›ç¨‹ä½¿ç”¨çš„GPUæ•°é‡")
    
    # æ·»åŠ æ•°æ®æ£€æŸ¥å‚æ•°
    args.add_argument("--enable_data_checking", action="store_true", help="å¯ç”¨æ•°æ®åˆ†å¸ƒæ­£äº¤æ€§æ£€æŸ¥")
    args.add_argument("--data_check_interval", type=int, default=10, help="æ•°æ®æ£€æŸ¥é—´éš”ï¼ˆbatchæ•°ï¼‰")
    
    # æ·»åŠ æ¨¡å‹ä¿å­˜æ§åˆ¶å‚æ•°
    args.add_argument("--no_save", action="store_true", help="è®­ç»ƒå®Œæˆåä¸ä¿å­˜æ¨¡å‹")
    
    # æ·»åŠ å•åŒ…è®­ç»ƒå‚æ•°
    args.add_argument("--single_package", type=str, default=None, help="è®­ç»ƒå•ä¸ªåŒ…ç‰ˆæœ¬ï¼Œæ ¼å¼: pkg:version")
    
    # æ·»åŠ é¢„è¿‡æ»¤å‚æ•°
    args.add_argument("--enable_prefilter", action="store_true", help="å¯ç”¨é¢„è¿‡æ»¤åŠŸèƒ½ï¼Œæå‰è¿‡æ»¤æ‰ä¸éœ€è¦è®­ç»ƒçš„åŒ…ç‰ˆæœ¬")
    
    args = args.parse_args()
    
    # è®¾ç½®æ—¥å¿—ï¼ˆä»…åœ¨rank 0ä¸Šè¾“å‡ºè¯¦ç»†ä¿¡æ¯ï¼‰
    if rank == 0:
        log_dir = setup_logging(args)
        logging.info(f"åˆ†å¸ƒå¼è®­ç»ƒåˆå§‹åŒ–å®Œæˆ - Rank: {rank}, World Size: {world_size}, Local Rank: {local_rank}")
    
    # è¯¦ç»†çš„CUDAç¯å¢ƒæ£€æµ‹å’Œè¯Šæ–­ï¼ˆä»…åœ¨rank 0ä¸Šï¼‰
    if rank == 0:
        logging.info("=== CUDAç¯å¢ƒè¯Šæ–­ ===")
        cuda_devices = os.environ.get('CUDA_VISIBLE_DEVICES', 'Not set')
        logging.info(f"CUDA_VISIBLE_DEVICES={cuda_devices}")
        
        # æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
        logging.info(f"torch.cuda.is_available(): {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            visible_gpu_count = torch.cuda.device_count()
            logging.info(f"PyTorchå¯è§GPUæ•°é‡: {visible_gpu_count}")
            
            if visible_gpu_count > 0:
                for i in range(visible_gpu_count):
                    gpu_name = torch.cuda.get_device_properties(i).name
                    total_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)
                    logging.info(f"GPU {i}: {gpu_name} ({total_memory:.1f}GB)")
        
        logging.info("=== é…ç½®ä¿¡æ¯ ===")
        logging.info(f"åˆ†å¸ƒå¼è®­ç»ƒ: rank={rank}, world_size={world_size}, local_rank={local_rank}")
        logging.info(f"æ¯ä¸ªè¿›ç¨‹GPUæ•°é‡: {args.gpus_per_process}")
    
    # è¦†ç›–åŸºç¡€é…ç½®
    model_config["precision"] = args.precision
    model_config["knowledge_type"] = args.dataset_type
    model_config["corpus_path"] = args.corpus_path
    model_config["benchmark_data_path"] = args.benchmark_data_path
    model_config["loraadaptor_save_path_base"] = args.loraadaptor_save_path_base
    model_config["model_name"] = args.model_name
    model_config["override_data_percentage"] = args.override_data_percentage
    model_config["use_balanced_device_map"] = args.use_balanced_device_map
    model_config["force_balance"] = args.force_balance
    model_config["exclude_cpu"] = args.exclude_cpu
    model_config["check_r_consistency"] = args.check_r_consistency
    model_config["strict_r_check"] = args.strict_r_check
    model_config["use_dynamic_device_map"] = args.use_dynamic_device_map
    model_config["balance_threshold"] = args.balance_threshold
    
    # æ ¹æ®device_map_strategyè‡ªåŠ¨è®¾ç½®æ˜ å°„å‚æ•°
    if args.device_map_strategy == "dynamic":
        model_config["use_dynamic_device_map"] = True
        model_config["use_balanced_device_map"] = False
    elif args.device_map_strategy == "balanced":
        model_config["use_dynamic_device_map"] = False
        model_config["use_balanced_device_map"] = True
    elif args.device_map_strategy == "auto":
        model_config["use_dynamic_device_map"] = False
        model_config["use_balanced_device_map"] = False
        model_config["device_map"] = "auto"
    
    # ç¡®å®šä½¿ç”¨å“ªç§æ–¹å¼åŠ è½½åŒ…ç‰ˆæœ¬ä¿¡æ¯
    pack_versions = None
    benchmark_data_path = None
    
    if args.single_package:
        # å•åŒ…è®­ç»ƒæ¨¡å¼
        if rank == 0:
            logging.info("=== å•åŒ…è®­ç»ƒæ¨¡å¼ ===")
            logging.info(f"è®­ç»ƒåŒ…: {args.single_package}")
        
        try:
            pkg, version = args.single_package.split(":")
            pack_versions = {pkg: [version]}
            if rank == 0:
                logging.info(f"å•åŒ…è®­ç»ƒ: {pkg}-{version}")
        except ValueError:
            if rank == 0:
                logging.error(f"å•åŒ…å‚æ•°æ ¼å¼é”™è¯¯: {args.single_package}ï¼Œåº”ä¸º pkg:version")
            sys.exit(1)
    
    elif args.benchmark_paths is not None:
        # ä½¿ç”¨å¤šä¸ªbenchmarkæ–‡ä»¶
        if rank == 0:
            logging.info("=== ä½¿ç”¨å¤šä¸ªbenchmarkæ–‡ä»¶æ¨¡å¼ ===")
            logging.info(f"Benchmarkæ–‡ä»¶åˆ—è¡¨: {args.benchmark_paths}")
        pack_versions = load_and_aggregate_package_versions(args.benchmark_paths)
    else:
        # ä½¿ç”¨å•ä¸ªbenchmarkæ–‡ä»¶
        if rank == 0:
            logging.info("=== ä½¿ç”¨å•ä¸ªbenchmarkæ–‡ä»¶æ¨¡å¼ ===")
            logging.info(f"Benchmarkæ–‡ä»¶: {args.benchmark_data_path}")
        benchmark_data_path = args.benchmark_data_path
    
    # æ•°æ®åˆ†å‰²ï¼ˆå¦‚æœæ˜¯å¤šè¿›ç¨‹è®­ç»ƒä¸”ä¸æ˜¯å•åŒ…æ¨¡å¼ï¼‰
    if pack_versions is not None and world_size > 1 and not args.single_package:
        if rank == 0:
            logging.info(f"åˆ†å¸ƒå¼è®­ç»ƒæ¨¡å¼: å°†æ•°æ®åˆ†å‰²ç»™ {world_size} ä¸ªè¿›ç¨‹")
        
        # æ”¶é›†æ‰€æœ‰åŒ…ç‰ˆæœ¬ç»„åˆ
        all_pkg_versions = []
        for pkg, versions in pack_versions.items():
            for version in versions:
                all_pkg_versions.append((pkg, version))
        
        # æŒ‰rankåˆ†å‰²æ•°æ®
        total_combinations = len(all_pkg_versions)
        combinations_per_rank = total_combinations // world_size
        remainder = total_combinations % world_size
        
        start_idx = rank * combinations_per_rank + min(rank, remainder)
        end_idx = start_idx + combinations_per_rank + (1 if rank < remainder else 0)
        
        rank_pkg_versions = all_pkg_versions[start_idx:end_idx]
        
        if rank == 0:
            logging.info(f"æ•°æ®åˆ†å‰²å®Œæˆ: æ€»è®¡ {total_combinations} ä¸ªåŒ…ç‰ˆæœ¬ç»„åˆ")
            logging.info(f"æ¯ä¸ªè¿›ç¨‹åˆ†é…: {combinations_per_rank}+{1 if rank < remainder else 0} ä¸ªç»„åˆ")
        
        # é‡æ–°æ„å»ºå½“å‰rankçš„pack_versionså­—å…¸
        rank_pack_versions = {}
        for pkg, version in rank_pkg_versions:
            if pkg not in rank_pack_versions:
                rank_pack_versions[pkg] = []
            rank_pack_versions[pkg].append(version)
        
        pack_versions = rank_pack_versions
        
        if rank == 0:
            logging.info(f"Rank {rank} åˆ†é…åˆ° {len(pack_versions)} ä¸ªåŒ…ï¼Œå…± {sum(len(versions) for versions in pack_versions.values())} ä¸ªåŒ…ç‰ˆæœ¬ç»„åˆ")
    
    if rank == 0:
        if args.no_save:
            logging.info("å¼€å§‹åˆ†å¸ƒå¼LoRAæ¨¡å‹è®­ç»ƒï¼ˆno-saveæ¨¡å¼ï¼Œè®­ç»ƒå®Œæˆåä¸ä¿å­˜æ¨¡å‹ï¼‰...")
        else:
            logging.info("å¼€å§‹åˆ†å¸ƒå¼LoRAæ¨¡å‹è®­ç»ƒ...")
    
    try:
        stats = trainLoraModelsForVersiBCB_DDP(
            benchmark_data_path=benchmark_data_path,
            model_config=model_config,
            precision=args.precision,
            knowledge_type=args.dataset_type,
            corpus_path=args.corpus_path,
            pack_versions=pack_versions,
            pre_filtered=pack_versions is not None and world_size > 1 and not args.single_package,
            rank=rank,
            world_size=world_size,
            local_rank=local_rank,
            gpus_per_process=args.gpus_per_process,
            enable_data_checking=args.enable_data_checking,
            no_save=args.no_save,
            enable_prefilter=args.enable_prefilter
        )
        
        if rank == 0:
            logging.info("åˆ†å¸ƒå¼LoRAæ¨¡å‹è®­ç»ƒæˆåŠŸå®Œæˆ!")
            logging.info(f"æœ€ç»ˆç»Ÿè®¡: è®­ç»ƒ={stats['trained']}, è·³è¿‡={stats['skipped']}, é”™è¯¯={stats['failed']}, æ€»è®¡={stats['total']}")
            logging.info(f"æ‰€æœ‰æ—¥å¿—å·²ä¿å­˜åˆ°: {log_dir}")
        if args.single_package:
            if stats['trained'] > 0:
                logging.info(f"å•åŒ…è®­ç»ƒæˆåŠŸå®Œæˆ! è®­ç»ƒäº† {stats['trained']} ä¸ªåŒ…ç‰ˆæœ¬")
            else:
                if stats['failed'] > 0:
                    logging.info(f"å•åŒ…è®­ç»ƒå¤±è´¥! å¤±è´¥å¯¼è‡´")
                    raise Exception(f"å•åŒ…è®­ç»ƒå¤±è´¥! å¤±è´¥å¯¼è‡´")
                else:
                    logging.info(f"å•åŒ…è®­ç»ƒå¤±è´¥! è·³è¿‡ï¼Œæ•°æ®ä¸ºç©º")
                    raise Exception(f"å•åŒ…è®­ç»ƒå¤±è´¥! è·³è¿‡ï¼Œæ•°æ®ä¸ºç©ºæˆ–è€…æ¨¡å‹å·²å­˜åœ¨")
                

    except Exception as e:
        if rank == 0:
            logging.error(f"åˆ†å¸ƒå¼è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        raise
    
    # æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒ
    if world_size > 1:
        dist.destroy_process_group()


if __name__ == "__main__":
    main() 