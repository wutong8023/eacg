#!/usr/bin/env python3
"""
å‰å‘ä¼ æ’­å†…å­˜åˆ†æå™¨
ç”¨äºç›‘æ§æ¨¡å‹å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­æ¯ä¸ªæ¨¡å—çš„æ˜¾å­˜å ç”¨æƒ…å†µ
"""

import torch
import torch.nn as nn
from typing import Dict, List, Any, Optional, Tuple
import logging
from collections import defaultdict, OrderedDict
import json
import os
from datetime import datetime
import traceback

class TensorInfo:
    """Tensorä¿¡æ¯ç±»ï¼Œç”¨äºå­˜å‚¨tensorçš„è¯¦ç»†ä¿¡æ¯"""
    
    def __init__(self, tensor: torch.Tensor, name: str = ""):
        self.name = name
        self.shape = list(tensor.shape)
        self.dtype = str(tensor.dtype)
        self.device = str(tensor.device)
        self.element_size = tensor.element_size()
        self.numel = tensor.numel()
        self.memory_mb = (self.numel * self.element_size) / (1024**2)
        self.requires_grad = tensor.requires_grad
        
    def to_dict(self) -> Dict:
        return {
            "name": self.name,
            "shape": self.shape,
            "dtype": self.dtype,
            "device": self.device,
            "element_size_bytes": self.element_size,
            "num_elements": self.numel,
            "memory_mb": self.memory_mb,
            "requires_grad": self.requires_grad
        }
        
    def __str__(self) -> str:
        return f"{self.name}: {self.shape} {self.dtype} on {self.device} ({self.memory_mb:.2f}MB)"

class ModuleProfiler:
    """å•ä¸ªæ¨¡å—çš„æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self, module_name: str, module: nn.Module):
        self.module_name = module_name
        self.module = module
        self.call_count = 0
        self.forward_records = []
        
    def profile_forward(self, inputs, outputs, gpu_memory_before, gpu_memory_after):
        """è®°å½•ä¸€æ¬¡forwardè°ƒç”¨çš„è¯¦ç»†ä¿¡æ¯"""
        record = {
            "call_id": self.call_count,
            "module_name": self.module_name,
            "timestamp": datetime.now().isoformat(),
            "gpu_memory_before_mb": gpu_memory_before,
            "gpu_memory_after_mb": gpu_memory_after,
            "memory_delta_mb": gpu_memory_after - gpu_memory_before,
            "inputs": [],
            "outputs": [],
            "module_parameters": self._analyze_module_parameters()
        }
        
        # åˆ†æè¾“å…¥tensors
        if isinstance(inputs, (tuple, list)):
            for i, inp in enumerate(inputs):
                if isinstance(inp, torch.Tensor):
                    tensor_info = TensorInfo(inp, f"input_{i}")
                    record["inputs"].append(tensor_info.to_dict())
        elif isinstance(inputs, torch.Tensor):
            tensor_info = TensorInfo(inputs, "input_0")
            record["inputs"].append(tensor_info.to_dict())
        elif isinstance(inputs, dict):
            # å¤„ç†å­—å…¸å½¢å¼çš„è¾“å…¥ï¼ˆå¦‚è®­ç»ƒæ—¶çš„batchæ•°æ®ï¼‰
            for key, value in inputs.items():
                if isinstance(value, torch.Tensor):
                    tensor_info = TensorInfo(value, key)
                    record["inputs"].append(tensor_info.to_dict())
                    
                    # æ‰“å°è¯¦ç»†çš„tensorä¿¡æ¯
                    self.logger.info(f"    ğŸ“Š {key}: {value.shape} (dtype: {value.dtype}, device: {value.device})")
            
        # åˆ†æè¾“å‡ºtensors
        if isinstance(outputs, (tuple, list)):
            for i, out in enumerate(outputs):
                if isinstance(out, torch.Tensor):
                    tensor_info = TensorInfo(out, f"output_{i}")
                    record["outputs"].append(tensor_info.to_dict())
        elif isinstance(outputs, torch.Tensor):
            tensor_info = TensorInfo(outputs, "output_0")
            record["outputs"].append(tensor_info.to_dict())
            
        self.forward_records.append(record)
        self.call_count += 1
        
        return record
        
    def _analyze_module_parameters(self) -> Dict:
        """åˆ†ææ¨¡å—å‚æ•°"""
        param_info = {
            "total_params": 0,
            "trainable_params": 0,
            "total_memory_mb": 0,
            "parameters": []
        }
        
        for name, param in self.module.named_parameters():
            if param is not None:
                tensor_info = TensorInfo(param, name)
                param_info["parameters"].append(tensor_info.to_dict())
                param_info["total_params"] += param.numel()
                param_info["total_memory_mb"] += tensor_info.memory_mb
                if param.requires_grad:
                    param_info["trainable_params"] += param.numel()
                    
        return param_info

class ForwardPassProfiler:
    """å‰å‘ä¼ æ’­å†…å­˜åˆ†æå™¨ä¸»ç±»"""
    
    def __init__(self, log_dir: str = "logs/forward_pass_profiler", enable_detailed_logging: bool = True):
        self.log_dir = log_dir
        self.enable_detailed_logging = enable_detailed_logging
        self.module_profilers = {}
        self.wrapped_modules = {}
        self.original_forwards = {}
        self.global_call_order = []
        self.is_profiling = False
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        os.makedirs(log_dir, exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        self.logger = self._setup_logger()
        
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—å™¨"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        logger = logging.getLogger(f"ForwardPassProfiler_{timestamp}")
        logger.setLevel(logging.INFO)
        
        # æ–‡ä»¶å¤„ç†å™¨
        log_file = os.path.join(self.log_dir, f"forward_pass_profile_{timestamp}.log")
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        
        # æ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.WARNING)
        
        # æ ¼å¼å™¨
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)
        
        return logger
        
    def _get_gpu_memory(self) -> float:
        """è·å–å½“å‰GPUå†…å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰"""
        if torch.cuda.is_available():
            return torch.cuda.memory_allocated() / (1024**2)
        return 0.0
        
    def wrap_module(self, module: nn.Module, module_name: str):
        """åŒ…è£…ä¸€ä¸ªæ¨¡å—ä»¥ç›‘æ§å…¶å‰å‘ä¼ æ’­"""
        if module_name in self.wrapped_modules:
            return  # å·²ç»åŒ…è£…è¿‡äº†
            
        # åˆ›å»ºæ¨¡å—åˆ†æå™¨
        profiler = ModuleProfiler(module_name, module)
        self.module_profilers[module_name] = profiler
        
        # ä¿å­˜åŸå§‹forwardæ–¹æ³•
        original_forward = module.forward
        self.original_forwards[module_name] = original_forward
        
        def profiled_forward(*args, **kwargs):
            if not self.is_profiling:
                return original_forward(*args, **kwargs)
                
            # è®°å½•è°ƒç”¨é¡ºåº
            call_id = len(self.global_call_order)
            self.global_call_order.append(module_name)
            
            # è®°å½•å‰å‘ä¼ æ’­å‰çš„GPUå†…å­˜
            gpu_memory_before = self._get_gpu_memory()
            
            try:
                # æ‰§è¡ŒåŸå§‹forward
                outputs = original_forward(*args, **kwargs)
                
                # è®°å½•å‰å‘ä¼ æ’­åçš„GPUå†…å­˜
                gpu_memory_after = self._get_gpu_memory()
                
                # åˆ†æè¿™æ¬¡è°ƒç”¨
                record = profiler.profile_forward(
                    inputs=args,
                    outputs=outputs,
                    gpu_memory_before=gpu_memory_before,
                    gpu_memory_after=gpu_memory_after
                )
                
                # è¯¦ç»†æ—¥å¿—
                if self.enable_detailed_logging:
                    memory_delta = gpu_memory_after - gpu_memory_before
                    self.logger.info(f"æ¨¡å— {module_name} [è°ƒç”¨#{call_id}]: å†…å­˜å˜åŒ– {memory_delta:+.2f}MB "
                                   f"(å‰: {gpu_memory_before:.2f}MB, å: {gpu_memory_after:.2f}MB)")
                    
                    # è®°å½•è¾“å…¥è¾“å‡ºtensorä¿¡æ¯
                    if record["inputs"]:
                        input_memory = sum(inp["memory_mb"] for inp in record["inputs"])
                        self.logger.info(f"  è¾“å…¥tensors: {len(record['inputs'])}ä¸ª, æ€»å†…å­˜: {input_memory:.2f}MB")
                        
                        # è¯¦ç»†æ‰“å°è¾“å…¥tensorä¿¡æ¯
                        for inp in record["inputs"]:
                            self.logger.info(f"    ğŸ“Š {inp['name']}: {inp['shape']} "
                                           f"(dtype: {inp['dtype']}, device: {inp['device']}, "
                                           f"memory: {inp['memory_mb']:.2f}MB)")
                        
                    if record["outputs"]:
                        output_memory = sum(out["memory_mb"] for out in record["outputs"])
                        self.logger.info(f"  è¾“å‡ºtensors: {len(record['outputs'])}ä¸ª, æ€»å†…å­˜: {output_memory:.2f}MB")
                        
                        # è¯¦ç»†æ‰“å°è¾“å‡ºtensorä¿¡æ¯
                        for out in record["outputs"]:
                            self.logger.info(f"    ğŸ“Š {out['name']}: {out['shape']} "
                                           f"(dtype: {out['dtype']}, device: {out['device']}, "
                                           f"memory: {out['memory_mb']:.2f}MB)")
                
                return outputs
                
            except Exception as e:
                self.logger.error(f"æ¨¡å— {module_name} å‰å‘ä¼ æ’­å‡ºé”™: {e}")
                traceback.print_exc()
                raise e
        
        # æ›¿æ¢forwardæ–¹æ³•
        module.forward = profiled_forward
        self.wrapped_modules[module_name] = module
        
    def wrap_model(self, model: nn.Module, prefix: str = ""):
        """é€’å½’åŒ…è£…æ¨¡å‹çš„æ‰€æœ‰å­æ¨¡å—"""
        for name, child in model.named_children():
            module_name = f"{prefix}.{name}" if prefix else name
            
            # åŒ…è£…è¿™ä¸ªå­æ¨¡å—
            self.wrap_module(child, module_name)
            
            # é€’å½’åŒ…è£…å­æ¨¡å—çš„å­æ¨¡å—
            if len(list(child.children())) > 0:
                self.wrap_model(child, module_name)
                
        # ä¹ŸåŒ…è£…æ ¹æ¨¡å—
        if not prefix:  # åªåœ¨æœ€é¡¶å±‚åŒ…è£…æ ¹æ¨¡å—
            self.wrap_module(model, "root_model")
            
    def start_profiling(self):
        """å¼€å§‹æ€§èƒ½åˆ†æ"""
        self.is_profiling = True
        self.global_call_order = []
        self.logger.info("ğŸš€ å¼€å§‹å‰å‘ä¼ æ’­å†…å­˜åˆ†æ")
        
    def stop_profiling(self):
        """åœæ­¢æ€§èƒ½åˆ†æ"""
        self.is_profiling = False
        self.logger.info("ğŸ›‘ åœæ­¢å‰å‘ä¼ æ’­å†…å­˜åˆ†æ")
        
    def unwrap_all(self):
        """æ¢å¤æ‰€æœ‰æ¨¡å—çš„åŸå§‹forwardæ–¹æ³•"""
        for module_name, module in self.wrapped_modules.items():
            if module_name in self.original_forwards:
                module.forward = self.original_forwards[module_name]
                
        self.wrapped_modules.clear()
        self.original_forwards.clear()
        self.logger.info("âœ… å·²æ¢å¤æ‰€æœ‰æ¨¡å—çš„åŸå§‹forwardæ–¹æ³•")
        
    def generate_summary_report(self) -> Dict:
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        report = {
            "profiling_summary": {
                "total_modules": len(self.module_profilers),
                "total_forward_calls": sum(p.call_count for p in self.module_profilers.values()),
                "call_order": self.global_call_order
            },
            "module_statistics": {},
            "memory_analysis": {
                "peak_memory_usage": 0,
                "total_memory_allocated": 0,
                "most_memory_intensive_modules": []
            }
        }
        
        # ç»Ÿè®¡æ¯ä¸ªæ¨¡å—
        module_stats = []
        for module_name, profiler in self.module_profilers.items():
            if profiler.call_count == 0:
                continue
                
            # è®¡ç®—è¯¥æ¨¡å—çš„ç»Ÿè®¡ä¿¡æ¯
            memory_deltas = [record["memory_delta_mb"] for record in profiler.forward_records]
            avg_memory_delta = sum(memory_deltas) / len(memory_deltas)
            max_memory_delta = max(memory_deltas)
            min_memory_delta = min(memory_deltas)
            
            # è®¡ç®—è¾“å…¥è¾“å‡ºå†…å­˜ç»Ÿè®¡
            total_input_memory = 0
            total_output_memory = 0
            for record in profiler.forward_records:
                total_input_memory += sum(inp["memory_mb"] for inp in record["inputs"])
                total_output_memory += sum(out["memory_mb"] for out in record["outputs"])
                
            module_stat = {
                "module_name": module_name,
                "call_count": profiler.call_count,
                "avg_memory_delta_mb": avg_memory_delta,
                "max_memory_delta_mb": max_memory_delta,
                "min_memory_delta_mb": min_memory_delta,
                "total_input_memory_mb": total_input_memory,
                "total_output_memory_mb": total_output_memory,
                "parameter_memory_mb": profiler.forward_records[0]["module_parameters"]["total_memory_mb"] if profiler.forward_records else 0
            }
            
            module_stats.append(module_stat)
            report["module_statistics"][module_name] = module_stat
            
        # æŒ‰å†…å­˜ä½¿ç”¨é‡æ’åº
        module_stats.sort(key=lambda x: x["max_memory_delta_mb"], reverse=True)
        report["memory_analysis"]["most_memory_intensive_modules"] = module_stats[:10]
        
        # è®¡ç®—å³°å€¼å†…å­˜
        all_memory_values = []
        for profiler in self.module_profilers.values():
            for record in profiler.forward_records:
                all_memory_values.extend([record["gpu_memory_before_mb"], record["gpu_memory_after_mb"]])
                
        if all_memory_values:
            report["memory_analysis"]["peak_memory_usage"] = max(all_memory_values)
            report["memory_analysis"]["total_memory_allocated"] = sum(all_memory_values) / len(all_memory_values)
            
        return report
        
    def save_detailed_report(self, filename: str = None):
        """ä¿å­˜è¯¦ç»†æŠ¥å‘Šåˆ°JSONæ–‡ä»¶"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"forward_pass_detailed_report_{timestamp}.json"
            
        filepath = os.path.join(self.log_dir, filename)
        
        # æ„å»ºè¯¦ç»†æŠ¥å‘Š
        detailed_report = {
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "total_modules": len(self.module_profilers),
                "profiling_enabled": self.is_profiling
            },
            "module_details": {}
        }
        
        for module_name, profiler in self.module_profilers.items():
            detailed_report["module_details"][module_name] = {
                "call_count": profiler.call_count,
                "forward_records": profiler.forward_records
            }
            
        # æ·»åŠ æ±‡æ€»ä¿¡æ¯
        detailed_report["summary"] = self.generate_summary_report()
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(detailed_report, f, ensure_ascii=False, indent=2)
            self.logger.info(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filepath}")
        except Exception as e:
            self.logger.error(f"ä¿å­˜è¯¦ç»†æŠ¥å‘Šå¤±è´¥: {e}")
            
        return filepath
        
    def print_summary(self):
        """æ‰“å°æ±‡æ€»ä¿¡æ¯"""
        report = self.generate_summary_report()
        
        print("\n" + "="*80)
        print("ğŸ” å‰å‘ä¼ æ’­å†…å­˜åˆ†ææ±‡æ€»æŠ¥å‘Š")
        print("="*80)
        
        print(f"ğŸ“Š æ€»æ¨¡å—æ•°: {report['profiling_summary']['total_modules']}")
        print(f"ğŸ“ æ€»è°ƒç”¨æ¬¡æ•°: {report['profiling_summary']['total_forward_calls']}")
        print(f"ğŸ”ï¸  å³°å€¼å†…å­˜ä½¿ç”¨: {report['memory_analysis']['peak_memory_usage']:.2f}MB")
        
        print(f"\nğŸ”¥ å†…å­˜ä½¿ç”¨æœ€å¤šçš„å‰5ä¸ªæ¨¡å—:")
        for i, module_stat in enumerate(report['memory_analysis']['most_memory_intensive_modules'][:5]):
            print(f"  {i+1}. {module_stat['module_name']}: "
                  f"æœ€å¤§å†…å­˜å¢é‡ {module_stat['max_memory_delta_mb']:.2f}MB, "
                  f"å¹³å‡å†…å­˜å¢é‡ {module_stat['avg_memory_delta_mb']:.2f}MB, "
                  f"è°ƒç”¨æ¬¡æ•° {module_stat['call_count']}")
                  
        print("="*80)

def profile_model_forward(model: nn.Module, inputs, log_dir: str = "logs/forward_pass_profiler") -> ForwardPassProfiler:
    """
    ä¾¿æ·å‡½æ•°ï¼šå¯¹æ¨¡å‹çš„ä¸€æ¬¡å‰å‘ä¼ æ’­è¿›è¡Œå®Œæ•´çš„å†…å­˜åˆ†æ
    
    Args:
        model: è¦åˆ†æçš„æ¨¡å‹
        inputs: æ¨¡å‹è¾“å…¥ï¼ˆå¯ä»¥æ˜¯tensorã€tupleæˆ–dictï¼‰
        log_dir: æ—¥å¿—ä¿å­˜ç›®å½•
        
    Returns:
        ForwardPassProfilerå®ä¾‹ï¼Œå¯ç”¨äºè¿›ä¸€æ­¥åˆ†æ
    """
    profiler = ForwardPassProfiler(log_dir=log_dir)
    
    try:
        # åŒ…è£…æ¨¡å‹
        profiler.wrap_model(model)
        
        # å¼€å§‹åˆ†æ
        profiler.start_profiling()
        
        # æ‰§è¡Œå‰å‘ä¼ æ’­
        with torch.no_grad():
            if isinstance(inputs, dict):
                outputs = model(**inputs)
            elif isinstance(inputs, (tuple, list)):
                outputs = model(*inputs)
            else:
                outputs = model(inputs)
                
        # åœæ­¢åˆ†æ
        profiler.stop_profiling()
        
        # ç”ŸæˆæŠ¥å‘Š
        profiler.print_summary()
        profiler.save_detailed_report()
        
        return profiler
        
    except Exception as e:
        profiler.logger.error(f"å‰å‘ä¼ æ’­åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        raise e
    finally:
        # ç¡®ä¿æ¢å¤åŸå§‹çŠ¶æ€
        profiler.unwrap_all()

# ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ”¯æŒ
class ProfiledModel:
    """ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç”¨äºä¸´æ—¶åˆ†ææ¨¡å‹"""
    
    def __init__(self, model: nn.Module, log_dir: str = "logs/forward_pass_profiler"):
        self.model = model
        self.profiler = ForwardPassProfiler(log_dir=log_dir)
        
    def __enter__(self):
        self.profiler.wrap_model(self.model)
        self.profiler.start_profiling()
        return self.profiler
        
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.profiler.stop_profiling()
        self.profiler.print_summary()
        self.profiler.save_detailed_report()
        self.profiler.unwrap_all() 