#!/usr/bin/env python3
"""
å†…å­˜åˆ†æå™¨ä½¿ç”¨ç¤ºä¾‹
å±•ç¤ºå¦‚ä½•åœ¨LoRAè®­ç»ƒä¸­ä½¿ç”¨GPUMemoryProfilerè¿›è¡Œå†…å­˜ç›‘æ§
"""

import torch
import torch.nn as nn
from memoryCheck import GPUMemoryProfiler
import time

def example_lora_training():
    """ç¤ºä¾‹ï¼šåœ¨LoRAè®­ç»ƒä¸­ä½¿ç”¨å†…å­˜åˆ†æå™¨"""
    
    # åˆ›å»ºå†…å­˜åˆ†æå™¨
    profiler = GPUMemoryProfiler(
        log_dir="logs/example_lora_training",
        enable_file_logging=True
    )
    
    # è®°å½•è®­ç»ƒå¼€å§‹
    profiler.record("è®­ç»ƒå¼€å§‹")
    
    # æ¨¡æ‹Ÿæ¨¡å‹åˆ›å»º
    print("ğŸ”§ åˆ›å»ºæ¨¡å‹...")
    hidden_size = 4096
    r = 16
    
    # åˆ›å»ºLoRAå‚æ•°
    lora_A = torch.randn(r, hidden_size, device='cuda', requires_grad=True)
    lora_B = torch.randn(hidden_size, r, device='cuda', requires_grad=True)
    
    # è®°å½•æ¨¡å‹åˆ›å»º
    tensor_info = {
        'lora_A': lora_A,
        'lora_B': lora_B
    }
    model_info = {
        'total_params': lora_A.numel() + lora_B.numel(),
        'trainable_params': lora_A.numel() + lora_B.numel(),
        'model_size_mb': (lora_A.numel() + lora_B.numel()) * lora_A.element_size() / (1024**2)
    }
    profiler.record_detailed("æ¨¡å‹åˆ›å»ºå®Œæˆ", tensor_info=tensor_info, model_info=model_info)
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    print("ğŸ”§ åˆ›å»ºä¼˜åŒ–å™¨...")
    optimizer = torch.optim.AdamW([lora_A, lora_B], lr=1e-4)
    profiler.record("ä¼˜åŒ–å™¨åˆ›å»ºå®Œæˆ")
    
    # æ¨¡æ‹Ÿè®­ç»ƒå¾ªç¯
    print("ğŸš€ å¼€å§‹è®­ç»ƒå¾ªç¯...")
    num_epochs = 3
    batch_size = 8
    seq_length = 512
    
    for epoch in range(num_epochs):
        profiler.record(f"Epoch {epoch+1} å¼€å§‹")
        
        for batch in range(5):  # æ¨¡æ‹Ÿ5ä¸ªbatch
            # åˆ›å»ºè¾“å…¥æ•°æ®
            input_tensor = torch.randn(batch_size, seq_length, hidden_size, device='cuda')
            
            # å‰å‘ä¼ æ’­
            output = torch.matmul(input_tensor, lora_A.T)
            output = torch.matmul(output, lora_B.T)
            loss = output.sum()
            
            profiler.record(f"Epoch {epoch+1} Batch {batch+1} å‰å‘ä¼ æ’­", 
                          f"loss={loss.item():.4f}, output_shape={output.shape}")
            
            # åå‘ä¼ æ’­
            loss.backward()
            profiler.record(f"Epoch {epoch+1} Batch {batch+1} åå‘ä¼ æ’­")
            
            # å‚æ•°æ›´æ–°
            optimizer.step()
            optimizer.zero_grad()
            profiler.record(f"Epoch {epoch+1} Batch {batch+1} å‚æ•°æ›´æ–°")
            
            # æ¸…ç†ä¸­é—´å˜é‡
            del input_tensor, output, loss
            torch.cuda.empty_cache()
        
        profiler.record(f"Epoch {epoch+1} å®Œæˆ")
    
    # è®°å½•è®­ç»ƒå®Œæˆ
    profiler.record("è®­ç»ƒå®Œæˆ")
    
    # ç”ŸæˆæŠ¥å‘Š
    print("\nğŸ“Š ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š...")
    profiler.print_report()
    summary = profiler.generate_summary_report()
    print(summary)
    
    # ä¿å­˜æ•°æ®
    json_file = profiler.save_to_json()
    print(f"ğŸ“„ è®­ç»ƒæ•°æ®å·²ä¿å­˜åˆ°: {json_file}")
    
    # æ¸…ç†
    profiler.cleanup()
    del lora_A, lora_B, optimizer
    
    print("âœ… ç¤ºä¾‹è®­ç»ƒå®Œæˆ!")

def example_memory_monitoring():
    """ç¤ºä¾‹ï¼šæŒç»­å†…å­˜ç›‘æ§"""
    
    profiler = GPUMemoryProfiler(
        log_dir="logs/example_monitoring",
        enable_file_logging=True
    )
    
    print("ğŸ” å¼€å§‹æŒç»­å†…å­˜ç›‘æ§...")
    
    # æ¨¡æ‹Ÿé•¿æ—¶é—´è¿è¡Œçš„ä»»åŠ¡
    for i in range(10):
        # åˆ›å»ºä¸€äº›å¼ é‡
        tensor = torch.randn(1000, 1000, device='cuda')
        
        # æ‰§è¡Œä¸€äº›è®¡ç®—
        result = torch.matmul(tensor, tensor.T)
        
        # è®°å½•å†…å­˜ä½¿ç”¨
        profiler.record(f"æ­¥éª¤ {i+1}", f"tensor_shape={tensor.shape}, result_shape={result.shape}")
        
        # æ¸…ç†
        del tensor, result
        torch.cuda.empty_cache()
        
        # ç­‰å¾…ä¸€æ®µæ—¶é—´
        time.sleep(0.5)
    
    # ç”ŸæˆæŠ¥å‘Š
    profiler.print_report()
    profiler.generate_summary_report()
    profiler.save_to_json()
    profiler.cleanup()
    
    print("âœ… æŒç»­ç›‘æ§å®Œæˆ!")

def example_memory_analysis():
    """ç¤ºä¾‹ï¼šå†…å­˜åˆ†æ"""
    
    profiler = GPUMemoryProfiler(
        log_dir="logs/example_analysis",
        enable_file_logging=True
    )
    
    print("ğŸ”¬ å¼€å§‹å†…å­˜åˆ†æ...")
    
    # æµ‹è¯•ä¸åŒå¤§å°çš„å¼ é‡
    sizes = [100, 500, 1000, 2000]
    
    for size in sizes:
        # åˆ›å»ºå¼ é‡
        tensor = torch.randn(size, size, device='cuda')
        
        # è®°å½•è¯¦ç»†ä¿¡æ¯
        tensor_info = {f'tensor_{size}x{size}': tensor}
        profiler.record_detailed(f"åˆ›å»º {size}x{size} å¼ é‡", tensor_info=tensor_info)
        
        # æ‰§è¡Œè®¡ç®—
        result = torch.matmul(tensor, tensor.T)
        profiler.record(f"è®¡ç®— {size}x{size} çŸ©é˜µä¹˜æ³•", f"result_shape={result.shape}")
        
        # æ¸…ç†
        del tensor, result
        torch.cuda.empty_cache()
    
    # åˆ†æå†…å­˜ä½¿ç”¨æ¨¡å¼
    profiler.print_report()
    summary = profiler.generate_summary_report()
    print(summary)
    
    # ä¿å­˜åˆ†æç»“æœ
    json_file = profiler.save_to_json()
    print(f"ğŸ“„ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {json_file}")
    
    profiler.cleanup()
    print("âœ… å†…å­˜åˆ†æå®Œæˆ!")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å†…å­˜åˆ†æå™¨ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 50)
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œæ— æ³•è¿è¡Œç¤ºä¾‹")
        return
    
    # è¿è¡Œç¤ºä¾‹
    print("\n1ï¸âƒ£ LoRAè®­ç»ƒç¤ºä¾‹")
    example_lora_training()
    
    print("\n2ï¸âƒ£ æŒç»­ç›‘æ§ç¤ºä¾‹")
    example_memory_monitoring()
    
    print("\n3ï¸âƒ£ å†…å­˜åˆ†æç¤ºä¾‹")
    example_memory_analysis()
    
    print("\nğŸ‰ æ‰€æœ‰ç¤ºä¾‹å®Œæˆ!")
    print("ğŸ“ æŸ¥çœ‹ logs/ ç›®å½•ä¸‹çš„æ—¥å¿—æ–‡ä»¶")

if __name__ == "__main__":
    main() 