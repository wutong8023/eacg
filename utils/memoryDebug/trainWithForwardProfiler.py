#!/usr/bin/env python3
"""
é›†æˆå‰å‘ä¼ æ’­åˆ†æå™¨åˆ°è®­ç»ƒè¿‡ç¨‹ä¸­
"""

import torch
import torch.nn as nn
from typing import Dict, Optional, Any
import os
from datetime import datetime
from .forwardPassProfiler import ForwardPassProfiler, ProfiledModel, profile_model_forward

class TrainingForwardProfiler:
    """è®­ç»ƒè¿‡ç¨‹ä¸­çš„å‰å‘ä¼ æ’­åˆ†æå™¨"""
    
    def __init__(self, log_dir: str = "logs/training_forward_profiler"):
        self.log_dir = log_dir
        self.profiler = None
        self.analysis_results = []
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        os.makedirs(log_dir, exist_ok=True)
        
    def profile_single_batch(self, model: nn.Module, batch_data: Dict, stage: str = "training") -> Dict:
        """
        åˆ†æå•ä¸ªbatchçš„å‰å‘ä¼ æ’­
        
        Args:
            model: æ¨¡å‹
            batch_data: batchæ•°æ®ï¼ŒåŒ…å«input_ids, labels, attention_maskç­‰
            stage: é˜¶æ®µåç§°
            
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        print(f"\nğŸ” å¼€å§‹åˆ†æ {stage} é˜¶æ®µçš„å‰å‘ä¼ æ’­...")
        
        # å‡†å¤‡è¾“å…¥æ•°æ®
        inputs = {
            'input_ids': batch_data['input_ids'],
            'attention_mask': batch_data['attention_mask'],
            'labels': batch_data['labels']
        }
        
        # ä½¿ç”¨ä¾¿æ·å‡½æ•°è¿›è¡Œåˆ†æ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        stage_log_dir = os.path.join(self.log_dir, f"{stage}_{timestamp}")
        
        profiler = profile_model_forward(model, inputs, log_dir=stage_log_dir)
        
        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        summary = profiler.generate_summary_report()
        
        # ä¿å­˜ç»“æœ
        result = {
            "stage": stage,
            "timestamp": timestamp,
            "batch_shape": {
                "input_ids": list(batch_data['input_ids'].shape),
                "attention_mask": list(batch_data['attention_mask'].shape),
                "labels": list(batch_data['labels'].shape)
            },
            "summary": summary,
            "log_directory": stage_log_dir
        }
        
        self.analysis_results.append(result)
        
        print(f"âœ… {stage} é˜¶æ®µåˆ†æå®Œæˆï¼Œè¯¦ç»†æŠ¥å‘Šä¿å­˜åœ¨: {stage_log_dir}")
        
        return result
        
    def setup_continuous_profiling(self, model: nn.Module) -> ForwardPassProfiler:
        """
        è®¾ç½®æŒç»­çš„å‰å‘ä¼ æ’­åˆ†æ
        
        Args:
            model: è¦åˆ†æçš„æ¨¡å‹
            
        Returns:
            ForwardPassProfilerå®ä¾‹
        """
        if self.profiler is not None:
            print("âš ï¸ è­¦å‘Šï¼šå·²å­˜åœ¨æ´»è·ƒçš„åˆ†æå™¨ï¼Œå°†å…ˆåœæ­¢ç°æœ‰åˆ†æå™¨")
            self.stop_continuous_profiling()
            
        self.profiler = ForwardPassProfiler(log_dir=self.log_dir, enable_detailed_logging=False)
        self.profiler.wrap_model(model)
        
        print("ğŸš€ æŒç»­å‰å‘ä¼ æ’­åˆ†æå·²è®¾ç½®å®Œæˆ")
        return self.profiler
        
    def start_continuous_profiling(self):
        """å¼€å§‹æŒç»­åˆ†æ"""
        if self.profiler is None:
            raise RuntimeError("è¯·å…ˆè°ƒç”¨ setup_continuous_profiling() è®¾ç½®åˆ†æå™¨")
        self.profiler.start_profiling()
        print("â–¶ï¸ å¼€å§‹æŒç»­å‰å‘ä¼ æ’­åˆ†æ")
        
    def stop_continuous_profiling(self) -> Optional[Dict]:
        """åœæ­¢æŒç»­åˆ†æå¹¶ç”ŸæˆæŠ¥å‘Š"""
        if self.profiler is None:
            return None
            
        self.profiler.stop_profiling()
        
        # ç”ŸæˆæŠ¥å‘Š
        summary = self.profiler.generate_summary_report()
        report_file = self.profiler.save_detailed_report()
        
        # æ¸…ç†
        self.profiler.unwrap_all()
        self.profiler = None
        
        print("â¹ï¸ æŒç»­å‰å‘ä¼ æ’­åˆ†æå·²åœæ­¢")
        
        return {
            "summary": summary,
            "detailed_report_file": report_file
        }
        
    def analyze_memory_pattern(self, model: nn.Module, dataloader, num_batches: int = 3) -> Dict:
        """
        åˆ†æå¤šä¸ªbatchçš„å†…å­˜ä½¿ç”¨æ¨¡å¼
        
        Args:
            model: æ¨¡å‹
            dataloader: æ•°æ®åŠ è½½å™¨
            num_batches: åˆ†æçš„batchæ•°é‡
            
        Returns:
            å†…å­˜æ¨¡å¼åˆ†æç»“æœ
        """
        print(f"\nğŸ“Š å¼€å§‹åˆ†æ {num_batches} ä¸ªbatchçš„å†…å­˜ä½¿ç”¨æ¨¡å¼...")
        
        batch_results = []
        
        with ProfiledModel(model, log_dir=self.log_dir) as profiler:
            for i, batch in enumerate(dataloader):
                if i >= num_batches:
                    break
                    
                print(f"\nåˆ†æç¬¬ {i+1}/{num_batches} ä¸ªbatch...")
                
                # æ‰§è¡Œå‰å‘ä¼ æ’­
                with torch.no_grad():
                    inputs = {
                        'input_ids': batch['input_ids'],
                        'attention_mask': batch['attention_mask'],
                        'labels': batch['labels']
                    }
                    outputs = model(**inputs)
                
                # è®°å½•è¿™ä¸ªbatchçš„ç»“æœ
                batch_result = {
                    "batch_id": i,
                    "batch_shape": {
                        "input_ids": list(batch['input_ids'].shape),
                        "attention_mask": list(batch['attention_mask'].shape),
                        "labels": list(batch['labels'].shape)
                    },
                    "output_shape": list(outputs.logits.shape) if hasattr(outputs, 'logits') else "unknown"
                }
                batch_results.append(batch_result)
                
        # ç”Ÿæˆæ¨¡å¼åˆ†æ
        summary = profiler.generate_summary_report()
        
        # åˆ†æå†…å­˜ä½¿ç”¨æ¨¡å¼
        memory_pattern = self._analyze_batch_memory_pattern(batch_results, summary)
        
        result = {
            "num_batches_analyzed": num_batches,
            "batch_results": batch_results,
            "memory_pattern": memory_pattern,
            "overall_summary": summary
        }
        
        print(f"âœ… å†…å­˜ä½¿ç”¨æ¨¡å¼åˆ†æå®Œæˆ")
        
        return result
        
    def _analyze_batch_memory_pattern(self, batch_results: list, summary: Dict) -> Dict:
        """åˆ†æbatché—´çš„å†…å­˜ä½¿ç”¨æ¨¡å¼"""
        pattern = {
            "memory_consistency": "unknown",
            "peak_modules": [],
            "potential_issues": []
        }
        
        if "memory_analysis" in summary:
            memory_analysis = summary["memory_analysis"]
            
            # è·å–æœ€è€—å†…å­˜çš„æ¨¡å—
            if "most_memory_intensive_modules" in memory_analysis:
                pattern["peak_modules"] = memory_analysis["most_memory_intensive_modules"][:5]
                
            # åˆ†ææ½œåœ¨é—®é¢˜
            if memory_analysis.get("peak_memory_usage", 0) > 10000:  # è¶…è¿‡10GB
                pattern["potential_issues"].append("å³°å€¼å†…å­˜ä½¿ç”¨è¿‡é«˜")
                
            # æ£€æŸ¥æ¨¡å—è°ƒç”¨ä¸€è‡´æ€§
            total_calls = summary.get("profiling_summary", {}).get("total_forward_calls", 0)
            if total_calls > len(batch_results) * 100:  # å‡è®¾æ¯ä¸ªbatchä¸åº”è¯¥è¶…è¿‡100æ¬¡æ¨¡å—è°ƒç”¨
                pattern["potential_issues"].append("æ¨¡å—è°ƒç”¨æ¬¡æ•°å¼‚å¸¸é«˜")
                
            pattern["memory_consistency"] = "normal" if len(pattern["potential_issues"]) == 0 else "æœ‰é—®é¢˜"
            
        return pattern

def integrate_forward_profiler_into_training(
    model: nn.Module, 
    dataloader,
    training_function,
    enable_batch_analysis: bool = True,
    enable_continuous_profiling: bool = False,
    num_analysis_batches: int = 2
) -> Dict:
    """
    å°†å‰å‘ä¼ æ’­åˆ†æå™¨é›†æˆåˆ°è®­ç»ƒå‡½æ•°ä¸­
    
    Args:
        model: æ¨¡å‹
        dataloader: æ•°æ®åŠ è½½å™¨
        training_function: è®­ç»ƒå‡½æ•°
        enable_batch_analysis: æ˜¯å¦å¯ç”¨å•batchåˆ†æ
        enable_continuous_profiling: æ˜¯å¦å¯ç”¨æŒç»­åˆ†æ
        num_analysis_batches: åˆ†æçš„batchæ•°é‡
        
    Returns:
        åˆ†æç»“æœæ±‡æ€»
    """
    profiler = TrainingForwardProfiler()
    results = {
        "batch_analysis": None,
        "continuous_profiling": None,
        "memory_pattern": None
    }
    
    try:
        # 1. å•batchåˆ†æï¼ˆåœ¨è®­ç»ƒå‰ï¼‰
        if enable_batch_analysis and len(dataloader) > 0:
            print("\n" + "="*60)
            print("ğŸ” ç¬¬ä¸€é˜¶æ®µï¼šè®­ç»ƒå‰å•batchå‰å‘ä¼ æ’­åˆ†æ")
            print("="*60)
            
            # è·å–ç¬¬ä¸€ä¸ªbatchè¿›è¡Œåˆ†æ
            first_batch = next(iter(dataloader))
            results["batch_analysis"] = profiler.profile_single_batch(
                model, first_batch, stage="pre_training"
            )
            
        # 2. å†…å­˜ä½¿ç”¨æ¨¡å¼åˆ†æ
        if num_analysis_batches > 0:
            print("\n" + "="*60)
            print("ğŸ” ç¬¬äºŒé˜¶æ®µï¼šå¤šbatchå†…å­˜ä½¿ç”¨æ¨¡å¼åˆ†æ")
            print("="*60)
            
            results["memory_pattern"] = profiler.analyze_memory_pattern(
                model, dataloader, num_batches=num_analysis_batches
            )
            
        # 3. æŒç»­åˆ†æï¼ˆåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼‰
        if enable_continuous_profiling:
            print("\n" + "="*60)
            print("ğŸ” ç¬¬ä¸‰é˜¶æ®µï¼šè®­ç»ƒè¿‡ç¨‹æŒç»­åˆ†æ")
            print("="*60)
            
            # è®¾ç½®æŒç»­åˆ†æ
            profiler.setup_continuous_profiling(model)
            profiler.start_continuous_profiling()
            
            try:
                # æ‰§è¡Œè®­ç»ƒï¼ˆè¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼Œå®é™…ä½¿ç”¨æ—¶ä¼ å…¥çœŸæ­£çš„è®­ç»ƒå‡½æ•°ï¼‰
                print("ğŸš‚ å¼€å§‹è®­ç»ƒè¿‡ç¨‹...")
                training_result = training_function()
                
            finally:
                # åœæ­¢åˆ†æ
                results["continuous_profiling"] = profiler.stop_continuous_profiling()
                
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        print("\n" + "="*60)
        print("ğŸ“‹ ç”Ÿæˆæœ€ç»ˆåˆ†ææŠ¥å‘Š")
        print("="*60)
        
        final_report = _generate_final_report(results)
        
        # ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
        report_file = os.path.join(profiler.log_dir, "final_forward_profiling_report.json")
        import json
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(final_report, f, ensure_ascii=False, indent=2)
            
        print(f"ğŸ“„ æœ€ç»ˆæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        return final_report
        
    except Exception as e:
        print(f"âŒ å‰å‘ä¼ æ’­åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        raise e

def _generate_final_report(results: Dict) -> Dict:
    """ç”Ÿæˆæœ€ç»ˆçš„åˆ†ææŠ¥å‘Š"""
    report = {
        "timestamp": datetime.now().isoformat(),
        "analysis_summary": {
            "batch_analysis_completed": results["batch_analysis"] is not None,
            "continuous_profiling_completed": results["continuous_profiling"] is not None,
            "memory_pattern_analysis_completed": results["memory_pattern"] is not None
        },
        "key_findings": [],
        "recommendations": [],
        "detailed_results": results
    }
    
    # åˆ†æå…³é”®å‘ç°
    if results["memory_pattern"]:
        pattern = results["memory_pattern"]["memory_pattern"]
        if pattern["memory_consistency"] != "normal":
            report["key_findings"].append(f"å†…å­˜ä½¿ç”¨ä¸€è‡´æ€§: {pattern['memory_consistency']}")
            report["key_findings"].extend(pattern["potential_issues"])
            
        if pattern["peak_modules"]:
            top_module = pattern["peak_modules"][0]
            report["key_findings"].append(
                f"æœ€è€—å†…å­˜æ¨¡å—: {top_module['module_name']} "
                f"(æœ€å¤§å†…å­˜å¢é‡: {top_module['max_memory_delta_mb']:.2f}MB)"
            )
            
    # ç”Ÿæˆå»ºè®®
    if results["memory_pattern"] and results["memory_pattern"]["memory_pattern"]["potential_issues"]:
        report["recommendations"].append("å»ºè®®ä¼˜åŒ–å†…å­˜ä½¿ç”¨æœ€é«˜çš„æ¨¡å—")
        report["recommendations"].append("è€ƒè™‘ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯å‡å°‘å•æ¬¡å‰å‘ä¼ æ’­çš„batch size")
        
    return report

# ä½¿ç”¨ç¤ºä¾‹å‡½æ•°
def example_usage():
    """ä½¿ç”¨ç¤ºä¾‹"""
    print("å‰å‘ä¼ æ’­åˆ†æå™¨ä½¿ç”¨ç¤ºä¾‹:")
    print("="*50)
    
    print("""
# æ–¹å¼1ï¼šç®€å•çš„å•æ¬¡åˆ†æ
from utils.memoryDebug.forwardPassProfiler import profile_model_forward

# å‡†å¤‡è¾“å…¥æ•°æ®
inputs = {
    'input_ids': torch.randint(0, 1000, (2, 512)),
    'attention_mask': torch.ones(2, 512),
    'labels': torch.randint(0, 1000, (2, 512))
}

# åˆ†æå‰å‘ä¼ æ’­
profiler = profile_model_forward(model, inputs)

# æ–¹å¼2ï¼šä¸Šä¸‹æ–‡ç®¡ç†å™¨æ–¹å¼
from utils.memoryDebug.forwardPassProfiler import ProfiledModel

with ProfiledModel(model) as profiler:
    for batch in dataloader:
        outputs = model(**batch)
        
# æ–¹å¼3ï¼šé›†æˆåˆ°è®­ç»ƒä¸­
from utils.memoryDebug.trainWithForwardProfiler import integrate_forward_profiler_into_training

def my_training_function():
    # ä½ çš„è®­ç»ƒä»£ç 
    return training_result

results = integrate_forward_profiler_into_training(
    model=model,
    dataloader=dataloader,
    training_function=my_training_function,
    enable_batch_analysis=True,
    enable_continuous_profiling=True,
    num_analysis_batches=3
)
    """)

if __name__ == "__main__":
    example_usage() 