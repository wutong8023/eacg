import torch
import logging
import time
import json
import os
from datetime import datetime
from collections import defaultdict
import traceback
import threading
import psutil
from typing import Dict, List, Optional, Tuple, Any

class MemoryDebugger:
    """
    æ˜¾å­˜ç›‘æ§å’Œè°ƒè¯•å·¥å…·ï¼Œç”¨äºåˆ†ææ¨¡å‹è®­ç»ƒæ—¶å„ä¸ªå‚æ•°çŸ©é˜µçš„æ˜¾å­˜å ç”¨æƒ…å†µ
    """
    
    def __init__(self, log_dir: str = "logs/memory_debug", enable_real_time: bool = True):
        """
        åˆå§‹åŒ–æ˜¾å­˜è°ƒè¯•å™¨
        
        Args:
            log_dir: æ—¥å¿—ä¿å­˜ç›®å½•
            enable_real_time: æ˜¯å¦å¯ç”¨å®æ—¶ç›‘æ§
        """
        self.log_dir = log_dir
        self.enable_real_time = enable_real_time
        self.monitoring_active = False
        self.memory_history = []
        self.parameter_registry = {}
        self.gradient_registry = {}
        self.checkpoint_history = []
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        os.makedirs(log_dir, exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—è®°å½•
        self.logger = self._setup_logger()
        
        # åˆå§‹åŒ–GPUä¿¡æ¯
        self.gpu_info = self._get_gpu_info()
        
        self.logger.info("=" * 80)
        self.logger.info("å†…å­˜è°ƒè¯•å™¨åˆå§‹åŒ–å®Œæˆ")
        self.logger.info("=" * 80)
        self.logger.info(f"æ—¥å¿—ç›®å½•: {log_dir}")
        self.logger.info(f"å®æ—¶ç›‘æ§: {enable_real_time}")
        self.logger.info(f"æ£€æµ‹åˆ°GPUæ•°é‡: {len(self.gpu_info)}")
        
        for i, gpu in enumerate(self.gpu_info):
            self.logger.info(f"GPU {i}: {gpu['name']} ({gpu['total_memory_gb']:.1f}GB)")
    
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger("MemoryDebugger")
        logger.setLevel(logging.INFO)
        
        # æ¸…é™¤ç°æœ‰çš„å¤„ç†å™¨
        logger.handlers.clear()
        
        # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = os.path.join(self.log_dir, f"memory_debug_{timestamp}.log")
        
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.INFO)
        
        # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # åˆ›å»ºæ ¼å¼åŒ–å™¨
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)
        
        return logger
    
    def _get_gpu_info(self) -> List[Dict]:
        """è·å–GPUä¿¡æ¯"""
        gpu_info = []
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                props = torch.cuda.get_device_properties(i)
                gpu_info.append({
                    'device_id': i,
                    'name': props.name,
                    'total_memory_gb': props.total_memory / (1024**3),
                    'total_memory_mb': props.total_memory / (1024**2)
                })
        return gpu_info
    
    def get_current_memory_usage(self) -> Dict:
        """è·å–å½“å‰æ˜¾å­˜ä½¿ç”¨æƒ…å†µ"""
        memory_info = {
            'timestamp': datetime.now().isoformat(),
            'gpu_memory': [],
            'system_memory': self._get_system_memory()
        }
        
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                try:
                    torch.cuda.set_device(i)
                    allocated = torch.cuda.memory_allocated(i)
                    reserved = torch.cuda.memory_reserved(i)
                    max_allocated = torch.cuda.max_memory_allocated(i)
                    max_reserved = torch.cuda.max_memory_reserved(i)
                    
                    total_memory = torch.cuda.get_device_properties(i).total_memory
                    
                    gpu_memory = {
                        'device_id': i,
                        'allocated_mb': allocated / (1024**2),
                        'reserved_mb': reserved / (1024**2),
                        'max_allocated_mb': max_allocated / (1024**2),
                        'max_reserved_mb': max_reserved / (1024**2),
                        'total_mb': total_memory / (1024**2),
                        'free_mb': (total_memory - reserved) / (1024**2),
                        'utilization_percent': (allocated / total_memory) * 100
                    }
                    
                    memory_info['gpu_memory'].append(gpu_memory)
                    
                except Exception as e:
                    self.logger.error(f"è·å–GPU {i} å†…å­˜ä¿¡æ¯å¤±è´¥: {e}")
        
        return memory_info
    
    def _get_system_memory(self) -> Dict:
        """è·å–ç³»ç»Ÿå†…å­˜ä½¿ç”¨æƒ…å†µ"""
        memory = psutil.virtual_memory()
        return {
            'total_gb': memory.total / (1024**3),
            'available_gb': memory.available / (1024**3),
            'used_gb': memory.used / (1024**3),
            'percent': memory.percent
        }
    
    def analyze_model_parameters(self, model, model_name: str = "model") -> Dict:
        """
        åˆ†ææ¨¡å‹å‚æ•°çš„æ˜¾å­˜å ç”¨æƒ…å†µ
        
        Args:
            model: å¾…åˆ†æçš„æ¨¡å‹
            model_name: æ¨¡å‹åç§°
            
        Returns:
            å‚æ•°åˆ†æç»“æœ
        """
        self.logger.info(f"å¼€å§‹åˆ†ææ¨¡å‹å‚æ•°: {model_name}")
        
        analysis_result = {
            'model_name': model_name,
            'timestamp': datetime.now().isoformat(),
            'parameter_groups': {},
            'device_distribution': defaultdict(list),
            'memory_usage': defaultdict(float),
            'parameter_counts': defaultdict(int),
            'gradient_info': {}
        }
        
        total_params = 0
        total_trainable_params = 0
        total_memory_mb = 0
        
        # åˆ†ææ¯ä¸ªå‚æ•°
        for name, param in model.named_parameters():
            param_info = self._analyze_parameter(name, param)
            
            # æŒ‰å±‚çº§åˆ†ç»„
            layer_group = self._get_layer_group(name)
            if layer_group not in analysis_result['parameter_groups']:
                analysis_result['parameter_groups'][layer_group] = {
                    'parameters': [],
                    'total_params': 0,
                    'trainable_params': 0,
                    'memory_mb': 0
                }
            
            analysis_result['parameter_groups'][layer_group]['parameters'].append(param_info)
            analysis_result['parameter_groups'][layer_group]['total_params'] += param_info['num_params']
            analysis_result['parameter_groups'][layer_group]['memory_mb'] += param_info['memory_mb']
            
            if param_info['requires_grad']:
                analysis_result['parameter_groups'][layer_group]['trainable_params'] += param_info['num_params']
            
            # è®¾å¤‡åˆ†å¸ƒç»Ÿè®¡
            device_str = str(param_info['device'])
            analysis_result['device_distribution'][device_str].append({
                'name': name,
                'memory_mb': param_info['memory_mb'],
                'params': param_info['num_params']
            })
            
            analysis_result['memory_usage'][device_str] += param_info['memory_mb']
            analysis_result['parameter_counts'][device_str] += param_info['num_params']
            
            total_params += param_info['num_params']
            total_memory_mb += param_info['memory_mb']
            
            if param_info['requires_grad']:
                total_trainable_params += param_info['num_params']
        
        # åˆ†ææ¢¯åº¦ä¿¡æ¯
        analysis_result['gradient_info'] = self._analyze_gradients(model)
        
        # æ±‡æ€»ç»Ÿè®¡
        analysis_result['summary'] = {
            'total_parameters': total_params,
            'trainable_parameters': total_trainable_params,
            'total_memory_mb': total_memory_mb,
            'memory_per_param_bytes': (total_memory_mb * 1024 * 1024) / total_params if total_params > 0 else 0
        }
        
        # è®°å½•åˆ°æ³¨å†Œè¡¨
        self.parameter_registry[model_name] = analysis_result
        
        # è¯¦ç»†æ—¥å¿—è¾“å‡º
        self._log_parameter_analysis(analysis_result)
        
        return analysis_result
    
    def _analyze_parameter(self, name: str, param: torch.Tensor) -> Dict:
        """åˆ†æå•ä¸ªå‚æ•°çš„è¯¦ç»†ä¿¡æ¯"""
        param_info = {
            'name': name,
            'shape': list(param.shape),
            'dtype': str(param.dtype),
            'device': str(param.device),
            'requires_grad': param.requires_grad,
            'num_params': param.numel(),
            'memory_mb': param.numel() * param.element_size() / (1024**2),
            'is_lora': 'lora' in name.lower(),
            'is_embedding': 'embed' in name.lower(),
            'is_linear': any(keyword in name.lower() for keyword in ['linear', 'proj', 'fc']),
            'is_norm': any(keyword in name.lower() for keyword in ['norm', 'layer_norm', 'batch_norm']),
            'is_attention': any(keyword in name.lower() for keyword in ['attn', 'attention', 'q_proj', 'k_proj', 'v_proj']),
        }
        
        # åˆ†æå‚æ•°æ•°æ®èŒƒå›´
        if param.is_cuda:
            try:
                param_info['data_stats'] = {
                    'min': param.min().item(),
                    'max': param.max().item(),
                    'mean': param.mean().item(),
                    'std': param.std().item(),
                    'has_nan': torch.isnan(param).any().item(),
                    'has_inf': torch.isinf(param).any().item()
                }
            except:
                param_info['data_stats'] = {'error': 'Cannot compute stats'}
        
        return param_info
    
    def _get_layer_group(self, param_name: str) -> str:
        """æ ¹æ®å‚æ•°åç¡®å®šå±‚çº§åˆ†ç»„"""
        name_lower = param_name.lower()
        
        if 'lora' in name_lower:
            return 'lora_adapters'
        elif 'embed' in name_lower:
            return 'embeddings'
        elif 'lm_head' in name_lower or 'classifier' in name_lower:
            return 'output_layers'
        elif 'norm' in name_lower:
            return 'normalization'
        elif any(keyword in name_lower for keyword in ['attn', 'attention', 'q_proj', 'k_proj', 'v_proj', 'o_proj']):
            return 'attention_layers'
        elif any(keyword in name_lower for keyword in ['mlp', 'feed_forward', 'ffn', 'gate_proj', 'up_proj', 'down_proj']):
            return 'feedforward_layers'
        elif 'layers' in name_lower:
            return 'transformer_layers'
        else:
            return 'other'
    
    def _analyze_gradients(self, model) -> Dict:
        """åˆ†ææ¢¯åº¦ä¿¡æ¯"""
        gradient_info = {
            'total_gradient_memory_mb': 0,
            'gradient_count': 0,
            'gradient_distribution': defaultdict(list),
            'gradient_stats': {}
        }
        
        for name, param in model.named_parameters():
            if param.requires_grad and param.grad is not None:
                grad_memory_mb = param.grad.numel() * param.grad.element_size() / (1024**2)
                gradient_info['total_gradient_memory_mb'] += grad_memory_mb
                gradient_info['gradient_count'] += 1
                
                device_str = str(param.device)
                gradient_info['gradient_distribution'][device_str].append({
                    'name': name,
                    'memory_mb': grad_memory_mb,
                    'shape': list(param.grad.shape)
                })
                
                # æ¢¯åº¦ç»Ÿè®¡
                try:
                    gradient_info['gradient_stats'][name] = {
                        'norm': param.grad.norm().item(),
                        'min': param.grad.min().item(),
                        'max': param.grad.max().item(),
                        'mean': param.grad.mean().item(),
                        'std': param.grad.std().item()
                    }
                except:
                    gradient_info['gradient_stats'][name] = {'error': 'Cannot compute gradient stats'}
        
        return gradient_info
    
    def _log_parameter_analysis(self, analysis_result: Dict):
        """è®°å½•å‚æ•°åˆ†æç»“æœåˆ°æ—¥å¿—"""
        self.logger.info("=" * 80)
        self.logger.info(f"æ¨¡å‹å‚æ•°åˆ†æç»“æœ: {analysis_result['model_name']}")
        self.logger.info("=" * 80)
        
        summary = analysis_result['summary']
        self.logger.info(f"æ€»å‚æ•°æ•°é‡: {summary['total_parameters']:,}")
        self.logger.info(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {summary['trainable_parameters']:,}")
        self.logger.info(f"æ€»å†…å­˜å ç”¨: {summary['total_memory_mb']:.2f} MB")
        self.logger.info(f"å¹³å‡æ¯å‚æ•°å†…å­˜: {summary['memory_per_param_bytes']:.2f} bytes")
        
        # æŒ‰å±‚çº§åˆ†ç»„ç»Ÿè®¡
        self.logger.info("\næŒ‰å±‚çº§åˆ†ç»„ç»Ÿè®¡:")
        for group_name, group_info in analysis_result['parameter_groups'].items():
            self.logger.info(f"  {group_name}:")
            self.logger.info(f"    å‚æ•°æ•°é‡: {group_info['total_params']:,}")
            self.logger.info(f"    å¯è®­ç»ƒå‚æ•°: {group_info['trainable_params']:,}")
            self.logger.info(f"    å†…å­˜å ç”¨: {group_info['memory_mb']:.2f} MB")
            
            # åˆ—å‡ºè¯¥ç»„ä¸­å†…å­˜å ç”¨æœ€å¤§çš„å‚æ•°
            if group_info['parameters']:
                top_params = sorted(group_info['parameters'], key=lambda x: x['memory_mb'], reverse=True)[:3]
                self.logger.info(f"    å†…å­˜å ç”¨æœ€å¤§çš„å‚æ•°:")
                for param in top_params:
                    self.logger.info(f"      {param['name']}: {param['memory_mb']:.2f} MB ({param['shape']})")
        
        # è®¾å¤‡åˆ†å¸ƒç»Ÿè®¡
        self.logger.info("\nè®¾å¤‡åˆ†å¸ƒç»Ÿè®¡:")
        for device, params in analysis_result['device_distribution'].items():
            total_memory = sum(p['memory_mb'] for p in params)
            total_params = sum(p['params'] for p in params)
            self.logger.info(f"  {device}: {len(params)} ä¸ªå‚æ•°, {total_params:,} ä¸ªå…ƒç´ , {total_memory:.2f} MB")
        
        # æ¢¯åº¦ä¿¡æ¯
        gradient_info = analysis_result['gradient_info']
        if gradient_info['gradient_count'] > 0:
            self.logger.info(f"\næ¢¯åº¦ä¿¡æ¯:")
            self.logger.info(f"  æœ‰æ¢¯åº¦çš„å‚æ•°æ•°é‡: {gradient_info['gradient_count']}")
            self.logger.info(f"  æ¢¯åº¦æ€»å†…å­˜å ç”¨: {gradient_info['total_gradient_memory_mb']:.2f} MB")
    
    def create_memory_checkpoint(self, checkpoint_name: str, model=None, additional_info: Dict = None):
        """
        åˆ›å»ºå†…å­˜æ£€æŸ¥ç‚¹
        
        Args:
            checkpoint_name: æ£€æŸ¥ç‚¹åç§°
            model: æ¨¡å‹å¯¹è±¡ï¼ˆå¯é€‰ï¼‰
            additional_info: é¢å¤–ä¿¡æ¯
        """
        self.logger.info(f"åˆ›å»ºå†…å­˜æ£€æŸ¥ç‚¹: {checkpoint_name}")
        
        checkpoint = {
            'name': checkpoint_name,
            'timestamp': datetime.now().isoformat(),
            'memory_usage': self.get_current_memory_usage(),
            'additional_info': additional_info or {}
        }
        
        if model is not None:
            checkpoint['model_analysis'] = self.analyze_model_parameters(model, checkpoint_name)
        
        self.checkpoint_history.append(checkpoint)
        
        # ä¿å­˜æ£€æŸ¥ç‚¹åˆ°æ–‡ä»¶
        checkpoint_file = os.path.join(self.log_dir, f"checkpoint_{checkpoint_name}_{int(time.time())}.json")
        with open(checkpoint_file, 'w') as f:
            json.dump(checkpoint, f, indent=2, default=str)
        
        self.logger.info(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_file}")
        
        return checkpoint
    
    def start_real_time_monitoring(self, interval: float = 1.0):
        """
        å¯åŠ¨å®æ—¶å†…å­˜ç›‘æ§
        
        Args:
            interval: ç›‘æ§é—´éš”ï¼ˆç§’ï¼‰
        """
        if not self.enable_real_time:
            self.logger.warning("å®æ—¶ç›‘æ§æœªå¯ç”¨")
            return
        
        if self.monitoring_active:
            self.logger.warning("å®æ—¶ç›‘æ§å·²åœ¨è¿è¡Œ")
            return
        
        self.monitoring_active = True
        self.logger.info(f"å¯åŠ¨å®æ—¶å†…å­˜ç›‘æ§ï¼Œé—´éš”: {interval}ç§’")
        
        def monitor_loop():
            while self.monitoring_active:
                try:
                    memory_info = self.get_current_memory_usage()
                    self.memory_history.append(memory_info)
                    
                    # æ£€æŸ¥å†…å­˜å¼‚å¸¸
                    self._check_memory_anomalies(memory_info)
                    
                    # é™åˆ¶å†å²è®°å½•é•¿åº¦
                    if len(self.memory_history) > 1000:
                        self.memory_history = self.memory_history[-1000:]
                    
                    time.sleep(interval)
                    
                except Exception as e:
                    self.logger.error(f"å®æ—¶ç›‘æ§å‡ºé”™: {e}")
                    time.sleep(interval)
        
        self.monitor_thread = threading.Thread(target=monitor_loop, daemon=True)
        self.monitor_thread.start()
    
    def stop_real_time_monitoring(self):
        """åœæ­¢å®æ—¶å†…å­˜ç›‘æ§"""
        if self.monitoring_active:
            self.monitoring_active = False
            self.logger.info("åœæ­¢å®æ—¶å†…å­˜ç›‘æ§")
            
            # ä¿å­˜ç›‘æ§å†å²
            self.save_monitoring_history()
    
    def _check_memory_anomalies(self, memory_info: Dict):
        """æ£€æŸ¥å†…å­˜å¼‚å¸¸"""
        for gpu_mem in memory_info['gpu_memory']:
            # æ£€æŸ¥å†…å­˜ä½¿ç”¨ç‡æ˜¯å¦è¿‡é«˜
            if gpu_mem['utilization_percent'] > 90:
                self.logger.warning(f"GPU {gpu_mem['device_id']} å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {gpu_mem['utilization_percent']:.1f}%")
            
            # æ£€æŸ¥å†…å­˜åˆ†é…æ˜¯å¦å¼‚å¸¸å¢é•¿
            if len(self.memory_history) > 5:
                recent_usage = [h['gpu_memory'][gpu_mem['device_id']]['allocated_mb'] 
                              for h in self.memory_history[-5:]]
                if len(recent_usage) > 1:
                    growth_rate = (recent_usage[-1] - recent_usage[0]) / len(recent_usage)
                    if growth_rate > 100:  # æ¯æ¬¡å¢é•¿è¶…è¿‡100MB
                        self.logger.warning(f"GPU {gpu_mem['device_id']} å†…å­˜å¢é•¿å¼‚å¸¸: {growth_rate:.2f} MB/step")
    
    def save_monitoring_history(self):
        """ä¿å­˜ç›‘æ§å†å²åˆ°æ–‡ä»¶"""
        if not self.memory_history:
            return
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        history_file = os.path.join(self.log_dir, f"memory_history_{timestamp}.json")
        
        with open(history_file, 'w') as f:
            json.dump(self.memory_history, f, indent=2, default=str)
        
        self.logger.info(f"ç›‘æ§å†å²å·²ä¿å­˜: {history_file}")
    
    def compare_checkpoints(self, checkpoint1_name: str, checkpoint2_name: str) -> Dict:
        """
        æ¯”è¾ƒä¸¤ä¸ªæ£€æŸ¥ç‚¹çš„å†…å­˜ä½¿ç”¨æƒ…å†µ
        
        Args:
            checkpoint1_name: ç¬¬ä¸€ä¸ªæ£€æŸ¥ç‚¹åç§°
            checkpoint2_name: ç¬¬äºŒä¸ªæ£€æŸ¥ç‚¹åç§°
            
        Returns:
            æ¯”è¾ƒç»“æœ
        """
        checkpoint1 = None
        checkpoint2 = None
        
        for checkpoint in self.checkpoint_history:
            if checkpoint['name'] == checkpoint1_name:
                checkpoint1 = checkpoint
            elif checkpoint['name'] == checkpoint2_name:
                checkpoint2 = checkpoint
        
        if not checkpoint1 or not checkpoint2:
            self.logger.error(f"æ‰¾ä¸åˆ°æ£€æŸ¥ç‚¹: {checkpoint1_name} æˆ– {checkpoint2_name}")
            return {}
        
        comparison = {
            'checkpoint1': checkpoint1_name,
            'checkpoint2': checkpoint2_name,
            'gpu_memory_diff': [],
            'model_parameter_diff': {}
        }
        
        # æ¯”è¾ƒGPUå†…å­˜
        for i, (gpu1, gpu2) in enumerate(zip(checkpoint1['memory_usage']['gpu_memory'], 
                                           checkpoint2['memory_usage']['gpu_memory'])):
            diff = {
                'device_id': i,
                'allocated_diff_mb': gpu2['allocated_mb'] - gpu1['allocated_mb'],
                'reserved_diff_mb': gpu2['reserved_mb'] - gpu1['reserved_mb'],
                'utilization_diff_percent': gpu2['utilization_percent'] - gpu1['utilization_percent']
            }
            comparison['gpu_memory_diff'].append(diff)
        
        # æ¯”è¾ƒæ¨¡å‹å‚æ•°ï¼ˆå¦‚æœæœ‰ï¼‰
        if 'model_analysis' in checkpoint1 and 'model_analysis' in checkpoint2:
            model1 = checkpoint1['model_analysis']
            model2 = checkpoint2['model_analysis']
            
            comparison['model_parameter_diff'] = {
                'total_params_diff': model2['summary']['total_parameters'] - model1['summary']['total_parameters'],
                'trainable_params_diff': model2['summary']['trainable_parameters'] - model1['summary']['trainable_parameters'],
                'memory_diff_mb': model2['summary']['total_memory_mb'] - model1['summary']['total_memory_mb']
            }
        
        # è®°å½•æ¯”è¾ƒç»“æœ
        self.logger.info(f"æ£€æŸ¥ç‚¹æ¯”è¾ƒç»“æœ: {checkpoint1_name} vs {checkpoint2_name}")
        for diff in comparison['gpu_memory_diff']:
            self.logger.info(f"  GPU {diff['device_id']}: å†…å­˜åˆ†é…å·®å¼‚ {diff['allocated_diff_mb']:.2f} MB, "
                           f"åˆ©ç”¨ç‡å·®å¼‚ {diff['utilization_diff_percent']:.1f}%")
        
        return comparison
    
    def generate_memory_report(self, output_file: str = None) -> str:
        """
        ç”Ÿæˆå†…å­˜ä½¿ç”¨æŠ¥å‘Š
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            æŠ¥å‘Šå†…å®¹
        """
        report_lines = []
        report_lines.append("=" * 80)
        report_lines.append("å†…å­˜ä½¿ç”¨æŠ¥å‘Š")
        report_lines.append("=" * 80)
        report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().isoformat()}")
        report_lines.append(f"ç›‘æ§å†å²è®°å½•æ•°: {len(self.memory_history)}")
        report_lines.append(f"æ£€æŸ¥ç‚¹æ•°é‡: {len(self.checkpoint_history)}")
        report_lines.append("")
        
        # å½“å‰å†…å­˜çŠ¶æ€
        current_memory = self.get_current_memory_usage()
        report_lines.append("å½“å‰å†…å­˜çŠ¶æ€:")
        for gpu_mem in current_memory['gpu_memory']:
            report_lines.append(f"  GPU {gpu_mem['device_id']}: "
                              f"{gpu_mem['allocated_mb']:.2f} MB / {gpu_mem['total_mb']:.2f} MB "
                              f"({gpu_mem['utilization_percent']:.1f}%)")
        
        # ç³»ç»Ÿå†…å­˜
        sys_mem = current_memory['system_memory']
        report_lines.append(f"ç³»ç»Ÿå†…å­˜: {sys_mem['used_gb']:.2f} GB / {sys_mem['total_gb']:.2f} GB "
                          f"({sys_mem['percent']:.1f}%)")
        report_lines.append("")
        
        # æ¨¡å‹å‚æ•°ç»Ÿè®¡
        if self.parameter_registry:
            report_lines.append("æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
            for model_name, analysis in self.parameter_registry.items():
                summary = analysis['summary']
                report_lines.append(f"  {model_name}:")
                report_lines.append(f"    æ€»å‚æ•°: {summary['total_parameters']:,}")
                report_lines.append(f"    å¯è®­ç»ƒå‚æ•°: {summary['trainable_parameters']:,}")
                report_lines.append(f"    å†…å­˜å ç”¨: {summary['total_memory_mb']:.2f} MB")
            report_lines.append("")
        
        # æ£€æŸ¥ç‚¹å†å²
        if self.checkpoint_history:
            report_lines.append("æ£€æŸ¥ç‚¹å†å²:")
            for checkpoint in self.checkpoint_history[-5:]:  # æœ€è¿‘5ä¸ªæ£€æŸ¥ç‚¹
                report_lines.append(f"  {checkpoint['name']} ({checkpoint['timestamp']})")
                for gpu_mem in checkpoint['memory_usage']['gpu_memory']:
                    report_lines.append(f"    GPU {gpu_mem['device_id']}: {gpu_mem['allocated_mb']:.2f} MB")
            report_lines.append("")
        
        # å†…å­˜ä½¿ç”¨è¶‹åŠ¿
        if len(self.memory_history) > 1:
            report_lines.append("å†…å­˜ä½¿ç”¨è¶‹åŠ¿:")
            first_record = self.memory_history[0]
            last_record = self.memory_history[-1]
            
            for i, (first_gpu, last_gpu) in enumerate(zip(first_record['gpu_memory'], 
                                                         last_record['gpu_memory'])):
                trend = last_gpu['allocated_mb'] - first_gpu['allocated_mb']
                report_lines.append(f"  GPU {i}: å˜åŒ– {trend:+.2f} MB")
        
        report_content = "\n".join(report_lines)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = os.path.join(self.log_dir, f"memory_report_{timestamp}.txt")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        self.logger.info(f"å†…å­˜æŠ¥å‘Šå·²ä¿å­˜: {output_file}")
        
        return report_content
    
    def generate_detailed_memory_json_report(self, model, model_name: str = "model", 
                                          optimizer=None, stage: str = "training") -> Dict:
        """
        ç”Ÿæˆè¯¦ç»†çš„JSONæ ¼å¼æ˜¾å­˜å ç”¨ç»Ÿè®¡è¡¨
        
        Args:
            model: æ¨¡å‹å¯¹è±¡
            model_name: æ¨¡å‹åç§°
            optimizer: ä¼˜åŒ–å™¨å¯¹è±¡ï¼ˆå¯é€‰ï¼‰
            stage: è®­ç»ƒé˜¶æ®µï¼ˆmodel_creation, training, validationç­‰ï¼‰
            
        Returns:
            è¯¦ç»†çš„JSONæ ¼å¼ç»Ÿè®¡è¡¨
        """
        self.logger.info(f"ğŸ” ç”Ÿæˆè¯¦ç»†æ˜¾å­˜å ç”¨ç»Ÿè®¡è¡¨ - é˜¶æ®µ: {stage}")
        
        # è·å–å½“å‰GPUå†…å­˜çŠ¶æ€
        gpu_memory = self.get_current_memory_usage()
        
        # åŸºç¡€ç»Ÿè®¡ç»“æ„
        memory_report = {
            "stage": stage,
            "model_name": model_name,
            "timestamp": datetime.now().isoformat(),
            "gpu_memory_status": gpu_memory,
            "detailed_memory_breakdown": {},
            "memory_summary": {},
            "occupancy_percentages": {}
        }
        
        # 1. åˆ†ææ¨¡å‹å‚æ•°
        self.logger.info("ğŸ“Š åˆ†ææ¨¡å‹å‚æ•°...")
        parameters_analysis = self._analyze_model_parameters_detailed(model)
        memory_report["detailed_memory_breakdown"]["parameters"] = parameters_analysis
        
        # 2. åˆ†ææ¢¯åº¦
        self.logger.info("ğŸ“Š åˆ†ææ¢¯åº¦...")
        gradients_analysis = self._analyze_gradients_detailed(model)
        memory_report["detailed_memory_breakdown"]["gradients"] = gradients_analysis
        
        # 3. åˆ†æä¼˜åŒ–å™¨çŠ¶æ€
        if optimizer is not None:
            self.logger.info("ğŸ“Š åˆ†æä¼˜åŒ–å™¨çŠ¶æ€...")
            optimizer_analysis = self._analyze_optimizer_detailed(optimizer)
            memory_report["detailed_memory_breakdown"]["optimizer"] = optimizer_analysis
        else:
            memory_report["detailed_memory_breakdown"]["optimizer"] = {
                "total_memory_mb": 0,
                "states": {},
                "message": "No optimizer provided"
            }
        
        # 4. åˆ†æåŸºç¡€æ¨¡å‹ç»„ä»¶
        self.logger.info("ğŸ“Š åˆ†æåŸºç¡€æ¨¡å‹ç»„ä»¶...")
        base_model_analysis = self._analyze_base_model_components(model)
        memory_report["detailed_memory_breakdown"]["base_model"] = base_model_analysis
        
        # 5. ä¼°ç®—æ¿€æ´»å€¼å†…å­˜ï¼ˆè¿‘ä¼¼ï¼‰
        self.logger.info("ğŸ“Š ä¼°ç®—æ¿€æ´»å€¼å†…å­˜...")
        activation_analysis = self._estimate_activation_memory(model)
        memory_report["detailed_memory_breakdown"]["activations"] = activation_analysis
        
        # 6. è®¡ç®—æ€»ä½“ç»Ÿè®¡å’Œå ç”¨ç‡
        self.logger.info("ğŸ“Š è®¡ç®—æ€»ä½“ç»Ÿè®¡...")
        memory_report["memory_summary"] = self._calculate_memory_summary(memory_report["detailed_memory_breakdown"])
        memory_report["occupancy_percentages"] = self._calculate_occupancy_percentages(
            memory_report["memory_summary"], gpu_memory
        )
        
        # 7. ä¿å­˜JSONæŠ¥å‘Š
        self.logger.info("ğŸ’¾ ä¿å­˜JSONæŠ¥å‘Š...")
        self._save_json_report(memory_report, stage)
        
        return memory_report
    
    def _analyze_model_parameters_detailed(self, model) -> Dict:
        """è¯¦ç»†åˆ†ææ¨¡å‹å‚æ•°"""
        parameters_info = {
            "total_memory_mb": 0,
            "total_parameters": 0,
            "trainable_parameters": 0,
            "frozen_parameters": 0,
            "parameter_groups": {},
            "device_distribution": {},
            "dtype_distribution": {},
            "detailed_parameters": []
        }
        
        # æŒ‰å‚æ•°ç»„åˆ†ç±»
        param_groups = {
            "base_model": [],
            "lora_adapters": [],
            "embeddings": [],
            "attention_layers": [],
            "feedforward_layers": [],
            "normalization": [],
            "output_layers": [],
            "other": []
        }
        
        for name, param in model.named_parameters():
            param_info = {
                "name": name,
                "shape": list(param.shape),
                "dtype": str(param.dtype),
                "device": str(param.device),
                "requires_grad": param.requires_grad,
                "num_params": param.numel(),
                "memory_mb": param.numel() * param.element_size() / (1024**2),
                "memory_bytes": param.numel() * param.element_size(),
                "element_size": param.element_size()
            }
            
            # åˆ†ç±»åˆ°å¯¹åº”çš„ç»„
            group = self._classify_parameter_group(name)
            param_groups[group].append(param_info)
            
            # ç´¯è®¡ç»Ÿè®¡
            parameters_info["total_memory_mb"] += param_info["memory_mb"]
            parameters_info["total_parameters"] += param_info["num_params"]
            
            if param.requires_grad:
                parameters_info["trainable_parameters"] += param_info["num_params"]
            else:
                parameters_info["frozen_parameters"] += param_info["num_params"]
            
            # è®¾å¤‡åˆ†å¸ƒ
            device_str = str(param.device)
            if device_str not in parameters_info["device_distribution"]:
                parameters_info["device_distribution"][device_str] = {"memory_mb": 0, "count": 0}
            parameters_info["device_distribution"][device_str]["memory_mb"] += param_info["memory_mb"]
            parameters_info["device_distribution"][device_str]["count"] += 1
            
            # æ•°æ®ç±»å‹åˆ†å¸ƒ
            dtype_str = str(param.dtype)
            if dtype_str not in parameters_info["dtype_distribution"]:
                parameters_info["dtype_distribution"][dtype_str] = {"memory_mb": 0, "count": 0}
            parameters_info["dtype_distribution"][dtype_str]["memory_mb"] += param_info["memory_mb"]
            parameters_info["dtype_distribution"][dtype_str]["count"] += 1
            
            parameters_info["detailed_parameters"].append(param_info)
        
        # æ±‡æ€»å‚æ•°ç»„ä¿¡æ¯
        for group_name, group_params in param_groups.items():
            parameters_info["parameter_groups"][group_name] = {
                "count": len(group_params),
                "total_memory_mb": sum(p["memory_mb"] for p in group_params),
                "total_parameters": sum(p["num_params"] for p in group_params),
                "trainable_parameters": sum(p["num_params"] for p in group_params if p["requires_grad"]),
                "parameters": group_params
            }
        
        return parameters_info
    
    def _analyze_gradients_detailed(self, model) -> Dict:
        """è¯¦ç»†åˆ†ææ¢¯åº¦"""
        gradients_info = {
            "total_memory_mb": 0,
            "gradient_count": 0,
            "gradient_groups": {},
            "device_distribution": {},
            "detailed_gradients": []
        }
        
        # æŒ‰æ¢¯åº¦ç»„åˆ†ç±»
        grad_groups = {
            "base_model": [],
            "lora_adapters": [],
            "embeddings": [],
            "attention_layers": [],
            "feedforward_layers": [],
            "normalization": [],
            "output_layers": [],
            "other": []
        }
        
        for name, param in model.named_parameters():
            if param.requires_grad and param.grad is not None:
                grad_info = {
                    "name": name,
                    "shape": list(param.grad.shape),
                    "dtype": str(param.grad.dtype),
                    "device": str(param.grad.device),
                    "memory_mb": param.grad.numel() * param.grad.element_size() / (1024**2),
                    "memory_bytes": param.grad.numel() * param.grad.element_size(),
                    "element_size": param.grad.element_size()
                }
                
                # åˆ†ç±»åˆ°å¯¹åº”çš„ç»„
                group = self._classify_parameter_group(name)
                grad_groups[group].append(grad_info)
                
                # ç´¯è®¡ç»Ÿè®¡
                gradients_info["total_memory_mb"] += grad_info["memory_mb"]
                gradients_info["gradient_count"] += 1
                
                # è®¾å¤‡åˆ†å¸ƒ
                device_str = str(param.grad.device)
                if device_str not in gradients_info["device_distribution"]:
                    gradients_info["device_distribution"][device_str] = {"memory_mb": 0, "count": 0}
                gradients_info["device_distribution"][device_str]["memory_mb"] += grad_info["memory_mb"]
                gradients_info["device_distribution"][device_str]["count"] += 1
                
                gradients_info["detailed_gradients"].append(grad_info)
        
        # æ±‡æ€»æ¢¯åº¦ç»„ä¿¡æ¯
        for group_name, group_grads in grad_groups.items():
            gradients_info["gradient_groups"][group_name] = {
                "count": len(group_grads),
                "total_memory_mb": sum(g["memory_mb"] for g in group_grads),
                "gradients": group_grads
            }
        
        return gradients_info
    
    def _analyze_optimizer_detailed(self, optimizer) -> Dict:
        """è¯¦ç»†åˆ†æä¼˜åŒ–å™¨çŠ¶æ€"""
        optimizer_info = {
            "total_memory_mb": 0,
            "optimizer_type": type(optimizer).__name__,
            "states": {},
            "param_groups": [],
            "device_distribution": {}
        }
        
        try:
            # åˆ†æä¼˜åŒ–å™¨çŠ¶æ€
            for param_id, param in enumerate(optimizer.param_groups):
                group_info = {
                    "group_id": param_id,
                    "lr": param.get('lr', 0),
                    "weight_decay": param.get('weight_decay', 0),
                    "params_count": len(param['params']),
                    "params_memory_mb": 0
                }
                
                for param_tensor in param['params']:
                    if param_tensor in optimizer.state:
                        state = optimizer.state[param_tensor]
                        param_memory = 0
                        
                        # è®¡ç®—çŠ¶æ€å ç”¨çš„å†…å­˜
                        for state_key, state_value in state.items():
                            if hasattr(state_value, 'numel') and hasattr(state_value, 'element_size'):
                                state_memory_mb = state_value.numel() * state_value.element_size() / (1024**2)
                                param_memory += state_memory_mb
                                
                                # è®¾å¤‡åˆ†å¸ƒ
                                device_str = str(state_value.device)
                                if device_str not in optimizer_info["device_distribution"]:
                                    optimizer_info["device_distribution"][device_str] = {"memory_mb": 0, "states": 0}
                                optimizer_info["device_distribution"][device_str]["memory_mb"] += state_memory_mb
                                optimizer_info["device_distribution"][device_str]["states"] += 1
                        
                        group_info["params_memory_mb"] += param_memory
                
                optimizer_info["param_groups"].append(group_info)
                optimizer_info["total_memory_mb"] += group_info["params_memory_mb"]
        
        except Exception as e:
            optimizer_info["error"] = f"Failed to analyze optimizer: {str(e)}"
        
        return optimizer_info
    
    def _analyze_base_model_components(self, model) -> Dict:
        """åˆ†æåŸºç¡€æ¨¡å‹ç»„ä»¶"""
        base_model_info = {
            "model_type": type(model).__name__,
            "total_memory_mb": 0,
            "components": {},
            "layer_analysis": {}
        }
        
        # åˆ†ææ¨¡å‹ç»“æ„
        try:
            for name, module in model.named_modules():
                if hasattr(module, 'weight') and module.weight is not None:
                    weight_memory = module.weight.numel() * module.weight.element_size() / (1024**2)
                    
                    if hasattr(module, 'bias') and module.bias is not None:
                        bias_memory = module.bias.numel() * module.bias.element_size() / (1024**2)
                    else:
                        bias_memory = 0
                    
                    component_info = {
                        "module_type": type(module).__name__,
                        "weight_memory_mb": weight_memory,
                        "bias_memory_mb": bias_memory,
                        "total_memory_mb": weight_memory + bias_memory
                    }
                    
                    base_model_info["components"][name] = component_info
                    base_model_info["total_memory_mb"] += component_info["total_memory_mb"]
        
        except Exception as e:
            base_model_info["error"] = f"Failed to analyze base model: {str(e)}"
        
        return base_model_info
    
    def _estimate_activation_memory(self, model) -> Dict:
        """åˆ†ææ¿€æ´»å€¼å†…å­˜"""
        try:
            # ä½¿ç”¨è¯¦ç»†æ–¹æ³•ä¼°ç®—æ¿€æ´»å€¼å†…å­˜
            activation_details = self._estimate_activation_memory_detailed(model)
            
            # æ„é€ ä¸åŸæœ‰æ ¼å¼å…¼å®¹çš„ç»“æœ
            activation_info = {
                "estimated_memory_mb": activation_details["total_memory_mb"],
                "estimation_method": activation_details["estimation_method"],
                "notes": activation_details["notes"],
                "model_config": activation_details["model_config"],
                "breakdown": activation_details["breakdown"]
            }
            
            return activation_info
            
        except Exception as e:
            return {
                "estimated_memory_mb": 0,
                "estimation_method": "failed",
                "error": f"Failed to estimate activation memory: {str(e)}"
            }
    
    def _classify_parameter_group(self, param_name: str) -> str:
        """åˆ†ç±»å‚æ•°ç»„"""
        name_lower = param_name.lower()
        
        if 'lora' in name_lower:
            return 'lora_adapters'
        elif 'embed' in name_lower:
            return 'embeddings'
        elif 'lm_head' in name_lower or 'classifier' in name_lower:
            return 'output_layers'
        elif 'norm' in name_lower:
            return 'normalization'
        elif any(keyword in name_lower for keyword in ['attn', 'attention', 'q_proj', 'k_proj', 'v_proj', 'o_proj']):
            return 'attention_layers'
        elif any(keyword in name_lower for keyword in ['mlp', 'feed_forward', 'ffn', 'gate_proj', 'up_proj', 'down_proj']):
            return 'feedforward_layers'
        elif 'base_model' in name_lower:
            return 'base_model'
        else:
            return 'other'
    
    def _calculate_memory_summary(self, memory_breakdown: Dict) -> Dict:
        """è®¡ç®—å†…å­˜æ€»ç»“"""
        summary = {
            "total_memory_mb": 0,
            "component_breakdown": {}
        }
        
        for component, info in memory_breakdown.items():
            component_memory = 0
            
            if isinstance(info, dict):
                # å°è¯•è·å– total_memory_mb å­—æ®µ
                if 'total_memory_mb' in info:
                    component_memory = info['total_memory_mb']
                elif 'estimated_memory_mb' in info:  # å¤„ç†æ¿€æ´»å€¼ä¼°ç®—
                    component_memory = info['estimated_memory_mb']
                elif isinstance(info, dict) and 'memory_mb' in info:
                    component_memory = info['memory_mb']
                else:
                    # å¦‚æœæ²¡æœ‰ç›´æ¥çš„å†…å­˜å­—æ®µï¼Œå°è¯•ä»å­ç»„ä»¶ç´¯åŠ 
                    if 'parameter_groups' in info:
                        for group_name, group_info in info['parameter_groups'].items():
                            if isinstance(group_info, dict) and 'total_memory_mb' in group_info:
                                component_memory += group_info['total_memory_mb']
                    elif 'gradient_groups' in info:
                        for group_name, group_info in info['gradient_groups'].items():
                            if isinstance(group_info, dict) and 'total_memory_mb' in group_info:
                                component_memory += group_info['total_memory_mb']
            elif isinstance(info, (int, float)):
                # å¦‚æœç›´æ¥æ˜¯æ•°å€¼
                component_memory = info
            else:
                # å…¶ä»–ç±»å‹ï¼Œè·³è¿‡
                self.logger.warning(f"è·³è¿‡ç»„ä»¶ {component}ï¼Œç±»å‹ä¸º {type(info)}")
                continue
            
            summary["component_breakdown"][component] = component_memory
            summary["total_memory_mb"] += component_memory
        
        return summary
    
    def _calculate_occupancy_percentages(self, memory_summary: Dict, gpu_memory: Dict) -> Dict:
        """è®¡ç®—å ç”¨ç‡ç™¾åˆ†æ¯”"""
        occupancy = {
            "total_used_percentage": 0,
            "component_percentages": {}
        }
        
        try:
            if gpu_memory and 'gpu_memory' in gpu_memory:
                gpu_memory_list = gpu_memory['gpu_memory']
                
                # å¤„ç†gpu_memoryæ˜¯åˆ—è¡¨çš„æƒ…å†µ
                if isinstance(gpu_memory_list, list) and len(gpu_memory_list) > 0:
                    # ä½¿ç”¨ç¬¬ä¸€ä¸ªGPUçš„ä¿¡æ¯è®¡ç®—å ç”¨ç‡
                    device_info = gpu_memory_list[0]
                    total_gpu_memory = device_info.get('total_mb', 0)
                    
                    if total_gpu_memory > 0:
                        # è®¡ç®—æ€»å ç”¨ç‡
                        occupancy["total_used_percentage"] = (memory_summary["total_memory_mb"] / total_gpu_memory) * 100
                        
                        # è®¡ç®—å„ç»„ä»¶å ç”¨ç‡
                        for component, component_memory in memory_summary["component_breakdown"].items():
                            occupancy["component_percentages"][component] = (component_memory / total_gpu_memory) * 100
                
                # å¤„ç†gpu_memoryæ˜¯å­—å…¸çš„æƒ…å†µï¼ˆå‘åå…¼å®¹ï¼‰
                elif isinstance(gpu_memory_list, dict):
                    for device_id, device_info in gpu_memory_list.items():
                        if 'total_memory_mb' in device_info:
                            total_gpu_memory = device_info['total_memory_mb']
                        elif 'total_mb' in device_info:
                            total_gpu_memory = device_info['total_mb']
                        else:
                            continue
                        
                        if total_gpu_memory > 0:
                            # è®¡ç®—æ€»å ç”¨ç‡
                            occupancy["total_used_percentage"] = (memory_summary["total_memory_mb"] / total_gpu_memory) * 100
                            
                            # è®¡ç®—å„ç»„ä»¶å ç”¨ç‡
                            for component, component_memory in memory_summary["component_breakdown"].items():
                                occupancy["component_percentages"][component] = (component_memory / total_gpu_memory) * 100
                            
                            break  # åªè®¡ç®—ç¬¬ä¸€ä¸ªGPUçš„å ç”¨ç‡
                else:
                    self.logger.warning(f"æ— æ³•è¯†åˆ«çš„gpu_memoryç±»å‹: {type(gpu_memory_list)}")
            else:
                self.logger.warning("æ²¡æœ‰æ‰¾åˆ°GPUå†…å­˜ä¿¡æ¯ï¼Œæ— æ³•è®¡ç®—å ç”¨ç‡")
                
        except Exception as e:
            self.logger.error(f"è®¡ç®—å ç”¨ç‡æ—¶å‡ºé”™: {e}")
            # è¿”å›é»˜è®¤å€¼ï¼Œé¿å…ç¨‹åºå´©æºƒ
            occupancy = {
                "total_used_percentage": 0,
                "component_percentages": {component: 0 for component in memory_summary.get("component_breakdown", {})}
            }
        
        return occupancy
    
    def _save_json_report(self, memory_report: Dict, stage: str):
        """ä¿å­˜JSONæŠ¥å‘Šåˆ°æ–‡ä»¶"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"memory_report_{stage}_{timestamp}.json"
        filepath = os.path.join(self.log_dir, filename)
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(memory_report, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ğŸ’¾ JSONæŠ¥å‘Šå·²ä¿å­˜: {filepath}")
            
            # åŒæ—¶ä¿å­˜ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬ç”¨äºå¿«é€ŸæŸ¥çœ‹
            simplified_report = {
                "stage": memory_report["stage"],
                "timestamp": memory_report["timestamp"],
                "memory_summary": memory_report["memory_summary"],
                "occupancy_percentages": memory_report["occupancy_percentages"],
                "gpu_memory_status": memory_report["gpu_memory_status"]
            }
            
            simplified_filename = f"memory_summary_{stage}_{timestamp}.json"
            simplified_filepath = os.path.join(self.log_dir, simplified_filename)
            
            with open(simplified_filepath, 'w', encoding='utf-8') as f:
                json.dump(simplified_report, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ğŸ’¾ ç®€åŒ–æŠ¥å‘Šå·²ä¿å­˜: {simplified_filepath}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜JSONæŠ¥å‘Šå¤±è´¥: {e}")
    
    def generate_brief_memory_report(self, model, model_name: str = "model", 
                                   optimizer=None, stage: str = "training") -> Dict:
        """
        ç”Ÿæˆç®€æ´çš„å†…å­˜å ç”¨æŠ¥å‘Š
        
        Args:
            model: æ¨¡å‹å¯¹è±¡
            model_name: æ¨¡å‹åç§°
            optimizer: ä¼˜åŒ–å™¨å¯¹è±¡ï¼ˆå¯é€‰ï¼‰
            stage: è®­ç»ƒé˜¶æ®µ
            
        Returns:
            ç®€æ´çš„å†…å­˜å ç”¨æŠ¥å‘Š
        """
        self.logger.info(f"ğŸ“Š ç”Ÿæˆç®€æ´å†…å­˜æŠ¥å‘Š - é˜¶æ®µ: {stage}")
        
        # è·å–å½“å‰GPUå†…å­˜çŠ¶æ€
        gpu_memory = self.get_current_memory_usage()
        
        # åˆå§‹åŒ–ç®€æ´æŠ¥å‘Šç»“æ„
        brief_report = {
            "stage": stage,
            "model_name": model_name,
            "timestamp": datetime.now().isoformat(),
            "gpu_memory_status": gpu_memory,
            "memory_breakdown": {
                "base_model": {"memory_mb": 0, "percentage": 0},
                "lora_parameters": {"memory_mb": 0, "percentage": 0},
                "gradients": {"memory_mb": 0, "percentage": 0},
                "optimizer_states": {"memory_mb": 0, "percentage": 0},
                "activations": {"memory_mb": 0, "percentage": 0}
            },
            "total_memory_mb": 0,
            "gpu_utilization_percentage": 0
        }
        
        # 1. åˆ†æåŸºç¡€æ¨¡å‹å†…å­˜
        base_model_memory = self._calculate_base_model_memory(model)
        brief_report["memory_breakdown"]["base_model"]["memory_mb"] = base_model_memory
        
        # 2. åˆ†æLoRAå‚æ•°å†…å­˜
        lora_memory = self._calculate_lora_memory(model)
        brief_report["memory_breakdown"]["lora_parameters"]["memory_mb"] = lora_memory
        
        # 3. åˆ†ææ¢¯åº¦å†…å­˜
        gradient_memory = self._calculate_gradient_memory(model)
        brief_report["memory_breakdown"]["gradients"]["memory_mb"] = gradient_memory
        
        # 4. åˆ†æä¼˜åŒ–å™¨çŠ¶æ€å†…å­˜
        optimizer_memory = 0
        if optimizer is not None:
            optimizer_memory = self._calculate_optimizer_memory(optimizer)
        brief_report["memory_breakdown"]["optimizer_states"]["memory_mb"] = optimizer_memory
        
        # 5. ä¼°ç®—æ¿€æ´»å€¼å†…å­˜ï¼ˆä½¿ç”¨è¯¦ç»†æ–¹æ³•ï¼‰
        activation_details = self._estimate_activation_memory_detailed(model)
        activation_memory = activation_details["total_memory_mb"]
        brief_report["memory_breakdown"]["activations"]["memory_mb"] = activation_memory
        brief_report["memory_breakdown"]["activations"]["details"] = activation_details
        
        # 6. è®¡ç®—æ€»å†…å­˜å’Œç™¾åˆ†æ¯”
        total_memory = (base_model_memory + lora_memory + gradient_memory + 
                       optimizer_memory + activation_memory)
        brief_report["total_memory_mb"] = total_memory
        
        # è®¡ç®—å„ç»„ä»¶å æ¯”
        if total_memory > 0:
            for component in brief_report["memory_breakdown"]:
                component_memory = brief_report["memory_breakdown"][component]["memory_mb"]
                percentage = (component_memory / total_memory) * 100
                brief_report["memory_breakdown"][component]["percentage"] = percentage
        
        # è®¡ç®—GPUåˆ©ç”¨ç‡
        try:
            if gpu_memory and 'gpu_memory' in gpu_memory:
                gpu_memory_list = gpu_memory['gpu_memory']
                
                # å¤„ç†gpu_memoryæ˜¯åˆ—è¡¨çš„æƒ…å†µ
                if isinstance(gpu_memory_list, list) and len(gpu_memory_list) > 0:
                    # ä½¿ç”¨ç¬¬ä¸€ä¸ªGPUçš„ä¿¡æ¯è®¡ç®—åˆ©ç”¨ç‡
                    device_info = gpu_memory_list[0]
                    if 'total_mb' in device_info:
                        gpu_total = device_info['total_mb']
                    elif 'total_memory_mb' in device_info:
                        gpu_total = device_info['total_memory_mb']
                    else:
                        gpu_total = 0
                    
                    if gpu_total > 0:
                        brief_report["gpu_utilization_percentage"] = (total_memory / gpu_total) * 100
                
                # å¤„ç†gpu_memoryæ˜¯å­—å…¸çš„æƒ…å†µï¼ˆå‘åå…¼å®¹ï¼‰
                elif isinstance(gpu_memory_list, dict):
                    for device_info in gpu_memory_list.values():
                        if 'total_memory_mb' in device_info:
                            gpu_total = device_info['total_memory_mb']
                        elif 'total_mb' in device_info:
                            gpu_total = device_info['total_mb']
                        else:
                            continue
                        
                        if gpu_total > 0:
                            brief_report["gpu_utilization_percentage"] = (total_memory / gpu_total) * 100
                        break
                else:
                    self.logger.warning(f"æ— æ³•è¯†åˆ«çš„gpu_memoryç±»å‹: {type(gpu_memory_list)}")
            else:
                self.logger.warning("æ²¡æœ‰æ‰¾åˆ°GPUå†…å­˜ä¿¡æ¯ï¼Œæ— æ³•è®¡ç®—GPUåˆ©ç”¨ç‡")
        except Exception as e:
            self.logger.error(f"è®¡ç®—GPUåˆ©ç”¨ç‡æ—¶å‡ºé”™: {e}")
            brief_report["gpu_utilization_percentage"] = 0
        
        # ä¿å­˜ç®€æ´æŠ¥å‘Š
        self._save_brief_report(brief_report, stage)
        
        # æ‰“å°åˆ°æ—¥å¿—
        self._log_brief_report(brief_report)
        
        return brief_report
    
    def _calculate_base_model_memory(self, model) -> float:
        """è®¡ç®—åŸºç¡€æ¨¡å‹å†…å­˜ï¼ˆä¸åŒ…æ‹¬LoRAï¼‰"""
        base_memory = 0
        for name, param in model.named_parameters():
            if 'lora' not in name.lower():
                base_memory += param.numel() * param.element_size() / (1024**2)
        return base_memory
    
    def _calculate_lora_memory(self, model) -> float:
        """è®¡ç®—LoRAå‚æ•°å†…å­˜"""
        lora_memory = 0
        for name, param in model.named_parameters():
            if 'lora' in name.lower():
                lora_memory += param.numel() * param.element_size() / (1024**2)
        return lora_memory
    
    def _calculate_gradient_memory(self, model) -> float:
        """è®¡ç®—æ¢¯åº¦å†…å­˜"""
        gradient_memory = 0
        for name, param in model.named_parameters():
            if param.requires_grad and param.grad is not None:
                gradient_memory += param.grad.numel() * param.grad.element_size() / (1024**2)
        return gradient_memory
    
    def _calculate_optimizer_memory(self, optimizer) -> float:
        """è®¡ç®—ä¼˜åŒ–å™¨çŠ¶æ€å†…å­˜"""
        optimizer_memory = 0
        try:
            for param_group in optimizer.param_groups:
                for param in param_group['params']:
                    if param in optimizer.state:
                        state = optimizer.state[param]
                        for state_key, state_value in state.items():
                            if hasattr(state_value, 'numel') and hasattr(state_value, 'element_size'):
                                optimizer_memory += state_value.numel() * state_value.element_size() / (1024**2)
        except Exception as e:
            self.logger.warning(f"è®¡ç®—ä¼˜åŒ–å™¨å†…å­˜æ—¶å‡ºé”™: {e}")
        return optimizer_memory
    
    def _estimate_activation_memory_simple(self, model) -> float:
        """
        ç®€å•ä¼°ç®—æ¿€æ´»å€¼å†…å­˜ï¼ˆä»…ä¼°ç®—å‰å‘ä¼ æ’­ä¸­çš„ä¸­é—´æ¿€æ´»å€¼ï¼‰
        ä¸åŒ…æ‹¬å‚æ•°æœ¬èº«çš„å†…å­˜ï¼Œé¿å…é‡å¤è®¡ç®—
        """
        try:
            # åŸºäºæ¨¡å‹æ¶æ„ä¼°ç®—æ¿€æ´»å€¼å†…å­˜
            # è¿™é‡Œä½¿ç”¨æ›´åˆç†çš„ä¼°ç®—æ–¹æ³•ï¼Œä¸åŸºäºå‚æ•°æ€»é‡
            
            # ä¼°ç®—éšè—å±‚å¤§å°ï¼ˆå‡è®¾å…¸å‹çš„transformeræ¶æ„ï¼‰
            hidden_size = 4096  # é»˜è®¤éšè—å±‚å¤§å°
            seq_length = 512    # é»˜è®¤åºåˆ—é•¿åº¦
            batch_size = 1      # é»˜è®¤æ‰¹æ¬¡å¤§å°
            
            # å°è¯•ä»æ¨¡å‹é…ç½®ä¸­è·å–å®é™…å€¼
            try:
                if hasattr(model, 'config'):
                    config = model.config
                    hidden_size = getattr(config, 'hidden_size', hidden_size)
                    if hasattr(config, 'max_position_embeddings'):
                        seq_length = min(seq_length, config.max_position_embeddings)
                elif hasattr(model, 'model') and hasattr(model.model, 'config'):
                    config = model.model.config
                    hidden_size = getattr(config, 'hidden_size', hidden_size)
                    if hasattr(config, 'max_position_embeddings'):
                        seq_length = min(seq_length, config.max_position_embeddings)
            except:
                pass
            
            # ä¼°ç®—æ¿€æ´»å€¼å†…å­˜ï¼ˆå•ä½ï¼šMBï¼‰
            # åŒ…æ‹¬ï¼šattentionæ¿€æ´»å€¼ã€MLPæ¿€æ´»å€¼ã€å±‚å½’ä¸€åŒ–æ¿€æ´»å€¼ç­‰
            # ä½¿ç”¨float16ç²¾åº¦è®¡ç®—ï¼ˆ2å­—èŠ‚æ¯ä¸ªæ•°å€¼ï¼‰
            bytes_per_element = 2
            
            # åŸºæœ¬æ¿€æ´»å€¼ä¼°ç®—
            attention_activations = batch_size * seq_length * hidden_size * bytes_per_element
            mlp_activations = batch_size * seq_length * hidden_size * 4 * bytes_per_element  # MLPé€šå¸¸4å€æ‰©å±•
            
            # ä¼°ç®—æ€»æ¿€æ´»å€¼å†…å­˜
            total_activation_bytes = attention_activations + mlp_activations
            activation_memory_mb = total_activation_bytes / (1024**2)
            
            self.logger.debug(f"ä¼°ç®—æ¿€æ´»å€¼å†…å­˜: {activation_memory_mb:.2f} MB (hidden_size={hidden_size}, seq_length={seq_length})")
            
            return activation_memory_mb
            
        except Exception as e:
            self.logger.warning(f"ä¼°ç®—æ¿€æ´»å€¼å†…å­˜æ—¶å‡ºé”™: {e}")
            return 0

    def _estimate_activation_memory_detailed(self, model) -> Dict:
        """
        è¯¦ç»†ä¼°ç®—æ¿€æ´»å€¼å†…å­˜å¹¶æä¾›breakdown
        """
        try:
            # åŸºç¡€é…ç½®å‚æ•°
            hidden_size = 4096
            seq_length = 512
            batch_size = 1
            num_layers = 32
            
            # å°è¯•ä»æ¨¡å‹é…ç½®ä¸­è·å–å®é™…å€¼
            try:
                if hasattr(model, 'config'):
                    config = model.config
                    hidden_size = getattr(config, 'hidden_size', hidden_size)
                    num_layers = getattr(config, 'num_hidden_layers', num_layers)
                    if hasattr(config, 'max_position_embeddings'):
                        seq_length = min(seq_length, config.max_position_embeddings)
                elif hasattr(model, 'model') and hasattr(model.model, 'config'):
                    config = model.model.config
                    hidden_size = getattr(config, 'hidden_size', hidden_size)
                    num_layers = getattr(config, 'num_hidden_layers', num_layers)
                    if hasattr(config, 'max_position_embeddings'):
                        seq_length = min(seq_length, config.max_position_embeddings)
            except:
                pass
            
            bytes_per_element = 2  # float16
            
            # è¯¦ç»†çš„æ¿€æ´»å€¼åˆ†ç±»
            activation_breakdown = {
                "embedding_activations": {
                    "memory_mb": 0,
                    "description": "è¯åµŒå…¥å±‚æ¿€æ´»å€¼"
                },
                "attention_activations": {
                    "memory_mb": 0,
                    "description": "æ³¨æ„åŠ›å±‚æ¿€æ´»å€¼ï¼ˆQã€Kã€VçŸ©é˜µè¿ç®—ç»“æœï¼‰"
                },
                "mlp_activations": {
                    "memory_mb": 0,
                    "description": "MLPå±‚æ¿€æ´»å€¼ï¼ˆå‰é¦ˆç½‘ç»œä¸­é—´ç»“æœï¼‰"
                },
                "layer_norm_activations": {
                    "memory_mb": 0,
                    "description": "å±‚å½’ä¸€åŒ–æ¿€æ´»å€¼"
                },
                "output_activations": {
                    "memory_mb": 0,
                    "description": "è¾“å‡ºå±‚æ¿€æ´»å€¼"
                }
            }
            
            # 1. è¯åµŒå…¥æ¿€æ´»å€¼
            embedding_memory = batch_size * seq_length * hidden_size * bytes_per_element
            activation_breakdown["embedding_activations"]["memory_mb"] = embedding_memory / (1024**2)
            
            # 2. æ³¨æ„åŠ›æ¿€æ´»å€¼ï¼ˆæ¯å±‚ï¼‰
            # Qã€Kã€VçŸ©é˜µ + æ³¨æ„åŠ›åˆ†æ•° + è¾“å‡º
            attention_per_layer = batch_size * seq_length * hidden_size * 4 * bytes_per_element
            attention_total = attention_per_layer * num_layers
            activation_breakdown["attention_activations"]["memory_mb"] = attention_total / (1024**2)
            
            # 3. MLPæ¿€æ´»å€¼ï¼ˆæ¯å±‚ï¼‰
            # å‡è®¾MLPæœ‰4å€éšè—å±‚å¤§å°çš„ä¸­é—´å±‚
            mlp_per_layer = batch_size * seq_length * hidden_size * 4 * bytes_per_element
            mlp_total = mlp_per_layer * num_layers
            activation_breakdown["mlp_activations"]["memory_mb"] = mlp_total / (1024**2)
            
            # 4. å±‚å½’ä¸€åŒ–æ¿€æ´»å€¼
            layer_norm_per_layer = batch_size * seq_length * hidden_size * bytes_per_element
            layer_norm_total = layer_norm_per_layer * num_layers * 2  # æ¯å±‚é€šå¸¸æœ‰2ä¸ªLayerNorm
            activation_breakdown["layer_norm_activations"]["memory_mb"] = layer_norm_total / (1024**2)
            
            # 5. è¾“å‡ºå±‚æ¿€æ´»å€¼
            output_memory = batch_size * seq_length * hidden_size * bytes_per_element
            activation_breakdown["output_activations"]["memory_mb"] = output_memory / (1024**2)
            
            # è®¡ç®—æ€»å†…å­˜
            total_activation_memory = sum(
                component["memory_mb"] for component in activation_breakdown.values()
            )
            
            # è®¡ç®—ç™¾åˆ†æ¯”
            if total_activation_memory > 0:
                for component in activation_breakdown.values():
                    component["percentage"] = (component["memory_mb"] / total_activation_memory) * 100
            
            return {
                "total_memory_mb": total_activation_memory,
                "estimation_method": "detailed_layer_based",
                "model_config": {
                    "hidden_size": hidden_size,
                    "num_layers": num_layers,
                    "seq_length": seq_length,
                    "batch_size": batch_size
                },
                "breakdown": activation_breakdown,
                "notes": "åŸºäºæ¨¡å‹æ¶æ„çš„è¯¦ç»†æ¿€æ´»å€¼å†…å­˜ä¼°ç®—ï¼Œä¸åŒ…æ‹¬å‚æ•°æœ¬èº«"
            }
            
        except Exception as e:
            self.logger.warning(f"è¯¦ç»†ä¼°ç®—æ¿€æ´»å€¼å†…å­˜æ—¶å‡ºé”™: {e}")
            return {
                "total_memory_mb": 0,
                "estimation_method": "failed",
                "error": str(e),
                "breakdown": {}
            }
    
    def _save_brief_report(self, brief_report: Dict, stage: str):
        """ä¿å­˜ç®€æ´æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"brief_memory_report_{stage}_{timestamp}.json"
        filepath = os.path.join(self.log_dir, filename)
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(brief_report, f, ensure_ascii=False, indent=2)
            self.logger.info(f"ğŸ’¾ ç®€æ´æŠ¥å‘Šå·²ä¿å­˜: {filepath}")
        except Exception as e:
            self.logger.error(f"ä¿å­˜ç®€æ´æŠ¥å‘Šå¤±è´¥: {e}")
    
    def _log_brief_report(self, brief_report: Dict):
        """å°†ç®€æ´æŠ¥å‘Šè®°å½•åˆ°æ—¥å¿—"""
        self.logger.info("=" * 60)
        self.logger.info(f"ğŸ“Š ç®€æ´å†…å­˜æŠ¥å‘Š - {brief_report['stage']}")
        self.logger.info("=" * 60)
        
        breakdown = brief_report["memory_breakdown"]
        total_memory = brief_report["total_memory_mb"]
        gpu_utilization = brief_report["gpu_utilization_percentage"]
        
        self.logger.info(f"ğŸ¯ æ€»å†…å­˜å ç”¨: {total_memory:.2f} MB")
        self.logger.info(f"ğŸ–¥ï¸  GPUåˆ©ç”¨ç‡: {gpu_utilization:.2f}%")
        self.logger.info("")
        self.logger.info("ğŸ“‹ å„ç»„ä»¶å†…å­˜å ç”¨:")
        
        # æŒ‰å†…å­˜å ç”¨æ’åº
        sorted_components = sorted(breakdown.items(), key=lambda x: x[1]["memory_mb"], reverse=True)
        
        for i, (component, info) in enumerate(sorted_components):
            memory_mb = info["memory_mb"]
            percentage = info["percentage"]
            
            # ä¸­æ–‡ç»„ä»¶åæ˜ å°„
            component_names = {
                "base_model": "åŸºç¡€æ¨¡å‹",
                "lora_parameters": "LoRAå‚æ•°",
                "gradients": "æ¢¯åº¦",
                "optimizer_states": "ä¼˜åŒ–å™¨çŠ¶æ€",
                "activations": "æ¿€æ´»å€¼"
            }
            
            display_name = component_names.get(component, component)
            self.logger.info(f"  {i+1}. {display_name}: {memory_mb:.2f} MB ({percentage:.1f}%)")
            
            # å¦‚æœæ˜¯æ¿€æ´»å€¼ï¼Œæ˜¾ç¤ºè¯¦ç»†breakdown
            if component == "activations" and "details" in info:
                details = info["details"]
                if "breakdown" in details and details["breakdown"]:
                    self.logger.info(f"     ğŸ“Š æ¿€æ´»å€¼è¯¦ç»†åˆ†è§£:")
                    for act_type, act_info in details["breakdown"].items():
                        act_memory = act_info["memory_mb"]
                        act_percentage = act_info.get("percentage", 0)
                        act_desc = act_info.get("description", "")
                        
                        # ä¸­æ–‡æ¿€æ´»å€¼ç±»å‹æ˜ å°„
                        act_type_names = {
                            "embedding_activations": "è¯åµŒå…¥æ¿€æ´»",
                            "attention_activations": "æ³¨æ„åŠ›æ¿€æ´»",
                            "mlp_activations": "MLPæ¿€æ´»",
                            "layer_norm_activations": "å±‚å½’ä¸€åŒ–æ¿€æ´»",
                            "output_activations": "è¾“å‡ºå±‚æ¿€æ´»"
                        }
                        
                        act_display_name = act_type_names.get(act_type, act_type)
                        self.logger.info(f"       - {act_display_name}: {act_memory:.2f} MB ({act_percentage:.1f}%)")
                        if act_desc:
                            self.logger.info(f"         {act_desc}")
                    
                    # æ˜¾ç¤ºæ¨¡å‹é…ç½®ä¿¡æ¯
                    if "model_config" in details:
                        config = details["model_config"]
                        self.logger.info(f"     ğŸ”§ æ¨¡å‹é…ç½®: hidden_size={config.get('hidden_size', 'N/A')}, "
                                       f"num_layers={config.get('num_layers', 'N/A')}, "
                                       f"seq_length={config.get('seq_length', 'N/A')}")
        
        self.logger.info("=" * 60)
    
    def __enter__(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        if self.enable_real_time:
            self.start_real_time_monitoring()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡º"""
        if self.monitoring_active:
            self.stop_real_time_monitoring()
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        try:
            self.generate_memory_report()
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šå¤±è´¥: {e}")

# ä¾¿æ·å‡½æ•°
def create_memory_debugger(log_dir: str = "logs/memory_debug", enable_real_time: bool = True) -> MemoryDebugger:
    """åˆ›å»ºå†…å­˜è°ƒè¯•å™¨å®ä¾‹"""
    return MemoryDebugger(log_dir=log_dir, enable_real_time=enable_real_time)

def debug_model_memory(model, model_name: str = "model", log_dir: str = "logs/memory_debug") -> Dict:
    """
    å¿«é€Ÿè°ƒè¯•æ¨¡å‹å†…å­˜ä½¿ç”¨æƒ…å†µ
    
    Args:
        model: æ¨¡å‹å¯¹è±¡
        model_name: æ¨¡å‹åç§°
        log_dir: æ—¥å¿—ç›®å½•
        
    Returns:
        åˆ†æç»“æœ
    """
    debugger = MemoryDebugger(log_dir=log_dir, enable_real_time=False)
    return debugger.analyze_model_parameters(model, model_name)

def monitor_training_memory(training_function, *args, **kwargs):
    """
    ç›‘æ§è®­ç»ƒè¿‡ç¨‹ä¸­çš„å†…å­˜ä½¿ç”¨
    
    Args:
        training_function: è®­ç»ƒå‡½æ•°
        *args: è®­ç»ƒå‡½æ•°å‚æ•°
        **kwargs: è®­ç»ƒå‡½æ•°å…³é”®å­—å‚æ•°
        
    Returns:
        è®­ç»ƒå‡½æ•°çš„è¿”å›å€¼
    """
    with create_memory_debugger() as debugger:
        debugger.create_memory_checkpoint("training_start")
        
        try:
            result = training_function(*args, **kwargs)
            debugger.create_memory_checkpoint("training_end")
            return result
        except Exception as e:
            debugger.create_memory_checkpoint("training_error")
            raise e

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # ç¤ºä¾‹ï¼šä½¿ç”¨å†…å­˜è°ƒè¯•å™¨
    with create_memory_debugger() as debugger:
        # åˆ›å»ºæ£€æŸ¥ç‚¹
        debugger.create_memory_checkpoint("start")
        
        # æ¨¡æ‹Ÿä¸€äº›å†…å­˜æ“ä½œ
        import time
        dummy_tensor = torch.randn(1000, 1000).cuda() if torch.cuda.is_available() else torch.randn(1000, 1000)
        time.sleep(2)
        
        debugger.create_memory_checkpoint("after_tensor_creation")
        
        # æ¸…ç†å¼ é‡
        del dummy_tensor
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        debugger.create_memory_checkpoint("after_cleanup")
        
        # æ¯”è¾ƒæ£€æŸ¥ç‚¹
        comparison = debugger.compare_checkpoints("start", "after_tensor_creation")
        print("æ£€æŸ¥ç‚¹æ¯”è¾ƒç»“æœ:", comparison) 