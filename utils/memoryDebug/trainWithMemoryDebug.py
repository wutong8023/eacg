"""
é›†æˆå†…å­˜è°ƒè¯•å™¨çš„è®­ç»ƒå·¥å…·
æä¾›ä¾¿æ·çš„æ¥å£åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç›‘æ§å’Œè°ƒè¯•å†…å­˜ä½¿ç”¨æƒ…å†µ
"""

import os
import sys
import torch
import logging
from typing import Dict, List, Optional, Any
from contextlib import contextmanager
from datetime import datetime
import json

# æ·»åŠ å½“å‰è·¯å¾„åˆ°sys.path
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

from memoryDebugger import MemoryDebugger, create_memory_debugger
from utils.loraTrain.loraTrainUtils import buildandTrainLoraModel, getEquipAdaptorModel

class TrainingMemoryProfiler:
    """è®­ç»ƒå†…å­˜æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self, log_dir: str = "logs/training_memory_debug", enable_real_time: bool = True):
        self.log_dir = log_dir
        self.enable_real_time = enable_real_time
        self.debugger = create_memory_debugger(log_dir=log_dir, enable_real_time=enable_real_time)
        self.training_stages = []
        self.json_reports = {}  # å­˜å‚¨å„é˜¶æ®µçš„JSONæŠ¥å‘Š
        
    def profile_model_creation(self, config: Dict, model_name: str = "lora_model"):
        """
        ç›‘æ§æ¨¡å‹åˆ›å»ºè¿‡ç¨‹çš„å†…å­˜ä½¿ç”¨
        
        Args:
            config: æ¨¡å‹é…ç½®
            model_name: æ¨¡å‹åç§°
            
        Returns:
            åˆ›å»ºçš„æ¨¡å‹å’Œåˆ†æç»“æœ
        """
        print(f"ğŸ” å¼€å§‹ç›‘æ§æ¨¡å‹åˆ›å»ºè¿‡ç¨‹: {model_name}")
        
        # åˆ›å»ºèµ·å§‹æ£€æŸ¥ç‚¹
        self.debugger.create_memory_checkpoint(f"{model_name}_creation_start")
        
        try:
            # åˆ›å»ºæ¨¡å‹
            model = getEquipAdaptorModel(config)
            
            # åˆ›å»ºæ¨¡å‹åˆ›å»ºå®Œæˆæ£€æŸ¥ç‚¹
            self.debugger.create_memory_checkpoint(f"{model_name}_creation_complete", model=model)
            
            # ğŸ†• ç”Ÿæˆè¯¦ç»†çš„JSONæŠ¥å‘Š - æ¨¡å‹åˆ›å»ºé˜¶æ®µ
            print("ğŸ“Š ç”Ÿæˆæ¨¡å‹åˆ›å»ºé˜¶æ®µçš„è¯¦ç»†JSONæŠ¥å‘Š...")
            json_report = self.debugger.generate_detailed_memory_json_report(
                model=model,
                model_name=model_name,
                optimizer=None,
                stage="model_creation"
            )
            self.json_reports["model_creation"] = json_report
            
            # ğŸ†• ç”Ÿæˆç®€æ´æŠ¥å‘Š - æ¨¡å‹åˆ›å»ºé˜¶æ®µ
            print("ğŸ“„ ç”Ÿæˆæ¨¡å‹åˆ›å»ºé˜¶æ®µçš„ç®€æ´æŠ¥å‘Š...")
            brief_report = self.debugger.generate_brief_memory_report(
                model=model,
                model_name=model_name,
                optimizer=None,
                stage="model_creation"
            )
            self.json_reports["model_creation_brief"] = brief_report
            
            # åˆ†ææ¨¡å‹åˆ›å»ºè¿‡ç¨‹çš„å†…å­˜å˜åŒ–
            comparison = self.debugger.compare_checkpoints(
                f"{model_name}_creation_start",
                f"{model_name}_creation_complete"
            )
            
            # è®°å½•åˆ°è®­ç»ƒé˜¶æ®µ
            self.training_stages.append({
                'stage': 'model_creation',
                'timestamp': datetime.now().isoformat(),
                'memory_comparison': comparison,
                'json_report': json_report,
                'brief_report': brief_report
            })
            
            return model, comparison
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åˆ›å»ºè¿‡ç¨‹å‡ºé”™: {e}")
            raise
    
    def profile_training_process(self, config: Dict, dataloader, precision: str = 'fp16', 
                               pkg: str = None, version: str = None, knowledge_type: str = None):
        """
        ç›‘æ§è®­ç»ƒè¿‡ç¨‹çš„å†…å­˜ä½¿ç”¨
        
        Args:
            config: è®­ç»ƒé…ç½®
            dataloader: æ•°æ®åŠ è½½å™¨
            precision: ç²¾åº¦è®¾ç½®
            pkg: åŒ…å
            version: ç‰ˆæœ¬
            knowledge_type: çŸ¥è¯†ç±»å‹
            
        Returns:
            è®­ç»ƒå¥½çš„æ¨¡å‹å’Œåˆ†æç»“æœ
        """
        print(f"ğŸš€ å¼€å§‹ç›‘æ§è®­ç»ƒè¿‡ç¨‹: {pkg}-{version}")
        
        # è·å–æ¨¡å‹ï¼ˆä»ä¹‹å‰çš„checkpointæˆ–é‡æ–°åˆ›å»ºï¼‰
        model_name = f"{pkg}_{version}" if pkg and version else "lora_model"
        
        # åˆ›å»ºè®­ç»ƒå¼€å§‹æ£€æŸ¥ç‚¹
        self.debugger.create_memory_checkpoint(f"{model_name}_training_start")
        
        try:
            # æ‰§è¡Œè®­ç»ƒ
            from utils.loraTrain.loraTrainUtils import buildandTrainLoraModel
            
            # ğŸ†• åœ¨è®­ç»ƒå¼€å§‹å‰ç”ŸæˆæŠ¥å‘Š
            if hasattr(self, '_current_model'):
                print("ğŸ“Š ç”Ÿæˆè®­ç»ƒå¼€å§‹é˜¶æ®µçš„è¯¦ç»†JSONæŠ¥å‘Š...")
                json_report_start = self.debugger.generate_detailed_memory_json_report(
                    model=self._current_model,
                    model_name=model_name,
                    optimizer=None,
                    stage="training_start"
                )
                self.json_reports["training_start"] = json_report_start
                
                # ğŸ†• ç”Ÿæˆç®€æ´æŠ¥å‘Š - è®­ç»ƒå¼€å§‹é˜¶æ®µ
                print("ğŸ“„ ç”Ÿæˆè®­ç»ƒå¼€å§‹é˜¶æ®µçš„ç®€æ´æŠ¥å‘Š...")
                brief_report_start = self.debugger.generate_brief_memory_report(
                    model=self._current_model,
                    model_name=model_name,
                    optimizer=None,
                    stage="training_start"
                )
                self.json_reports["training_start_brief"] = brief_report_start
            
            # æ‰§è¡Œå®é™…çš„è®­ç»ƒ
            trained_model = buildandTrainLoraModel(config, dataloader, precision, pkg, version, knowledge_type=knowledge_type)
            
            # åˆ›å»ºè®­ç»ƒå®Œæˆæ£€æŸ¥ç‚¹
            self.debugger.create_memory_checkpoint(f"{model_name}_training_complete", model=trained_model)
            
            # ğŸ†• ç”Ÿæˆè¯¦ç»†çš„JSONæŠ¥å‘Š - è®­ç»ƒå®Œæˆé˜¶æ®µ
            print("ğŸ“Š ç”Ÿæˆè®­ç»ƒå®Œæˆé˜¶æ®µçš„è¯¦ç»†JSONæŠ¥å‘Š...")
            json_report_complete = self.debugger.generate_detailed_memory_json_report(
                model=trained_model,
                model_name=model_name,
                optimizer=None,  # æ³¨æ„ï¼šè¿™é‡Œå¯èƒ½éœ€è¦ä¼ å…¥å®é™…çš„optimizer
                stage="training_complete"
            )
            self.json_reports["training_complete"] = json_report_complete
            
            # ğŸ†• ç”Ÿæˆç®€æ´æŠ¥å‘Š - è®­ç»ƒå®Œæˆé˜¶æ®µ
            print("ğŸ“„ ç”Ÿæˆè®­ç»ƒå®Œæˆé˜¶æ®µçš„ç®€æ´æŠ¥å‘Š...")
            brief_report_complete = self.debugger.generate_brief_memory_report(
                model=trained_model,
                model_name=model_name,
                optimizer=None,
                stage="training_complete"
            )
            self.json_reports["training_complete_brief"] = brief_report_complete
            
            # æ¯”è¾ƒè®­ç»ƒå‰åçš„å†…å­˜å˜åŒ–
            comparison = self.debugger.compare_checkpoints(
                f"{model_name}_training_start",
                f"{model_name}_training_complete"
            )
            
            # è®°å½•åˆ°è®­ç»ƒé˜¶æ®µ
            stage_info = {
                'stage': 'training_process',
                'timestamp': datetime.now().isoformat(),
                'memory_comparison': comparison,
                'json_report_start': json_report_start if 'json_report_start' in locals() else None,
                'json_report_complete': json_report_complete,
                'brief_report_start': brief_report_start if 'brief_report_start' in locals() else None,
                'brief_report_complete': brief_report_complete
            }
            
            self.training_stages.append(stage_info)
            
            return trained_model, comparison
            
        except Exception as e:
            print(f"âŒ è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {e}")
            raise
    
    def profile_stage_with_json_report(self, stage_name: str, stage_function, model=None, optimizer=None, *args, **kwargs):
        """
        ç›‘æ§ç‰¹å®šé˜¶æ®µå¹¶ç”ŸæˆJSONæŠ¥å‘Š
        
        Args:
            stage_name: é˜¶æ®µåç§°
            stage_function: é˜¶æ®µå‡½æ•°
            model: æ¨¡å‹å¯¹è±¡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            optimizer: ä¼˜åŒ–å™¨å¯¹è±¡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            *args, **kwargs: ä¼ é€’ç»™é˜¶æ®µå‡½æ•°çš„å‚æ•°
            
        Returns:
            é˜¶æ®µå‡½æ•°çš„ç»“æœå’Œåˆ†æç»“æœ
        """
        print(f"ğŸ” å¼€å§‹ç›‘æ§é˜¶æ®µ: {stage_name}")
        
        # åˆ›å»ºé˜¶æ®µå¼€å§‹æ£€æŸ¥ç‚¹
        self.debugger.create_memory_checkpoint(f"{stage_name}_start")
        
        try:
            # ğŸ†• åœ¨é˜¶æ®µå¼€å§‹å‰ç”ŸæˆJSONæŠ¥å‘Šï¼ˆå¦‚æœæœ‰æ¨¡å‹ï¼‰
            if model is not None:
                print(f"ğŸ“Š ç”Ÿæˆ{stage_name}é˜¶æ®µå¼€å§‹çš„è¯¦ç»†JSONæŠ¥å‘Š...")
                json_report_start = self.debugger.generate_detailed_memory_json_report(
                    model=model,
                    model_name=f"{stage_name}_model",
                    optimizer=optimizer,
                    stage=f"{stage_name}_start"
                )
                self.json_reports[f"{stage_name}_start"] = json_report_start
            
            # æ‰§è¡Œé˜¶æ®µå‡½æ•°
            result = stage_function(*args, **kwargs)
            
            # åˆ›å»ºé˜¶æ®µå®Œæˆæ£€æŸ¥ç‚¹
            self.debugger.create_memory_checkpoint(f"{stage_name}_complete", model=model)
            
            # ğŸ†• åœ¨é˜¶æ®µå®Œæˆåç”ŸæˆJSONæŠ¥å‘Šï¼ˆå¦‚æœæœ‰æ¨¡å‹ï¼‰
            if model is not None:
                print(f"ğŸ“Š ç”Ÿæˆ{stage_name}é˜¶æ®µå®Œæˆçš„è¯¦ç»†JSONæŠ¥å‘Š...")
                json_report_complete = self.debugger.generate_detailed_memory_json_report(
                    model=model,
                    model_name=f"{stage_name}_model",
                    optimizer=optimizer,
                    stage=f"{stage_name}_complete"
                )
                self.json_reports[f"{stage_name}_complete"] = json_report_complete
            
            # æ¯”è¾ƒé˜¶æ®µå‰åçš„å†…å­˜å˜åŒ–
            comparison = self.debugger.compare_checkpoints(
                f"{stage_name}_start",
                f"{stage_name}_complete"
            )
            
            # è®°å½•åˆ°è®­ç»ƒé˜¶æ®µ
            stage_info = {
                'stage': stage_name,
                'timestamp': datetime.now().isoformat(),
                'memory_comparison': comparison
            }
            
            if model is not None:
                stage_info['json_report_start'] = json_report_start if 'json_report_start' in locals() else None
                stage_info['json_report_complete'] = json_report_complete if 'json_report_complete' in locals() else None
            
            self.training_stages.append(stage_info)
            
            return result, comparison
            
        except Exception as e:
            print(f"âŒ é˜¶æ®µ {stage_name} å‡ºé”™: {e}")
            raise

    def profile_stage(self, stage_name: str, stage_function, *args, **kwargs):
        """
        ç›‘æ§ç‰¹å®šé˜¶æ®µçš„å†…å­˜ä½¿ç”¨
        
        Args:
            stage_name: é˜¶æ®µåç§°
            stage_function: é˜¶æ®µå‡½æ•°
            *args, **kwargs: ä¼ é€’ç»™é˜¶æ®µå‡½æ•°çš„å‚æ•°
            
        Returns:
            é˜¶æ®µå‡½æ•°çš„ç»“æœå’Œåˆ†æç»“æœ
        """
        print(f"ğŸ” å¼€å§‹ç›‘æ§é˜¶æ®µ: {stage_name}")
        
        # åˆ›å»ºé˜¶æ®µå¼€å§‹æ£€æŸ¥ç‚¹
        self.debugger.create_memory_checkpoint(f"{stage_name}_start")
        
        try:
            # æ‰§è¡Œé˜¶æ®µå‡½æ•°
            result = stage_function(*args, **kwargs)
            
            # åˆ›å»ºé˜¶æ®µå®Œæˆæ£€æŸ¥ç‚¹
            self.debugger.create_memory_checkpoint(f"{stage_name}_complete")
            
            # æ¯”è¾ƒé˜¶æ®µå‰åçš„å†…å­˜å˜åŒ–
            comparison = self.debugger.compare_checkpoints(
                f"{stage_name}_start",
                f"{stage_name}_complete"
            )
            
            # è®°å½•åˆ°è®­ç»ƒé˜¶æ®µ
            self.training_stages.append({
                'stage': stage_name,
                'timestamp': datetime.now().isoformat(),
                'memory_comparison': comparison
            })
            
            return result, comparison
            
        except Exception as e:
            print(f"âŒ é˜¶æ®µ {stage_name} å‡ºé”™: {e}")
            raise
    
    def get_json_reports(self) -> Dict:
        """è·å–æ‰€æœ‰JSONæŠ¥å‘Š"""
        return self.json_reports
    
    def save_all_json_reports(self):
        """ä¿å­˜æ‰€æœ‰JSONæŠ¥å‘Šåˆ°æ–‡ä»¶"""
        print("ğŸ’¾ ä¿å­˜æ‰€æœ‰JSONæŠ¥å‘Š...")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        all_reports_file = os.path.join(self.log_dir, f"all_memory_reports_{timestamp}.json")
        
        try:
            with open(all_reports_file, 'w', encoding='utf-8') as f:
                json.dump(self.json_reports, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ æ‰€æœ‰JSONæŠ¥å‘Šå·²ä¿å­˜: {all_reports_file}")
            
            # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
            summary_report = self._generate_summary_report()
            summary_file = os.path.join(self.log_dir, f"memory_summary_all_stages_{timestamp}.json")
            
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary_report, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {summary_file}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜JSONæŠ¥å‘Šå¤±è´¥: {e}")
    
    def _generate_summary_report(self) -> Dict:
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        summary = {
            "timestamp": datetime.now().isoformat(),
            "stages_analyzed": list(self.json_reports.keys()),
            "memory_progression": [],
            "peak_memory_usage": {"stage": None, "memory_mb": 0},
            "memory_efficiency": {}
        }
        
        for stage, report in self.json_reports.items():
            if 'memory_summary' in report:
                memory_info = {
                    "stage": stage,
                    "total_memory_mb": report['memory_summary']['total_memory_mb'],
                    "component_breakdown": report['memory_summary']['component_breakdown'],
                    "occupancy_percentage": report.get('occupancy_percentages', {}).get('total_used_percentage', 0)
                }
                summary["memory_progression"].append(memory_info)
                
                # æ‰¾å‡ºå³°å€¼å†…å­˜ä½¿ç”¨
                if memory_info["total_memory_mb"] > summary["peak_memory_usage"]["memory_mb"]:
                    summary["peak_memory_usage"] = {
                        "stage": stage,
                        "memory_mb": memory_info["total_memory_mb"]
                    }
        
        return summary
    
    def compare_training_stages(self):
        """æ¯”è¾ƒè®­ç»ƒå„é˜¶æ®µçš„å†…å­˜ä½¿ç”¨"""
        print("ğŸ“Š æ¯”è¾ƒè®­ç»ƒå„é˜¶æ®µçš„å†…å­˜ä½¿ç”¨...")
        
        # æ‰¾åˆ°æ‰€æœ‰ç›¸å…³çš„æ£€æŸ¥ç‚¹
        checkpoints = [cp['name'] for cp in self.debugger.checkpoint_history]
        
        comparisons = []
        
        # æ¯”è¾ƒè®­ç»ƒå¼€å§‹å’Œå®Œæˆ
        if "training_start" in checkpoints and "training_complete" in checkpoints:
            comparison = self.debugger.compare_checkpoints("training_start", "training_complete")
            comparisons.append(("è®­ç»ƒå¼€å§‹ vs è®­ç»ƒå®Œæˆ", comparison))
        
        # æ¯”è¾ƒæ¨¡å‹åˆ›å»ºå‰å
        creation_start = [cp for cp in checkpoints if cp.endswith("_creation_start")]
        creation_complete = [cp for cp in checkpoints if cp.endswith("_creation_complete")]
        
        for start_cp, complete_cp in zip(creation_start, creation_complete):
            comparison = self.debugger.compare_checkpoints(start_cp, complete_cp)
            comparisons.append((f"æ¨¡å‹åˆ›å»º: {start_cp} vs {complete_cp}", comparison))
        
        return comparisons
    
    def detect_memory_issues(self) -> Dict:
        """æ£€æµ‹å†…å­˜ä½¿ç”¨é—®é¢˜"""
        print("ğŸ” æ£€æµ‹å†…å­˜ä½¿ç”¨é—®é¢˜...")
        
        issues = {
            'memory_leaks': [],
            'excessive_usage': [],
            'abnormal_growth': [],
            'fragmentation': []
        }
        
        if len(self.debugger.checkpoint_history) < 2:
            print("âš ï¸  æ£€æŸ¥ç‚¹æ•°é‡ä¸è¶³ï¼Œæ— æ³•æ£€æµ‹å†…å­˜é—®é¢˜")
            return issues
        
        # æ£€æŸ¥å†…å­˜æ³„æ¼
        for i in range(len(self.debugger.checkpoint_history) - 1):
            current = self.debugger.checkpoint_history[i]
            next_cp = self.debugger.checkpoint_history[i + 1]
            
            for gpu_idx in range(len(current['memory_usage']['gpu_memory'])):
                current_allocated = current['memory_usage']['gpu_memory'][gpu_idx]['allocated_mb']
                next_allocated = next_cp['memory_usage']['gpu_memory'][gpu_idx]['allocated_mb']
                
                growth = next_allocated - current_allocated
                
                # æ£€æŸ¥å¼‚å¸¸å¢é•¿
                if growth > 500:  # 500MBå¢é•¿è®¤ä¸ºå¼‚å¸¸
                    issues['abnormal_growth'].append({
                        'from_checkpoint': current['name'],
                        'to_checkpoint': next_cp['name'],
                        'gpu_id': gpu_idx,
                        'growth_mb': growth
                    })
        
        # æ£€æŸ¥è¿‡åº¦ä½¿ç”¨
        for checkpoint in self.debugger.checkpoint_history:
            for gpu_mem in checkpoint['memory_usage']['gpu_memory']:
                if gpu_mem['utilization_percent'] > 95:
                    issues['excessive_usage'].append({
                        'checkpoint': checkpoint['name'],
                        'gpu_id': gpu_mem['device_id'],
                        'utilization_percent': gpu_mem['utilization_percent']
                    })
        
        # æ£€æŸ¥å†…å­˜ç¢ç‰‡åŒ–
        for checkpoint in self.debugger.checkpoint_history:
            for gpu_mem in checkpoint['memory_usage']['gpu_memory']:
                reserved = gpu_mem['reserved_mb']
                allocated = gpu_mem['allocated_mb']
                
                if reserved > 0 and allocated > 0:
                    fragmentation_ratio = (reserved - allocated) / reserved
                    if fragmentation_ratio > 0.3:  # 30%ä»¥ä¸Šçš„ç¢ç‰‡åŒ–
                        issues['fragmentation'].append({
                            'checkpoint': checkpoint['name'],
                            'gpu_id': gpu_mem['device_id'],
                            'fragmentation_ratio': fragmentation_ratio,
                            'wasted_mb': reserved - allocated
                        })
        
        # è¾“å‡ºé—®é¢˜æ‘˜è¦
        print("\nğŸ“‹ å†…å­˜é—®é¢˜æ£€æµ‹ç»“æœ:")
        print(f"  å¼‚å¸¸å¢é•¿: {len(issues['abnormal_growth'])} ä¸ª")
        print(f"  è¿‡åº¦ä½¿ç”¨: {len(issues['excessive_usage'])} ä¸ª")
        print(f"  å†…å­˜ç¢ç‰‡åŒ–: {len(issues['fragmentation'])} ä¸ª")
        
        return issues
    
    def generate_training_report(self, output_file: str = None) -> str:
        """ç”Ÿæˆè®­ç»ƒå†…å­˜æŠ¥å‘Š"""
        print("ğŸ“„ ç”Ÿæˆè®­ç»ƒå†…å­˜æŠ¥å‘Š...")
        
        # ç”ŸæˆåŸºç¡€æŠ¥å‘Š
        base_report = self.debugger.generate_memory_report(output_file)
        
        # æ·»åŠ è®­ç»ƒç‰¹å®šä¿¡æ¯
        training_info = []
        training_info.append("\n" + "=" * 80)
        training_info.append("è®­ç»ƒç‰¹å®šä¿¡æ¯")
        training_info.append("=" * 80)
        
        # æ·»åŠ é˜¶æ®µæ¯”è¾ƒ
        comparisons = self.compare_training_stages()
        training_info.append("\nè®­ç»ƒé˜¶æ®µæ¯”è¾ƒ:")
        for name, comparison in comparisons:
            training_info.append(f"\n{name}:")
            for diff in comparison['gpu_memory_diff']:
                training_info.append(f"  GPU {diff['device_id']}: "
                                   f"å†…å­˜å˜åŒ– {diff['allocated_diff_mb']:+.2f} MB")
        
        # æ·»åŠ é—®é¢˜æ£€æµ‹ç»“æœ
        issues = self.detect_memory_issues()
        training_info.append("\nå†…å­˜é—®é¢˜æ£€æµ‹:")
        
        if issues['abnormal_growth']:
            training_info.append("  å¼‚å¸¸å¢é•¿:")
            for issue in issues['abnormal_growth']:
                training_info.append(f"    {issue['from_checkpoint']} -> {issue['to_checkpoint']}: "
                                   f"GPU {issue['gpu_id']} å¢é•¿ {issue['growth_mb']:.2f} MB")
        
        if issues['excessive_usage']:
            training_info.append("  è¿‡åº¦ä½¿ç”¨:")
            for issue in issues['excessive_usage']:
                training_info.append(f"    {issue['checkpoint']}: "
                                   f"GPU {issue['gpu_id']} ä½¿ç”¨ç‡ {issue['utilization_percent']:.1f}%")
        
        if issues['fragmentation']:
            training_info.append("  å†…å­˜ç¢ç‰‡åŒ–:")
            for issue in issues['fragmentation']:
                training_info.append(f"    {issue['checkpoint']}: "
                                   f"GPU {issue['gpu_id']} ç¢ç‰‡åŒ– {issue['fragmentation_ratio']:.1%}, "
                                   f"æµªè´¹ {issue['wasted_mb']:.2f} MB")
        
        # åˆå¹¶æŠ¥å‘Š
        full_report = base_report + "\n".join(training_info)
        
        # ä¿å­˜å®Œæ•´æŠ¥å‘Š
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(full_report)
            print(f"ğŸ“„ è®­ç»ƒå†…å­˜æŠ¥å‘Šå·²ä¿å­˜: {output_file}")
        
        return full_report
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.debugger.monitoring_active:
            self.debugger.stop_real_time_monitoring()
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        self.generate_training_report()

# ä¸Šä¸‹æ–‡ç®¡ç†å™¨
@contextmanager
def memory_profiled_training(log_dir: str = "logs/training_memory_debug", 
                           enable_real_time: bool = True):
    """
    ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç”¨äºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç›‘æ§å†…å­˜ä½¿ç”¨
    
    Args:
        log_dir: æ—¥å¿—ç›®å½•
        enable_real_time: æ˜¯å¦å¯ç”¨å®æ—¶ç›‘æ§
        
    Yields:
        TrainingMemoryProfiler å®ä¾‹
    """
    profiler = TrainingMemoryProfiler(log_dir=log_dir, enable_real_time=enable_real_time)
    
    try:
        yield profiler
    finally:
        # åœ¨é€€å‡ºæ—¶ä¿å­˜æ‰€æœ‰JSONæŠ¥å‘Š
        profiler.save_all_json_reports()
        
        # ç”Ÿæˆæœ€ç»ˆçš„è®­ç»ƒæŠ¥å‘Š
        final_report = profiler.generate_training_report()
        print("âœ… å†…å­˜åˆ†æå®Œæˆ")
        print(f"ğŸ“Š è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {log_dir}")


def monitor_memory(log_dir: str = "logs/training_memory_debug", 
                  enable_real_time: bool = True):
    """
    è£…é¥°å™¨ï¼Œç”¨äºç›‘æ§å‡½æ•°æ‰§è¡Œè¿‡ç¨‹ä¸­çš„å†…å­˜ä½¿ç”¨
    
    Args:
        log_dir: æ—¥å¿—ç›®å½•
        enable_real_time: æ˜¯å¦å¯ç”¨å®æ—¶ç›‘æ§
        
    Returns:
        è£…é¥°å™¨å‡½æ•°
    """
    def decorator(func):
        def wrapper(*args, **kwargs):
            with memory_profiled_training(log_dir=log_dir, enable_real_time=enable_real_time) as profiler:
                return func(profiler, *args, **kwargs)
        return wrapper
    return decorator


def debug_lora_training(config: Dict, dataloader, precision: str = 'fp16', 
                       pkg: str = None, version: str = None, knowledge_type: str = None,
                       log_dir: str = "logs/lora_training_debug"):
    """
    è°ƒè¯•LoRAè®­ç»ƒè¿‡ç¨‹çš„å†…å­˜ä½¿ç”¨ï¼Œç”Ÿæˆè¯¦ç»†çš„JSONæŠ¥å‘Š
    
    Args:
        config: è®­ç»ƒé…ç½®
        dataloader: æ•°æ®åŠ è½½å™¨
        precision: ç²¾åº¦è®¾ç½®
        pkg: åŒ…å
        version: ç‰ˆæœ¬
        knowledge_type: çŸ¥è¯†ç±»å‹
        log_dir: æ—¥å¿—ç›®å½•
        
    Returns:
        è®­ç»ƒå¥½çš„æ¨¡å‹å’Œåˆ†æç»“æœï¼ˆåŒ…æ‹¬è¯¦ç»†çš„JSONæŠ¥å‘Šï¼‰
    """
    print(f"ğŸ” å¼€å§‹è°ƒè¯•LoRAè®­ç»ƒ: {pkg}-{version}")
    
    with memory_profiled_training(log_dir=log_dir, enable_real_time=True) as profiler:
        # ğŸ†• ç›‘æ§æ¨¡å‹åˆ›å»ºè¿‡ç¨‹ï¼Œç”ŸæˆJSONæŠ¥å‘Š
        print("ğŸ“Š ç¬¬ä¸€é˜¶æ®µï¼šæ¨¡å‹åˆ›å»º")
        model, creation_analysis = profiler.profile_model_creation(config, f"{pkg}_{version}")
        
        # ä¿å­˜å½“å‰æ¨¡å‹å¼•ç”¨ä¾›åç»­ä½¿ç”¨
        profiler._current_model = model
        
        # ğŸ†• ç›‘æ§è®­ç»ƒè¿‡ç¨‹ï¼Œç”ŸæˆJSONæŠ¥å‘Š
        print("ğŸ“Š ç¬¬äºŒé˜¶æ®µï¼šæ¨¡å‹è®­ç»ƒ")
        trained_model, training_analysis = profiler.profile_training_process(
            config, dataloader, precision, pkg, version, knowledge_type
        )
        
        # ğŸ†• è·å–æ‰€æœ‰JSONæŠ¥å‘Š
        json_reports = profiler.get_json_reports()
        
        # ğŸ†• ç”Ÿæˆå†…å­˜é—®é¢˜æ£€æµ‹æŠ¥å‘Š
        print("ğŸ“Š ç¬¬ä¸‰é˜¶æ®µï¼šå†…å­˜é—®é¢˜æ£€æµ‹")
        memory_issues = profiler.detect_memory_issues()
        
        # ğŸ†• ç”Ÿæˆè®­ç»ƒå®Œæ•´æ€§æŠ¥å‘Š
        print("ğŸ“Š ç¬¬å››é˜¶æ®µï¼šç”Ÿæˆå®Œæ•´æ€§æŠ¥å‘Š")
        stage_comparisons = profiler.compare_training_stages()
        
        # æ„å»ºç»¼åˆåˆ†æç»“æœ
        comprehensive_analysis = {
            'creation_analysis': creation_analysis,
            'training_analysis': training_analysis,
            'memory_issues': memory_issues,
            'stage_comparisons': stage_comparisons,
            'json_reports': json_reports,  # ğŸ†• åŒ…å«æ‰€æœ‰é˜¶æ®µçš„è¯¦ç»†JSONæŠ¥å‘Š
            'summary': {
                'total_stages_analyzed': len(json_reports),
                'stages': list(json_reports.keys()),
                'peak_memory_stage': None,
                'memory_efficiency_score': None
            }
        }
        
        # ğŸ†• è®¡ç®—å³°å€¼å†…å­˜ä½¿ç”¨é˜¶æ®µ
        peak_memory = 0
        peak_stage = None
        for stage, report in json_reports.items():
            if 'memory_summary' in report:
                memory_usage = report['memory_summary']['total_memory_mb']
                if memory_usage > peak_memory:
                    peak_memory = memory_usage
                    peak_stage = stage
        
        comprehensive_analysis['summary']['peak_memory_stage'] = peak_stage
        comprehensive_analysis['summary']['peak_memory_mb'] = peak_memory
        
        # ğŸ†• æ‰“å°è¯¦ç»†çš„JSONæŠ¥å‘Šæ‘˜è¦åˆ°æ—¥å¿—
        print("="*80)
        print("ğŸ“Š è¯¦ç»†å†…å­˜åˆ†ææŠ¥å‘Šæ‘˜è¦")
        print("="*80)
        
        for stage, report in json_reports.items():
            if 'memory_summary' in report:
                memory_summary = report['memory_summary']
                occupancy = report.get('occupancy_percentages', {})
                
                print(f"\nğŸ” é˜¶æ®µ: {stage}")
                print(f"  æ€»å†…å­˜ä½¿ç”¨: {memory_summary['total_memory_mb']:.2f} MB")
                print(f"  GPUå ç”¨ç‡: {occupancy.get('total_used_percentage', 0):.2f}%")
                print(f"  ç»„ä»¶åˆ†è§£:")
                
                for component, memory_mb in memory_summary['component_breakdown'].items():
                    component_percentage = occupancy.get('component_percentages', {}).get(component, 0)
                    print(f"    {component}: {memory_mb:.2f} MB ({component_percentage:.2f}%)")
        
        print(f"\nğŸ† å³°å€¼å†…å­˜ä½¿ç”¨é˜¶æ®µ: {peak_stage} ({peak_memory:.2f} MB)")
        print(f"ğŸ“‚ è¯¦ç»†JSONæŠ¥å‘Šå·²ä¿å­˜åˆ°: {log_dir}")
        print("="*80)
        
        return trained_model, comprehensive_analysis


def analyze_model_memory_only(model, model_name: str = "model", 
                            log_dir: str = "logs/model_memory_analysis"):
    """
    ä»…åˆ†ææ¨¡å‹çš„å†…å­˜ä½¿ç”¨æƒ…å†µï¼Œç”Ÿæˆè¯¦ç»†çš„JSONæŠ¥å‘Š
    
    Args:
        model: æ¨¡å‹å¯¹è±¡
        model_name: æ¨¡å‹åç§°
        log_dir: æ—¥å¿—ç›®å½•
        
    Returns:
        å†…å­˜åˆ†æç»“æœ
    """
    print(f"ğŸ” å¼€å§‹åˆ†ææ¨¡å‹å†…å­˜: {model_name}")
    
    # åˆ›å»ºå†…å­˜è°ƒè¯•å™¨
    debugger = create_memory_debugger(log_dir=log_dir, enable_real_time=False)
    
    try:
        # ğŸ†• ç”Ÿæˆè¯¦ç»†çš„JSONæŠ¥å‘Š
        print("ğŸ“Š ç”Ÿæˆè¯¦ç»†çš„JSONæŠ¥å‘Š...")
        json_report = debugger.generate_detailed_memory_json_report(
            model=model,
            model_name=model_name,
            optimizer=None,
            stage="model_analysis"
        )
        
        # ğŸ†• ä¼ ç»Ÿçš„å‚æ•°åˆ†æï¼ˆå‘åå…¼å®¹ï¼‰
        parameter_analysis = debugger.analyze_model_parameters(model, model_name)
        
        # ğŸ†• ç”Ÿæˆå†…å­˜æŠ¥å‘Š
        memory_report = debugger.generate_memory_report()
        
        # æ„å»ºç»¼åˆåˆ†æç»“æœ
        analysis_result = {
            'json_report': json_report,
            'parameter_analysis': parameter_analysis,
            'memory_report': memory_report,
            'summary': {
                'total_parameters': parameter_analysis['summary']['total_parameters'],
                'total_memory_mb': json_report['memory_summary']['total_memory_mb'],
                'gpu_occupancy_percentage': json_report.get('occupancy_percentages', {}).get('total_used_percentage', 0)
            }
        }
        
        print("âœ… æ¨¡å‹å†…å­˜åˆ†æå®Œæˆ")
        print(f"ğŸ“Š è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {log_dir}")
        
        return analysis_result
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹å†…å­˜åˆ†æå¤±è´¥: {e}")
        raise


# ğŸ†• æ–°å¢å‡½æ•°ï¼šç”Ÿæˆå†…å­˜ä¼˜åŒ–å»ºè®®
def generate_memory_optimization_suggestions(json_reports: Dict) -> Dict:
    """
    åŸºäºJSONæŠ¥å‘Šç”Ÿæˆå†…å­˜ä¼˜åŒ–å»ºè®®
    
    Args:
        json_reports: å„é˜¶æ®µçš„JSONæŠ¥å‘Šå­—å…¸
        
    Returns:
        ä¼˜åŒ–å»ºè®®å­—å…¸
    """
    suggestions = {
        "general_suggestions": [],
        "parameter_optimization": [],
        "gradient_optimization": [],
        "optimizer_optimization": [],
        "training_optimization": []
    }
    
    # åˆ†ææ‰€æœ‰é˜¶æ®µçš„æŠ¥å‘Š
    for stage, report in json_reports.items():
        if 'detailed_memory_breakdown' not in report:
            continue
            
        breakdown = report['detailed_memory_breakdown']
        occupancy = report.get('occupancy_percentages', {})
        
        # æ£€æŸ¥GPUå ç”¨ç‡
        total_occupancy = occupancy.get('total_used_percentage', 0)
        if total_occupancy > 90:
            suggestions["general_suggestions"].append(
                f"é˜¶æ®µ{stage}: GPUå ç”¨ç‡è¿‡é«˜({total_occupancy:.1f}%)ï¼Œå»ºè®®é™ä½batch_sizeæˆ–ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯"
            )
        elif total_occupancy < 50:
            suggestions["general_suggestions"].append(
                f"é˜¶æ®µ{stage}: GPUåˆ©ç”¨ç‡è¾ƒä½({total_occupancy:.1f}%)ï¼Œå¯ä»¥è€ƒè™‘å¢åŠ batch_sizeæé«˜è®­ç»ƒæ•ˆç‡"
            )
        
        # æ£€æŸ¥å‚æ•°åˆ†å¸ƒ
        if 'parameters' in breakdown:
            param_info = breakdown['parameters']
            lora_memory = param_info['parameter_groups'].get('lora_adapters', {}).get('total_memory_mb', 0)
            base_memory = param_info['parameter_groups'].get('base_model', {}).get('total_memory_mb', 0)
            
            if lora_memory > base_memory * 0.1:  # LoRAå‚æ•°å æ¯”è¿‡é«˜
                suggestions["parameter_optimization"].append(
                    f"é˜¶æ®µ{stage}: LoRAå‚æ•°å ç”¨è¿‡å¤šå†…å­˜({lora_memory:.1f}MB)ï¼Œå»ºè®®é™ä½rank(r)å€¼"
                )
        
        # æ£€æŸ¥æ¢¯åº¦å†…å­˜
        if 'gradients' in breakdown:
            grad_memory = breakdown['gradients']['total_memory_mb']
            if grad_memory > 1000:  # æ¢¯åº¦å†…å­˜è¶…è¿‡1GB
                suggestions["gradient_optimization"].append(
                    f"é˜¶æ®µ{stage}: æ¢¯åº¦å†…å­˜å ç”¨è¿‡é«˜({grad_memory:.1f}MB)ï¼Œå»ºè®®ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹æˆ–æ··åˆç²¾åº¦è®­ç»ƒ"
                )
        
        # æ£€æŸ¥ä¼˜åŒ–å™¨å†…å­˜
        if 'optimizer' in breakdown:
            optimizer_memory = breakdown['optimizer']['total_memory_mb']
            if optimizer_memory > 2000:  # ä¼˜åŒ–å™¨å†…å­˜è¶…è¿‡2GB
                suggestions["optimizer_optimization"].append(
                    f"é˜¶æ®µ{stage}: ä¼˜åŒ–å™¨å†…å­˜å ç”¨è¿‡é«˜({optimizer_memory:.1f}MB)ï¼Œå»ºè®®ä½¿ç”¨AdamWæˆ–å…¶ä»–å†…å­˜å‹å¥½çš„ä¼˜åŒ–å™¨"
                )
    
    return suggestions


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # ç¤ºä¾‹1ï¼šä½¿ç”¨è£…é¥°å™¨ç›‘æ§å‡½æ•°
    @monitor_memory(log_dir="logs/decorated_training")
    def example_training_function(profiler, config):
        # åœ¨è¿™ä¸ªå‡½æ•°ä¸­ï¼Œprofilerå·²ç»å¯ç”¨
        model, analysis = profiler.profile_model_creation(config, "example_model")
        return model
    
    # ç¤ºä¾‹2ï¼šä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    # with memory_profiled_training(log_dir="logs/context_training") as profiler:
    #     model, analysis = profiler.profile_model_creation(config, "context_model")
    #     
    #     # è·å–æ‰€æœ‰JSONæŠ¥å‘Š
    #     json_reports = profiler.get_json_reports()
    #     
    #     # ç”Ÿæˆä¼˜åŒ–å»ºè®®
    #     suggestions = generate_memory_optimization_suggestions(json_reports)
    
    pass 