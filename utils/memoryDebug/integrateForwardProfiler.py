#!/usr/bin/env python3
"""
å°†å‰å‘ä¼ æ’­åˆ†æå™¨é›†æˆåˆ°ç°æœ‰è®­ç»ƒä»£ç ä¸­
"""

import torch
import torch.nn as nn
from typing import Dict, Optional, Any, Callable
import os
from datetime import datetime
import traceback

try:
    from .forwardPassProfiler import ForwardPassProfiler, ProfiledModel, profile_model_forward
    from .trainWithForwardProfiler import TrainingForwardProfiler
except ImportError:
    print("Warning: æ— æ³•å¯¼å…¥å‰å‘ä¼ æ’­åˆ†æå™¨æ¨¡å—ï¼Œåˆ†æåŠŸèƒ½å°†è¢«ç¦ç”¨")
    ForwardPassProfiler = None
    ProfiledModel = None
    TrainingForwardProfiler = None

def enhance_training_with_forward_profiling(
    original_training_function: Callable,
    enable_profiling: bool = True,
    profile_first_batch: bool = True,
    profile_memory_pattern: bool = True,
    continuous_profiling: bool = False,
    log_dir: str = "logs/enhanced_forward_profiling"
):
    """
    è£…é¥°å™¨ï¼šä¸ºç°æœ‰è®­ç»ƒå‡½æ•°æ·»åŠ å‰å‘ä¼ æ’­åˆ†æåŠŸèƒ½
    
    Args:
        original_training_function: åŸå§‹è®­ç»ƒå‡½æ•°
        enable_profiling: æ˜¯å¦å¯ç”¨åˆ†æ
        profile_first_batch: æ˜¯å¦åˆ†æç¬¬ä¸€ä¸ªbatch
        profile_memory_pattern: æ˜¯å¦åˆ†æå†…å­˜ä½¿ç”¨æ¨¡å¼
        continuous_profiling: æ˜¯å¦å¯ç”¨æŒç»­åˆ†æ
        log_dir: æ—¥å¿—ç›®å½•
        
    Returns:
        å¢å¼ºåçš„è®­ç»ƒå‡½æ•°
    """
    
    def enhanced_training(*args, **kwargs):
        if not enable_profiling or ForwardPassProfiler is None:
            # å¦‚æœæœªå¯ç”¨åˆ†ææˆ–æ¨¡å—ä¸å¯ç”¨ï¼Œç›´æ¥æ‰§è¡ŒåŸå‡½æ•°
            return original_training_function(*args, **kwargs)
            
        print("\n" + "ğŸ”¬" + "="*70)
        print("ğŸ”¬ å¯åŠ¨å¢å¼ºå‹å‰å‘ä¼ æ’­åˆ†æè®­ç»ƒ")
        print("ğŸ”¬" + "="*70)
        
        # ä»å‚æ•°ä¸­æå–æ¨¡å‹å’Œæ•°æ®åŠ è½½å™¨ï¼ˆéœ€è¦æ ¹æ®å®é™…å‡½æ•°ç­¾åè°ƒæ•´ï¼‰
        model = None
        dataloader = None
        
        # å°è¯•ä»argså’Œkwargsä¸­æ‰¾åˆ°modelå’Œdataloader
        for arg in args:
            if isinstance(arg, nn.Module):
                model = arg
                break
                
        # å¸¸è§çš„å‚æ•°å
        for key in ['model', 'lora_model', 'dataloader', 'train_dataloader']:
            if key in kwargs and kwargs[key] is not None:
                if isinstance(kwargs[key], nn.Module):
                    model = kwargs[key]
                elif hasattr(kwargs[key], '__iter__'):  # å¯èƒ½æ˜¯dataloader
                    dataloader = kwargs[key]
                    
        if model is None:
            print("âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ°æ¨¡å‹å¯¹è±¡ï¼Œè·³è¿‡å‰å‘ä¼ æ’­åˆ†æ")
            return original_training_function(*args, **kwargs)
            
        # åˆ›å»ºåˆ†æå™¨
        profiler_mgr = TrainingForwardProfiler(log_dir=log_dir)
        analysis_results = {}
        
        try:
            # é˜¶æ®µ1ï¼šåˆ†æç¬¬ä¸€ä¸ªbatchï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if profile_first_batch and dataloader is not None:
                try:
                    print("\nğŸ” é˜¶æ®µ1ï¼šç¬¬ä¸€ä¸ªbatchå‰å‘ä¼ æ’­åˆ†æ")
                    first_batch = next(iter(dataloader))
                    analysis_results["first_batch"] = profiler_mgr.profile_single_batch(
                        model, first_batch, stage="first_batch_analysis"
                    )
                except Exception as e:
                    print(f"âš ï¸ ç¬¬ä¸€ä¸ªbatchåˆ†æå¤±è´¥: {e}")
                    
            # é˜¶æ®µ2ï¼šå†…å­˜ä½¿ç”¨æ¨¡å¼åˆ†æï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if profile_memory_pattern and dataloader is not None:
                try:
                    print("\nğŸ“Š é˜¶æ®µ2ï¼šå†…å­˜ä½¿ç”¨æ¨¡å¼åˆ†æ")
                    analysis_results["memory_pattern"] = profiler_mgr.analyze_memory_pattern(
                        model, dataloader, num_batches=2
                    )
                except Exception as e:
                    print(f"âš ï¸ å†…å­˜æ¨¡å¼åˆ†æå¤±è´¥: {e}")
                    
            # é˜¶æ®µ3ï¼šæŒç»­åˆ†æï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if continuous_profiling:
                print("\nğŸ“ˆ é˜¶æ®µ3ï¼šè®¾ç½®æŒç»­å‰å‘ä¼ æ’­åˆ†æ")
                try:
                    profiler_mgr.setup_continuous_profiling(model)
                    profiler_mgr.start_continuous_profiling()
                except Exception as e:
                    print(f"âš ï¸ æŒç»­åˆ†æè®¾ç½®å¤±è´¥: {e}")
                    continuous_profiling = False
                    
            # æ‰§è¡ŒåŸå§‹è®­ç»ƒå‡½æ•°
            print("\nğŸš‚ å¼€å§‹æ‰§è¡Œè®­ç»ƒ...")
            training_result = original_training_function(*args, **kwargs)
            
            # åœæ­¢æŒç»­åˆ†æ
            if continuous_profiling:
                try:
                    analysis_results["continuous"] = profiler_mgr.stop_continuous_profiling()
                except Exception as e:
                    print(f"âš ï¸ åœæ­¢æŒç»­åˆ†æå¤±è´¥: {e}")
                    
            # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            _generate_enhanced_training_report(analysis_results, log_dir)
            
            return training_result
            
        except Exception as e:
            print(f"âŒ å¢å¼ºè®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            traceback.print_exc()
            
            # ç¡®ä¿æ¸…ç†èµ„æº
            try:
                if profiler_mgr.profiler is not None:
                    profiler_mgr.stop_continuous_profiling()
            except:
                pass
                
            # ä»ç„¶æ‰§è¡ŒåŸå§‹è®­ç»ƒå‡½æ•°
            print("ğŸ”„ å›é€€åˆ°åŸå§‹è®­ç»ƒå‡½æ•°...")
            return original_training_function(*args, **kwargs)
    
    return enhanced_training

def _generate_enhanced_training_report(analysis_results: Dict, log_dir: str):
    """ç”Ÿæˆå¢å¼ºè®­ç»ƒçš„åˆ†ææŠ¥å‘Š"""
    try:
        import json
        
        report = {
            "timestamp": datetime.now().isoformat(),
            "enhanced_training_summary": {
                "first_batch_analyzed": "first_batch" in analysis_results,
                "memory_pattern_analyzed": "memory_pattern" in analysis_results,
                "continuous_profiling_completed": "continuous" in analysis_results
            },
            "analysis_results": analysis_results,
            "recommendations": []
        }
        
        # ç”Ÿæˆå»ºè®®
        if "memory_pattern" in analysis_results:
            pattern = analysis_results["memory_pattern"].get("memory_pattern", {})
            if pattern.get("potential_issues"):
                report["recommendations"].extend([
                    "æ£€æµ‹åˆ°æ½œåœ¨çš„å†…å­˜ä½¿ç”¨é—®é¢˜",
                    "å»ºè®®æŸ¥çœ‹è¯¦ç»†çš„æ¨¡å—åˆ†ææŠ¥å‘Š",
                    "è€ƒè™‘ä¼˜åŒ–å†…å­˜ä½¿ç”¨æœ€é«˜çš„æ¨¡å—"
                ])
                
        # ä¿å­˜æŠ¥å‘Š
        report_file = os.path.join(log_dir, "enhanced_training_report.json")
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
            
        print(f"ğŸ“‹ å¢å¼ºè®­ç»ƒåˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
    except Exception as e:
        print(f"âš ï¸ ç”Ÿæˆå¢å¼ºè®­ç»ƒæŠ¥å‘Šå¤±è´¥: {e}")

# ç®€åŒ–çš„é›†æˆå‡½æ•°ï¼Œä¸“é—¨é’ˆå¯¹LoRAè®­ç»ƒ
def add_forward_profiling_to_lora_training(
    model,
    dataloader, 
    enable_first_batch_analysis: bool = True,
    enable_memory_pattern_analysis: bool = False,
    enable_continuous_profiling: bool = False,
    analysis_log_dir: str = "logs/lora_forward_profiling"
) -> Dict:
    """
    ä¸ºLoRAè®­ç»ƒæ·»åŠ å‰å‘ä¼ æ’­åˆ†æ
    
    Args:
        model: LoRAæ¨¡å‹
        dataloader: æ•°æ®åŠ è½½å™¨
        enable_first_batch_analysis: æ˜¯å¦åˆ†æç¬¬ä¸€ä¸ªbatch
        enable_memory_pattern_analysis: æ˜¯å¦åˆ†æå†…å­˜æ¨¡å¼
        enable_continuous_profiling: æ˜¯å¦å¯ç”¨æŒç»­åˆ†æ
        analysis_log_dir: åˆ†ææ—¥å¿—ç›®å½•
        
    Returns:
        åˆ†æç»“æœå­—å…¸
    """
    if ForwardPassProfiler is None:
        print("âš ï¸ å‰å‘ä¼ æ’­åˆ†æå™¨ä¸å¯ç”¨ï¼Œè·³è¿‡åˆ†æ")
        return {}
        
    print("\n" + "ğŸ”¬" + "="*60)
    print("ğŸ”¬ LoRAè®­ç»ƒå‰å‘ä¼ æ’­åˆ†æ")
    print("ğŸ”¬" + "="*60)
    
    profiler_mgr = TrainingForwardProfiler(log_dir=analysis_log_dir)
    results = {}
    
    try:
        # ç¬¬ä¸€ä¸ªbatchåˆ†æ
        if enable_first_batch_analysis:
            print("\nğŸ” åˆ†æç¬¬ä¸€ä¸ªbatchçš„å‰å‘ä¼ æ’­...")
            try:
                first_batch = next(iter(dataloader))
                results["first_batch_analysis"] = profiler_mgr.profile_single_batch(
                    model, first_batch, stage="lora_first_batch"
                )
                print("âœ… ç¬¬ä¸€ä¸ªbatchåˆ†æå®Œæˆ")
            except Exception as e:
                print(f"âŒ ç¬¬ä¸€ä¸ªbatchåˆ†æå¤±è´¥: {e}")
                
        # å†…å­˜æ¨¡å¼åˆ†æ
        if enable_memory_pattern_analysis:
            print("\nğŸ“Š åˆ†æå†…å­˜ä½¿ç”¨æ¨¡å¼...")
            try:
                results["memory_pattern_analysis"] = profiler_mgr.analyze_memory_pattern(
                    model, dataloader, num_batches=2
                )
                print("âœ… å†…å­˜æ¨¡å¼åˆ†æå®Œæˆ")
            except Exception as e:
                print(f"âŒ å†…å­˜æ¨¡å¼åˆ†æå¤±è´¥: {e}")
                
        # æŒç»­åˆ†æè®¾ç½®
        if enable_continuous_profiling:
            print("\nğŸ“ˆ è®¾ç½®æŒç»­å‰å‘ä¼ æ’­åˆ†æ...")
            try:
                profiler_mgr.setup_continuous_profiling(model)
                profiler_mgr.start_continuous_profiling()
                results["continuous_profiler"] = profiler_mgr
                print("âœ… æŒç»­åˆ†æå·²å¯åŠ¨")
            except Exception as e:
                print(f"âŒ æŒç»­åˆ†æè®¾ç½®å¤±è´¥: {e}")
                
        return results
        
    except Exception as e:
        print(f"âŒ LoRAè®­ç»ƒå‰å‘ä¼ æ’­åˆ†æå‡ºé”™: {e}")
        return {}

def finalize_lora_forward_profiling(analysis_results: Dict, log_dir: str = "logs/lora_forward_profiling"):
    """
    å®ŒæˆLoRAè®­ç»ƒçš„å‰å‘ä¼ æ’­åˆ†æ
    
    Args:
        analysis_results: ä»add_forward_profiling_to_lora_trainingè¿”å›çš„ç»“æœ
        log_dir: æ—¥å¿—ç›®å½•
    """
    if "continuous_profiler" in analysis_results:
        try:
            profiler_mgr = analysis_results["continuous_profiler"]
            continuous_results = profiler_mgr.stop_continuous_profiling()
            analysis_results["continuous_analysis"] = continuous_results
            print("âœ… æŒç»­å‰å‘ä¼ æ’­åˆ†æå·²å®Œæˆ")
        except Exception as e:
            print(f"âŒ å®ŒæˆæŒç»­åˆ†æå¤±è´¥: {e}")
            
    # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    try:
        _generate_lora_profiling_summary(analysis_results, log_dir)
    except Exception as e:
        print(f"âš ï¸ ç”ŸæˆLoRAåˆ†ææ€»ç»“å¤±è´¥: {e}")

def _generate_lora_profiling_summary(analysis_results: Dict, log_dir: str):
    """ç”ŸæˆLoRAå‰å‘ä¼ æ’­åˆ†ææ€»ç»“"""
    import json
    
    summary = {
        "timestamp": datetime.now().isoformat(),
        "lora_forward_profiling_summary": {
            "analyses_completed": list(analysis_results.keys()),
            "total_analyses": len(analysis_results)
        },
        "key_findings": [],
        "performance_insights": [],
        "detailed_results": analysis_results
    }
    
    # åˆ†æå…³é”®å‘ç°
    if "first_batch_analysis" in analysis_results:
        batch_analysis = analysis_results["first_batch_analysis"]
        if "summary" in batch_analysis:
            memory_analysis = batch_analysis["summary"].get("memory_analysis", {})
            peak_memory = memory_analysis.get("peak_memory_usage", 0)
            if peak_memory > 0:
                summary["key_findings"].append(f"ç¬¬ä¸€ä¸ªbatchå³°å€¼å†…å­˜ä½¿ç”¨: {peak_memory:.2f}MB")
                
    if "memory_pattern_analysis" in analysis_results:
        pattern_analysis = analysis_results["memory_pattern_analysis"]
        pattern = pattern_analysis.get("memory_pattern", {})
        if pattern.get("potential_issues"):
            summary["key_findings"].extend(pattern["potential_issues"])
            
    # ç”Ÿæˆæ€§èƒ½æ´å¯Ÿ
    summary["performance_insights"].append("å»ºè®®æŸ¥çœ‹è¯¦ç»†çš„JSONæŠ¥å‘Šè·å–æ›´å¤šä¿¡æ¯")
    if summary["key_findings"]:
        summary["performance_insights"].append("æ£€æµ‹åˆ°æ½œåœ¨çš„å†…å­˜ä¼˜åŒ–æœºä¼š")
        
    # ä¿å­˜æ€»ç»“
    summary_file = os.path.join(log_dir, "lora_forward_profiling_summary.json")
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
        
    print(f"ğŸ“‹ LoRAå‰å‘ä¼ æ’­åˆ†ææ€»ç»“å·²ä¿å­˜: {summary_file}")
    
    # æ‰“å°ç®€è¦æ€»ç»“
    print("\n" + "ğŸ“Š" + "="*60)
    print("ğŸ“Š LoRAå‰å‘ä¼ æ’­åˆ†ææ€»ç»“")
    print("ğŸ“Š" + "="*60)
    print(f"âœ… å®Œæˆçš„åˆ†æ: {len(analysis_results)}")
    if summary["key_findings"]:
        print("ğŸ” å…³é”®å‘ç°:")
        for finding in summary["key_findings"]:
            print(f"  - {finding}")
    print("ğŸ“Š" + "="*60)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("""
LoRAè®­ç»ƒå‰å‘ä¼ æ’­åˆ†æå™¨é›†æˆç¤ºä¾‹:

# æ–¹å¼1ï¼šè£…é¥°å™¨æ–¹å¼ï¼ˆæ¨èç”¨äºæ–°ä»£ç ï¼‰
@enhance_training_with_forward_profiling(
    enable_profiling=True,
    profile_first_batch=True,
    continuous_profiling=False
)
def train_lora_model(model, dataloader, ...):
    # åŸæœ‰çš„è®­ç»ƒä»£ç 
    pass

# æ–¹å¼2ï¼šæ‰‹åŠ¨é›†æˆæ–¹å¼ï¼ˆæ¨èç”¨äºç°æœ‰ä»£ç ï¼‰
def train_lora_with_profiling(model, dataloader, ...):
    # åœ¨è®­ç»ƒå¼€å§‹å‰è¿›è¡Œåˆ†æ
    analysis_results = add_forward_profiling_to_lora_training(
        model=model,
        dataloader=dataloader,
        enable_first_batch_analysis=True,
        enable_memory_pattern_analysis=True,
        enable_continuous_profiling=True
    )
    
    try:
        # æ‰§è¡Œå®é™…è®­ç»ƒ
        training_result = train_lora_model(model, dataloader, ...)
        
    finally:
        # å®Œæˆåˆ†æ
        finalize_lora_forward_profiling(analysis_results)
    
    return training_result

# æ–¹å¼3ï¼šæœ€ç®€å•çš„æ–¹å¼ï¼ˆä»…åˆ†æç¬¬ä¸€ä¸ªbatchï¼‰
analysis = add_forward_profiling_to_lora_training(
    model=my_model,
    dataloader=my_dataloader,
    enable_first_batch_analysis=True
)
    """) 