#!/usr/bin/env python3
"""
å¿«é€Ÿå†…å­˜è°ƒè¯•å·¥å…·
æä¾›ç®€å•çš„æ¥å£æ¥å¿«é€Ÿç›‘æ§å’Œè°ƒè¯•è®­ç»ƒè¿‡ç¨‹ä¸­çš„å†…å­˜ä½¿ç”¨æƒ…å†µ
"""

import os
import sys
import torch
import logging
from datetime import datetime
from typing import Dict, Any, Optional

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

from memoryDebugger import MemoryDebugger

class QuickMemoryDebugger:
    """å¿«é€Ÿå†…å­˜è°ƒè¯•å™¨"""
    
    def __init__(self, log_dir: str = None, enable_logging: bool = True):
        if log_dir is None:
            log_dir = f"logs/quick_debug_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        self.log_dir = log_dir
        self.enable_logging = enable_logging
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        os.makedirs(log_dir, exist_ok=True)
        
        # åˆå§‹åŒ–è°ƒè¯•å™¨
        self.debugger = MemoryDebugger(log_dir=log_dir, enable_real_time=False)
        
        # è®¾ç½®ç®€å•çš„æ—¥å¿—è®°å½•
        if enable_logging:
            self.logger = self._setup_simple_logger()
        else:
            self.logger = None
    
    def _setup_simple_logger(self):
        """è®¾ç½®ç®€å•çš„æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger("QuickMemoryDebug")
        logger.setLevel(logging.INFO)
        
        # æ¸…é™¤ç°æœ‰å¤„ç†å™¨
        logger.handlers.clear()
        
        # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
        log_file = os.path.join(self.log_dir, "quick_debug.log")
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.INFO)
        
        # åˆ›å»ºæ ¼å¼åŒ–å™¨
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        
        logger.addHandler(file_handler)
        
        return logger
    
    def log_info(self, message: str):
        """è®°å½•ä¿¡æ¯"""
        if self.logger:
            self.logger.info(message)
        print(f"[INFO] {message}")
    
    def log_warning(self, message: str):
        """è®°å½•è­¦å‘Š"""
        if self.logger:
            self.logger.warning(message)
        print(f"[WARNING] {message}")
    
    def log_error(self, message: str):
        """è®°å½•é”™è¯¯"""
        if self.logger:
            self.logger.error(message)
        print(f"[ERROR] {message}")
    
    def get_memory_snapshot(self, name: str = "snapshot") -> Dict:
        """è·å–å†…å­˜å¿«ç…§"""
        self.log_info(f"è·å–å†…å­˜å¿«ç…§: {name}")
        
        memory_info = self.debugger.get_current_memory_usage()
        
        # ç®€åŒ–çš„å†…å­˜ä¿¡æ¯
        snapshot = {
            'name': name,
            'timestamp': memory_info['timestamp'],
            'gpu_summary': []
        }
        
        for gpu_mem in memory_info['gpu_memory']:
            gpu_summary = {
                'gpu_id': gpu_mem['device_id'],
                'allocated_mb': gpu_mem['allocated_mb'],
                'utilization_percent': gpu_mem['utilization_percent'],
                'free_mb': gpu_mem['free_mb']
            }
            snapshot['gpu_summary'].append(gpu_summary)
            
            self.log_info(f"  GPU {gpu_mem['device_id']}: "
                         f"{gpu_mem['allocated_mb']:.1f}MB / "
                         f"{gpu_mem['total_mb']:.1f}MB "
                         f"({gpu_mem['utilization_percent']:.1f}%)")
        
        return snapshot
    
    def analyze_model_quick(self, model, name: str = "model") -> Dict:
        """å¿«é€Ÿåˆ†ææ¨¡å‹å‚æ•°"""
        self.log_info(f"å¿«é€Ÿåˆ†ææ¨¡å‹: {name}")
        
        analysis = self.debugger.analyze_model_parameters(model, name)
        
        # æå–å…³é”®ä¿¡æ¯
        summary = analysis['summary']
        quick_summary = {
            'model_name': name,
            'total_params': summary['total_parameters'],
            'trainable_params': summary['trainable_parameters'],
            'memory_mb': summary['total_memory_mb'],
            'param_efficiency': summary['trainable_parameters'] / summary['total_parameters'] * 100
        }
        
        self.log_info(f"  æ€»å‚æ•°: {quick_summary['total_params']:,}")
        self.log_info(f"  å¯è®­ç»ƒå‚æ•°: {quick_summary['trainable_params']:,}")
        self.log_info(f"  å†…å­˜å ç”¨: {quick_summary['memory_mb']:.2f} MB")
        self.log_info(f"  å‚æ•°æ•ˆç‡: {quick_summary['param_efficiency']:.2f}%")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸
        self._check_model_anomalies(analysis)
        
        return quick_summary
    
    def _check_model_anomalies(self, analysis: Dict):
        """æ£€æŸ¥æ¨¡å‹å¼‚å¸¸"""
        summary = analysis['summary']
        
        # æ£€æŸ¥å¼‚å¸¸å¤§çš„å‚æ•°
        for group_name, group_info in analysis['parameter_groups'].items():
            for param_info in group_info['parameters']:
                if param_info['memory_mb'] > 100:  # è¶…è¿‡100MBçš„å‚æ•°
                    self.log_warning(f"å¤§å‚æ•°æ£€æµ‹: {param_info['name']} "
                                   f"({param_info['memory_mb']:.2f} MB)")
                
                # æ£€æŸ¥æ•°æ®å¼‚å¸¸
                if 'data_stats' in param_info:
                    stats = param_info['data_stats']
                    if isinstance(stats, dict) and 'has_nan' in stats:
                        if stats['has_nan']:
                            self.log_error(f"å‚æ•°åŒ…å«NaN: {param_info['name']}")
                        if stats['has_inf']:
                            self.log_error(f"å‚æ•°åŒ…å«Inf: {param_info['name']}")
    
    def compare_snapshots(self, snapshot1: Dict, snapshot2: Dict) -> Dict:
        """æ¯”è¾ƒä¸¤ä¸ªå†…å­˜å¿«ç…§"""
        self.log_info(f"æ¯”è¾ƒå†…å­˜å¿«ç…§: {snapshot1['name']} vs {snapshot2['name']}")
        
        comparison = {
            'from': snapshot1['name'],
            'to': snapshot2['name'],
            'gpu_changes': []
        }
        
        for gpu1, gpu2 in zip(snapshot1['gpu_summary'], snapshot2['gpu_summary']):
            change = {
                'gpu_id': gpu1['gpu_id'],
                'memory_change_mb': gpu2['allocated_mb'] - gpu1['allocated_mb'],
                'utilization_change': gpu2['utilization_percent'] - gpu1['utilization_percent']
            }
            comparison['gpu_changes'].append(change)
            
            self.log_info(f"  GPU {gpu1['gpu_id']}: "
                         f"å†…å­˜å˜åŒ– {change['memory_change_mb']:+.1f}MB, "
                         f"åˆ©ç”¨ç‡å˜åŒ– {change['utilization_change']:+.1f}%")
        
        return comparison
    
    def monitor_function(self, func, *args, **kwargs):
        """ç›‘æ§å‡½æ•°æ‰§è¡Œçš„å†…å­˜å˜åŒ–"""
        func_name = func.__name__ if hasattr(func, '__name__') else str(func)
        self.log_info(f"å¼€å§‹ç›‘æ§å‡½æ•°: {func_name}")
        
        # æ‰§è¡Œå‰å¿«ç…§
        before_snapshot = self.get_memory_snapshot(f"{func_name}_before")
        
        try:
            # æ‰§è¡Œå‡½æ•°
            result = func(*args, **kwargs)
            
            # æ‰§è¡Œåå¿«ç…§
            after_snapshot = self.get_memory_snapshot(f"{func_name}_after")
            
            # æ¯”è¾ƒå¿«ç…§
            comparison = self.compare_snapshots(before_snapshot, after_snapshot)
            
            self.log_info(f"å‡½æ•° {func_name} æ‰§è¡Œå®Œæˆ")
            
            return result, comparison
            
        except Exception as e:
            error_snapshot = self.get_memory_snapshot(f"{func_name}_error")
            self.log_error(f"å‡½æ•° {func_name} æ‰§è¡Œå¤±è´¥: {e}")
            
            error_comparison = self.compare_snapshots(before_snapshot, error_snapshot)
            
            raise e
    
    def check_memory_growth(self, threshold_mb: float = 100.0):
        """æ£€æŸ¥å†…å­˜å¢é•¿"""
        memory_info = self.debugger.get_current_memory_usage()
        
        for gpu_mem in memory_info['gpu_memory']:
            if gpu_mem['allocated_mb'] > threshold_mb:
                self.log_warning(f"GPU {gpu_mem['device_id']} å†…å­˜ä½¿ç”¨è¶…è¿‡é˜ˆå€¼: "
                               f"{gpu_mem['allocated_mb']:.1f}MB > {threshold_mb}MB")
            
            if gpu_mem['utilization_percent'] > 90:
                self.log_warning(f"GPU {gpu_mem['device_id']} åˆ©ç”¨ç‡è¿‡é«˜: "
                               f"{gpu_mem['utilization_percent']:.1f}%")
    
    def cleanup_memory(self):
        """æ¸…ç†å†…å­˜"""
        self.log_info("æ¸…ç†GPUå†…å­˜...")
        
        before_snapshot = self.get_memory_snapshot("before_cleanup")
        
        # æ¸…ç†GPUç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        import gc
        gc.collect()
        
        after_snapshot = self.get_memory_snapshot("after_cleanup")
        
        # æ¯”è¾ƒæ¸…ç†æ•ˆæœ
        comparison = self.compare_snapshots(before_snapshot, after_snapshot)
        
        self.log_info("å†…å­˜æ¸…ç†å®Œæˆ")
        
        return comparison
    
    def generate_quick_report(self) -> str:
        """ç”Ÿæˆå¿«é€ŸæŠ¥å‘Š"""
        self.log_info("ç”Ÿæˆå¿«é€ŸæŠ¥å‘Š...")
        
        report_lines = []
        report_lines.append("=" * 60)
        report_lines.append("å¿«é€Ÿå†…å­˜è°ƒè¯•æŠ¥å‘Š")
        report_lines.append("=" * 60)
        report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"æ—¥å¿—ç›®å½•: {self.log_dir}")
        report_lines.append("")
        
        # å½“å‰å†…å­˜çŠ¶æ€
        current_memory = self.debugger.get_current_memory_usage()
        report_lines.append("å½“å‰å†…å­˜çŠ¶æ€:")
        
        for gpu_mem in current_memory['gpu_memory']:
            report_lines.append(f"  GPU {gpu_mem['device_id']}: "
                              f"{gpu_mem['allocated_mb']:.1f}MB / "
                              f"{gpu_mem['total_mb']:.1f}MB "
                              f"({gpu_mem['utilization_percent']:.1f}%)")
        
        # ç³»ç»Ÿå†…å­˜
        sys_mem = current_memory['system_memory']
        report_lines.append(f"ç³»ç»Ÿå†…å­˜: {sys_mem['used_gb']:.1f}GB / "
                          f"{sys_mem['total_gb']:.1f}GB ({sys_mem['percent']:.1f}%)")
        
        report_content = "\n".join(report_lines)
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = os.path.join(self.log_dir, "quick_report.txt")
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        self.log_info(f"å¿«é€ŸæŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        return report_content

# å…¨å±€å¿«é€Ÿè°ƒè¯•å™¨å®ä¾‹
_quick_debugger = None

def get_quick_debugger(log_dir: str = None) -> QuickMemoryDebugger:
    """è·å–å…¨å±€å¿«é€Ÿè°ƒè¯•å™¨å®ä¾‹"""
    global _quick_debugger
    if _quick_debugger is None:
        _quick_debugger = QuickMemoryDebugger(log_dir=log_dir)
    return _quick_debugger

# ä¾¿æ·å‡½æ•°
def memory_snapshot(name: str = "snapshot"):
    """å¿«é€Ÿå†…å­˜å¿«ç…§"""
    debugger = get_quick_debugger()
    return debugger.get_memory_snapshot(name)

def analyze_model(model, name: str = "model"):
    """å¿«é€Ÿåˆ†ææ¨¡å‹"""
    debugger = get_quick_debugger()
    return debugger.analyze_model_quick(model, name)

def monitor_function(func, *args, **kwargs):
    """ç›‘æ§å‡½æ•°å†…å­˜ä½¿ç”¨"""
    debugger = get_quick_debugger()
    return debugger.monitor_function(func, *args, **kwargs)

def check_memory_status(threshold_mb: float = 100.0):
    """æ£€æŸ¥å†…å­˜çŠ¶æ€"""
    debugger = get_quick_debugger()
    return debugger.check_memory_growth(threshold_mb)

def cleanup_memory():
    """æ¸…ç†å†…å­˜"""
    debugger = get_quick_debugger()
    return debugger.cleanup_memory()

def generate_report():
    """ç”ŸæˆæŠ¥å‘Š"""
    debugger = get_quick_debugger()
    return debugger.generate_quick_report()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # ç¤ºä¾‹ï¼šå¿«é€Ÿå†…å­˜è°ƒè¯•
    print("ğŸš€ å¿«é€Ÿå†…å­˜è°ƒè¯•ç¤ºä¾‹")
    
    # è·å–åˆå§‹å†…å­˜å¿«ç…§
    initial_snapshot = memory_snapshot("initial")
    
    # æ¨¡æ‹Ÿä¸€äº›å†…å­˜æ“ä½œ
    def dummy_operation():
        import time
        if torch.cuda.is_available():
            dummy_tensor = torch.randn(1000, 1000).cuda()
            time.sleep(1)
            return dummy_tensor
        else:
            dummy_tensor = torch.randn(1000, 1000)
            time.sleep(1)
            return dummy_tensor
    
    # ç›‘æ§å‡½æ•°æ‰§è¡Œ
    result, comparison = monitor_function(dummy_operation)
    print(f"å†…å­˜å˜åŒ–: {comparison}")
    
    # æ£€æŸ¥å†…å­˜çŠ¶æ€
    check_memory_status(threshold_mb=50.0)
    
    # æ¸…ç†å†…å­˜
    cleanup_comparison = cleanup_memory()
    
    # ç”ŸæˆæŠ¥å‘Š
    report = generate_report()
    print("\næŠ¥å‘Šé¢„è§ˆ:")
    print(report[:500] + "...")
    
    print("\nâœ… å¿«é€Ÿå†…å­˜è°ƒè¯•å®Œæˆ!") 