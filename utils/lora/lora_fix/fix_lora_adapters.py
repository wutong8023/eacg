#!/usr/bin/env python3
"""
LoRAé€‚é…å™¨æ¨¡å‹æ–‡ä»¶ä¿®å¤å·¥å…·
ç”¨äºæ£€æŸ¥å’Œä¿®å¤ç¼ºå¤±çš„adapter_model.safetensorså’Œé…ç½®æ–‡ä»¶
"""

import os
import shutil
import glob
import argparse
import logging
import re
from datetime import datetime
from pathlib import Path

def setup_logging(verbose=False):
    """è®¾ç½®æ—¥å¿—é…ç½®"""
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler()
        ]
    )

def find_lora_model_directories(base_path):
    """
    æŸ¥æ‰¾æ‰€æœ‰LoRAæ¨¡å‹ç›®å½•
    
    Args:
        base_path: åŸºç¡€è·¯å¾„ï¼Œå¦‚ /datanfs2/chenrongyi/models/loraadaptors/
        
    Returns:
        list: LoRAæ¨¡å‹ç›®å½•åˆ—è¡¨
    """
    lora_dirs = []
    
    # éå†æ‰€æœ‰æ¨¡å‹ç›®å½•
    if not os.path.exists(base_path):
        logging.error(f"åŸºç¡€è·¯å¾„ä¸å­˜åœ¨: {base_path}")
        return lora_dirs
    
    # æŸ¥æ‰¾æ¨¡å¼: base_path/model_name/package_name/package_version_knowledge_type_...
    for model_dir in os.listdir(base_path):
        model_path = os.path.join(base_path, model_dir)
        if not os.path.isdir(model_path):
            continue
            
        for pkg_dir in os.listdir(model_path):
            pkg_path = os.path.join(model_path, pkg_dir)
            if not os.path.isdir(pkg_path):
                continue
                
            for version_dir in os.listdir(pkg_path):
                version_path = os.path.join(pkg_path, version_dir)
                if os.path.isdir(version_path):
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«checkpointsç›®å½•ï¼Œç¡®è®¤æ˜¯LoRAæ¨¡å‹ç›®å½•
                    checkpoints_path = os.path.join(version_path, "checkpoints")
                    if os.path.exists(checkpoints_path):
                        lora_dirs.append(version_path)
                        logging.debug(f"æ‰¾åˆ°LoRAæ¨¡å‹ç›®å½•: {version_path}")
    
    logging.info(f"æ€»å…±æ‰¾åˆ° {len(lora_dirs)} ä¸ªLoRAæ¨¡å‹ç›®å½•")
    return lora_dirs

def check_required_files(model_dir):
    """
    æ£€æŸ¥ä¸»ç›®å½•æ˜¯å¦åŒ…å«å¿…éœ€çš„æ–‡ä»¶
    
    Args:
        model_dir: æ¨¡å‹ç›®å½•è·¯å¾„
        
    Returns:
        dict: ç¼ºå¤±æ–‡ä»¶çš„çŠ¶æ€
    """
    required_files = {
        'adapter_model.safetensors': False,
        'adapter_config.json': False,
    }
    
    # å¯é€‰æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ä¼šä¸€èµ·å¤åˆ¶ï¼‰
    optional_files = [
        'README.md',
        'training_args.bin',
        'trainer_state.json',
        'pytorch_model.bin'  # å¯èƒ½æœ‰äº›æ¨¡å‹ä½¿ç”¨è¿™ä¸ªæ ¼å¼
    ]
    
    for filename in required_files.keys():
        file_path = os.path.join(model_dir, filename)
        if os.path.exists(file_path):
            required_files[filename] = True
            logging.debug(f"æ‰¾åˆ°æ–‡ä»¶: {file_path}")
    
    missing_count = sum(1 for exists in required_files.values() if not exists)
    
    return {
        'required_files': required_files,
        'missing_count': missing_count,
        'all_present': missing_count == 0
    }

def find_best_checkpoint(checkpoints_dir):
    """
    æ‰¾åˆ°æœ€ä½³çš„checkpointç›®å½•ï¼ˆä¼˜å…ˆepoch4ï¼Œç„¶åæ˜¯æœ€é«˜epochï¼‰
    
    Args:
        checkpoints_dir: checkpointsç›®å½•è·¯å¾„
        
    Returns:
        str or None: æœ€ä½³checkpointç›®å½•è·¯å¾„
    """
    if not os.path.exists(checkpoints_dir):
        return None
    
    checkpoint_dirs = []
    for item in os.listdir(checkpoints_dir):
        item_path = os.path.join(checkpoints_dir, item)
        if os.path.isdir(item_path):
            checkpoint_dirs.append(item)
    
    if not checkpoint_dirs:
        return None
    
    # ä¼˜å…ˆæŸ¥æ‰¾epoch4
    epoch4_pattern = re.compile(r'.*epoch4$')
    for checkpoint_dir in checkpoint_dirs:
        if epoch4_pattern.match(checkpoint_dir):
            best_checkpoint = os.path.join(checkpoints_dir, checkpoint_dir)
            logging.debug(f"æ‰¾åˆ°epoch4 checkpoint: {best_checkpoint}")
            return best_checkpoint
    
    # å¦‚æœæ²¡æœ‰epoch4ï¼Œæ‰¾æœ€é«˜çš„epoch
    epoch_pattern = re.compile(r'.*epoch(\d+)$')
    max_epoch = -1
    best_checkpoint_dir = None
    
    for checkpoint_dir in checkpoint_dirs:
        match = epoch_pattern.match(checkpoint_dir)
        if match:
            epoch_num = int(match.group(1))
            if epoch_num > max_epoch:
                max_epoch = epoch_num
                best_checkpoint_dir = checkpoint_dir
    
    if best_checkpoint_dir:
        best_checkpoint = os.path.join(checkpoints_dir, best_checkpoint_dir)
        logging.debug(f"æ‰¾åˆ°æœ€é«˜epoch checkpoint: {best_checkpoint} (epoch{max_epoch})")
        return best_checkpoint
    
    # å¦‚æœæ²¡æœ‰ç¬¦åˆæ¨¡å¼çš„ï¼Œè¿”å›ç¬¬ä¸€ä¸ª
    fallback_checkpoint = os.path.join(checkpoints_dir, checkpoint_dirs[0])
    logging.debug(f"ä½¿ç”¨å¤‡ç”¨checkpoint: {fallback_checkpoint}")
    return fallback_checkpoint

def copy_files_from_checkpoint(checkpoint_dir, target_dir, required_files):
    """
    ä»checkpointç›®å½•å¤åˆ¶æ–‡ä»¶åˆ°ç›®æ ‡ç›®å½•
    
    Args:
        checkpoint_dir: checkpointç›®å½•è·¯å¾„
        target_dir: ç›®æ ‡ç›®å½•è·¯å¾„
        required_files: éœ€è¦å¤åˆ¶çš„æ–‡ä»¶çŠ¶æ€å­—å…¸
        
    Returns:
        dict: å¤åˆ¶ç»“æœ
    """
    copied_files = []
    failed_files = []
    
    # é¦–å…ˆå¤åˆ¶å¿…éœ€æ–‡ä»¶
    for filename, exists in required_files.items():
        if not exists:  # åªå¤åˆ¶ç¼ºå¤±çš„æ–‡ä»¶
            src_file = os.path.join(checkpoint_dir, filename)
            dst_file = os.path.join(target_dir, filename)
            
            if os.path.exists(src_file):
                try:
                    shutil.copy2(src_file, dst_file)
                    copied_files.append(filename)
                    logging.debug(f"å¤åˆ¶æ–‡ä»¶: {src_file} -> {dst_file}")
                except Exception as e:
                    failed_files.append((filename, str(e)))
                    logging.error(f"å¤åˆ¶æ–‡ä»¶å¤±è´¥: {filename}, é”™è¯¯: {e}")
            else:
                failed_files.append((filename, "æºæ–‡ä»¶ä¸å­˜åœ¨"))
                logging.warning(f"checkpointä¸­ç¼ºå°‘æ–‡ä»¶: {src_file}")
    
    # å¤åˆ¶å¯é€‰æ–‡ä»¶
    optional_files = ['README.md', 'training_args.bin', 'trainer_state.json', 'pytorch_model.bin']
    for filename in optional_files:
        src_file = os.path.join(checkpoint_dir, filename)
        dst_file = os.path.join(target_dir, filename)
        
        if os.path.exists(src_file) and not os.path.exists(dst_file):
            try:
                shutil.copy2(src_file, dst_file)
                copied_files.append(filename)
                logging.debug(f"å¤åˆ¶å¯é€‰æ–‡ä»¶: {src_file} -> {dst_file}")
            except Exception as e:
                logging.warning(f"å¤åˆ¶å¯é€‰æ–‡ä»¶å¤±è´¥: {filename}, é”™è¯¯: {e}")
    
    return {
        'copied_files': copied_files,
        'failed_files': failed_files,
        'success': len(failed_files) == 0
    }

def fix_single_model(model_dir, dry_run=False):
    """
    ä¿®å¤å•ä¸ªæ¨¡å‹ç›®å½•
    
    Args:
        model_dir: æ¨¡å‹ç›®å½•è·¯å¾„
        dry_run: æ˜¯å¦ä¸ºå¹²è¿è¡Œæ¨¡å¼
        
    Returns:
        dict: ä¿®å¤ç»“æœ
    """
    model_name = os.path.basename(model_dir)
    logging.info(f"æ£€æŸ¥æ¨¡å‹: {model_name}")
    
    # æ£€æŸ¥å¿…éœ€æ–‡ä»¶
    file_status = check_required_files(model_dir)
    
    if file_status['all_present']:
        logging.info(f"âœ“ æ¨¡å‹ {model_name} æ–‡ä»¶å®Œæ•´ï¼Œæ— éœ€ä¿®å¤")
        return {
            'model_dir': model_dir,
            'status': 'complete',
            'message': 'æ–‡ä»¶å®Œæ•´',
            'fixed': False
        }
    
    logging.info(f"âš  æ¨¡å‹ {model_name} ç¼ºå°‘ {file_status['missing_count']} ä¸ªå¿…éœ€æ–‡ä»¶")
    
    # æŸ¥æ‰¾æœ€ä½³checkpoint
    checkpoints_dir = os.path.join(model_dir, "checkpoints")
    best_checkpoint = find_best_checkpoint(checkpoints_dir)
    
    if not best_checkpoint:
        logging.error(f"âœ— æ¨¡å‹ {model_name} æ²¡æœ‰å¯ç”¨çš„checkpoint")
        return {
            'model_dir': model_dir,
            'status': 'error',
            'message': 'æ²¡æœ‰å¯ç”¨çš„checkpoint',
            'fixed': False
        }
    
    checkpoint_name = os.path.basename(best_checkpoint)
    logging.info(f"ä½¿ç”¨checkpoint: {checkpoint_name}")
    
    if dry_run:
        logging.info(f"[DRY RUN] å°†ä» {checkpoint_name} æ¢å¤æ–‡ä»¶åˆ° {model_name}")
        return {
            'model_dir': model_dir,
            'status': 'dry_run',
            'message': f'å°†ä» {checkpoint_name} æ¢å¤æ–‡ä»¶',
            'fixed': False
        }
    
    # æ‰§è¡Œæ–‡ä»¶å¤åˆ¶
    copy_result = copy_files_from_checkpoint(
        best_checkpoint, 
        model_dir, 
        file_status['required_files']
    )
    
    if copy_result['success']:
        logging.info(f"âœ“ æ¨¡å‹ {model_name} ä¿®å¤æˆåŠŸï¼Œå¤åˆ¶äº† {len(copy_result['copied_files'])} ä¸ªæ–‡ä»¶")
        return {
            'model_dir': model_dir,
            'status': 'fixed',
            'message': f'ä» {checkpoint_name} æ¢å¤äº† {len(copy_result["copied_files"])} ä¸ªæ–‡ä»¶',
            'copied_files': copy_result['copied_files'],
            'fixed': True
        }
    else:
        logging.error(f"âœ— æ¨¡å‹ {model_name} ä¿®å¤å¤±è´¥")
        return {
            'model_dir': model_dir,
            'status': 'failed',
            'message': f'å¤åˆ¶å¤±è´¥: {copy_result["failed_files"]}',
            'fixed': False
        }

def generate_report(results, output_file=None):
    """
    ç”Ÿæˆä¿®å¤æŠ¥å‘Š
    
    Args:
        results: ä¿®å¤ç»“æœåˆ—è¡¨
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    total_models = len(results)
    complete_models = sum(1 for r in results if r['status'] == 'complete')
    fixed_models = sum(1 for r in results if r['status'] == 'fixed')
    failed_models = sum(1 for r in results if r['status'] == 'failed')
    error_models = sum(1 for r in results if r['status'] == 'error')
    
    report = {
        'timestamp': datetime.now().isoformat(),
        'summary': {
            'total_models': total_models,
            'complete_models': complete_models,
            'fixed_models': fixed_models,
            'failed_models': failed_models,
            'error_models': error_models,
            'success_rate': f"{(complete_models + fixed_models) / total_models * 100:.1f}%" if total_models > 0 else "0.0%"
        },
        'detailed_results': results
    }
    
    # æ‰“å°æ‘˜è¦
    print("\n" + "=" * 60)
    print("LoRAé€‚é…å™¨æ¨¡å‹ä¿®å¤æŠ¥å‘Š")
    print("=" * 60)
    print(f"æ€»æ¨¡å‹æ•°: {total_models}")
    print(f"å®Œæ•´æ¨¡å‹: {complete_models}")
    print(f"æˆåŠŸä¿®å¤: {fixed_models}")
    print(f"ä¿®å¤å¤±è´¥: {failed_models}")
    print(f"é”™è¯¯æ¨¡å‹: {error_models}")
    print(f"æˆåŠŸç‡: {report['summary']['success_rate']}")
    
    if fixed_models > 0:
        print(f"\nğŸ‰ æˆåŠŸè·å–å¹¶æ¢å¤äº† {fixed_models} ä¸ªæ¨¡å‹çš„æ–‡ä»¶ï¼")
    
    if failed_models > 0 or error_models > 0:
        print(f"\nâš ï¸ æœ‰ {failed_models + error_models} ä¸ªæ¨¡å‹éœ€è¦æ‰‹åŠ¨å¤„ç†")
        print("\nå¤±è´¥çš„æ¨¡å‹:")
        for result in results:
            if result['status'] in ['failed', 'error']:
                print(f"  - {os.path.basename(result['model_dir'])}: {result['message']}")
    
    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    if output_file:
        try:
            import json
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            print(f"\nè¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        except Exception as e:
            logging.error(f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ä¿®å¤LoRAé€‚é…å™¨æ¨¡å‹æ–‡ä»¶")
    parser.add_argument("--base_path", type=str,
                       default="/datanfs2/chenrongyi/models/loraadaptors/",
                       help="LoRAæ¨¡å‹åŸºç¡€è·¯å¾„")
    parser.add_argument("--model_name", type=str,
                       default="Llama-3.1-8B",
                       help="æŒ‡å®šæ¨¡å‹åç§°ï¼Œåªå¤„ç†è¯¥æ¨¡å‹ä¸‹çš„é€‚é…å™¨")
    parser.add_argument("--package_filter", type=str,
                       default=None,
                       help="åŒ…åè¿‡æ»¤å™¨ï¼ˆå¯é€‰ï¼‰ï¼Œåªå¤„ç†åŒ¹é…çš„åŒ…")
    parser.add_argument("--dry_run", action="store_true",
                       help="å¹²è¿è¡Œæ¨¡å¼ï¼Œåªæ£€æŸ¥ä¸æ‰§è¡Œä¿®å¤")
    parser.add_argument("--output_report", type=str,
                       default=None,
                       help="è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--verbose", action="store_true",
                       help="æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—")
    
    args = parser.parse_args()
    
    setup_logging(args.verbose)
    
    if args.dry_run:
        logging.info("è¿è¡Œåœ¨å¹²è¿è¡Œæ¨¡å¼ï¼Œå°†åªæ£€æŸ¥ä¸æ‰§è¡Œä¿®å¤")
    
    # æ„å»ºæœç´¢è·¯å¾„
    if args.model_name:
        search_path = os.path.join(args.base_path, args.model_name)
    else:
        search_path = args.base_path
    
    logging.info(f"å¼€å§‹æ‰«æLoRAæ¨¡å‹ç›®å½•: {search_path}")
    
    # æŸ¥æ‰¾æ‰€æœ‰æ¨¡å‹ç›®å½•
    model_dirs = find_lora_model_directories(args.base_path)
    
    # è¿‡æ»¤æŒ‡å®šæ¨¡å‹
    if args.model_name:
        model_dirs = [d for d in model_dirs if args.model_name in d]
    
    # è¿‡æ»¤æŒ‡å®šåŒ…
    if args.package_filter:
        model_dirs = [d for d in model_dirs if args.package_filter in d]
    
    if not model_dirs:
        logging.error("æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„LoRAæ¨¡å‹ç›®å½•")
        return
    
    logging.info(f"å°†å¤„ç† {len(model_dirs)} ä¸ªæ¨¡å‹ç›®å½•")
    
    # å¤„ç†æ¯ä¸ªæ¨¡å‹
    results = []
    for i, model_dir in enumerate(model_dirs, 1):
        logging.info(f"\n[{i}/{len(model_dirs)}] å¤„ç†: {os.path.basename(model_dir)}")
        
        try:
            result = fix_single_model(model_dir, args.dry_run)
            results.append(result)
        except Exception as e:
            logging.error(f"å¤„ç†æ¨¡å‹æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            results.append({
                'model_dir': model_dir,
                'status': 'error',
                'message': f'å¤„ç†å¼‚å¸¸: {str(e)}',
                'fixed': False
            })
    
    # ç”ŸæˆæŠ¥å‘Š
    if args.output_report:
        report_file = args.output_report
    else:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"lora_fix_report_{timestamp}.json"
    
    generate_report(results, report_file)

if __name__ == "__main__":
    main() 