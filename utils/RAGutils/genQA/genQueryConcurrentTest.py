import json
import sys
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from threading import Lock

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../../..'))
from utils.callapi import _qdd_api_inference
from config.apikey import QDD_APIKEY

# çº¿ç¨‹é”ï¼Œç”¨äºå®‰å…¨å†™å…¥æ–‡ä»¶
write_lock = Lock()

with open("data/VersiBCB_Benchmark/vscc_datas.json","r") as f:
    vscc_datas = json.load(f)

def getIDcode(datas, id):
    for data in datas:
        if data["id"] == id:
            return data
    return None

def buildQueryOnly(codeinfo, error_infos):
    '''
    ä»…æ ¹æ®error_infosç”Ÿæˆqueryï¼Œä¸ç”Ÿæˆfix
    '''
    code = codeinfo["code"]
    dependency = codeinfo["dependency"]
    
    # æ ¼å¼åŒ–error_infosä¸ºå­—ç¬¦ä¸²
    error_infos_str = ""
    for error_info in error_infos:
        error_infos_str += f"Error ID: {error_info.get('error_id', 'N/A')}\n"
        error_infos_str += f"Tool: {error_info.get('tool', 'N/A')}\n"
        error_infos_str += f"Rule: {error_info.get('rule', 'N/A')}\n"
        error_infos_str += f"Error: {error_info.get('error_info', 'N/A')}\n\n"
    
    prompt = f"""
You are a helpful assistant that generates queries for code errors that need more information or clarification.

Dependency:
{dependency}

Here is the code:
{code}

Here is the error infos generated by static analysis for the code:
{error_infos_str}

Your task is to analyze these errors and generate queries for situations where you need more information to provide a proper fix. You should generate queries for:
1. Errors related to external APIs or libraries where the correct usage might have changed
2. Errors where the intended behavior is unclear
3. Errors that might be false positives from static analysis tools
4. Complex errors that require domain knowledge

For each query, provide:
* **`error_ids`**: An array of strings, listing the IDs of the errors addressed by this query.
* **`target_api_path`**: The API path that the error is related to (e.g., "sklearn.feature_extraction.text.TfidfVectorizer.get_feature_names_out").
* **`query_content`**: A clear question asking for more information to resolve the error.
* **`explanation`**: Brief explanation of why this query is needed.

## Examples:
- target_api_path: "sklearn.feature_extraction.text.TfidfVectorizer"
- query_content: "What is the correct method to get feature names from TfidfVectorizer in the current version?"
- explanation: "The error indicates get_feature_names_out is unknown, but this might be a version issue."

## Notes:
1. Focus on errors that genuinely require external information or clarification
2. Avoid generating queries for simple syntax errors or obvious missing imports
3. Consider that some static analysis errors might be false positives
4. Group related errors into single queries when appropriate

The final output should be a JSON object with queries array:
{{
    "queries": [{{
        "error_ids": ["error_0001", "error_0002"],
        "target_api_path": "matplotlib.pyplot.subplots",
        "query_content": "What is the correct way to use plt.subplots with ax parameter in seaborn functions?",
        "explanation": "The error suggests ax parameter issue with seaborn.pairplot, need to clarify correct usage pattern."
    }}]
}}

Please respond with ONLY the JSON object, no additional text.
"""

    try:
        # è°ƒç”¨QDD APIè¿›è¡Œæ¨ç†
        response = _qdd_api_inference(
            prompt=prompt,
            max_new_tokens=1024,
            temperature=0.7,
            top_p=0.95,
            api_key=QDD_APIKEY,
            model_name="@cf/meta/llama-3.1-8b-instruct",
            api_base_url=None
        )
        
        # å°è¯•è§£æJSONå“åº”
        try:
            result = json.loads(response)
            return result
        except json.JSONDecodeError:
            # å¦‚æœç›´æ¥è§£æå¤±è´¥ï¼Œå°è¯•æå–JSONéƒ¨åˆ†
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                return result
            else:
                return {"error": f"æ— æ³•è§£æAPIå“åº”: {response[:100]}..."}
                
    except Exception as e:
        return {"error": f"è°ƒç”¨APIæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"}

def process_single_id(data_item, output_file):
    """
    å¤„ç†å•ä¸ªIDçš„å‡½æ•°ï¼Œç”¨äºå¹¶å‘æ‰§è¡Œ
    """
    id = data_item["id"]
    start_time = time.time()
    
    print(f"ğŸš€ å¼€å§‹å¤„ç† ID {id}")
    
    # åˆå§‹åŒ–ç»“æœå¯¹è±¡
    result = {
        "id": id,
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "status": "processing"
    }
    
    try:
        # è·å–ä»£ç ä¿¡æ¯
        codeinfo = getIDcode(vscc_datas, id)
        if codeinfo is None:
            result.update({
                "status": "error",
                "error": f"æœªæ‰¾åˆ°ID {id} å¯¹åº”çš„ä»£ç ä¿¡æ¯",
                "processing_time": time.time() - start_time
            })
            write_result_to_jsonl(result, output_file)
            return result
            
        # è·å–é”™è¯¯ä¿¡æ¯
        error_infos = data_item.get("error_infos", [])
        if not error_infos:
            result.update({
                "status": "error", 
                "error": f"æœªæ‰¾åˆ°ID {id} å¯¹åº”çš„é”™è¯¯ä¿¡æ¯",
                "processing_time": time.time() - start_time
            })
            write_result_to_jsonl(result, output_file)
            return result
        
        print(f"ğŸ“‹ ID {id}: å‘ç° {len(error_infos)} ä¸ªé”™è¯¯ï¼Œå¼€å§‹ç”ŸæˆæŸ¥è¯¢...")
            
        # ç”ŸæˆæŸ¥è¯¢
        query_info = buildQueryOnly(codeinfo, error_infos)
        
        if query_info and "queries" in query_info and query_info["queries"]:
            # ä¸ºæ¯ä¸ªqueryæ·»åŠ åŸå§‹IDä¿¡æ¯
            for query in query_info["queries"]:
                query["original_id"] = id
                
            result.update({
                "status": "success",
                "queries": query_info["queries"],
                "query_count": len(query_info["queries"]),
                "error_count": len(error_infos),
                "processing_time": time.time() - start_time
            })
            print(f"âœ… ID {id}: æˆåŠŸç”Ÿæˆ {len(query_info['queries'])} ä¸ªæŸ¥è¯¢")
        elif query_info and "error" in query_info:
            result.update({
                "status": "api_error",
                "error": query_info["error"],
                "processing_time": time.time() - start_time
            })
            print(f"âŒ ID {id}: APIé”™è¯¯ - {query_info['error']}")
        else:
            result.update({
                "status": "no_queries",
                "message": "æœªç”ŸæˆæŸ¥è¯¢ï¼ˆå¯èƒ½æ˜¯ç®€å•é”™è¯¯æˆ–é™æ€åˆ†æè¯¯æŠ¥ï¼‰",
                "error_count": len(error_infos),
                "processing_time": time.time() - start_time
            })
            print(f"â„¹ï¸ ID {id}: æœªç”ŸæˆæŸ¥è¯¢")
            
    except Exception as e:
        result.update({
            "status": "exception",
            "error": str(e),
            "processing_time": time.time() - start_time
        })
        print(f"ğŸ’¥ ID {id}: å¤„ç†å¼‚å¸¸ - {str(e)}")
    
    # å†™å…¥ç»“æœ
    write_result_to_jsonl(result, output_file)
    return result

def write_result_to_jsonl(result, output_file):
    """
    çº¿ç¨‹å®‰å…¨åœ°å°†ç»“æœå†™å…¥JSONLæ–‡ä»¶
    """
    with write_lock:
        with open(output_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(result, ensure_ascii=False) + '\n')

def run_test_concurrent_processing(test_data, max_workers=2, output_file="data/temp/query_results_test.jsonl"):
    """
    å¹¶å‘å¤„ç†æµ‹è¯•æ•°æ®
    """
    # æ¸…ç©ºè¾“å‡ºæ–‡ä»¶
    with open(output_file, 'w', encoding='utf-8') as f:
        pass
    
    print(f"ğŸ”§ å¼€å§‹æµ‹è¯•å¹¶å‘å¤„ç† {len(test_data)} ä¸ªæ•°æ®é¡¹ï¼Œæœ€å¤§å¹¶å‘æ•°: {max_workers}")
    print(f"ğŸ“ ç»“æœå°†ä¿å­˜åˆ°: {output_file}")
    print("=" * 60)
    
    start_time = time.time()
    successful_count = 0
    error_count = 0
    
    # ä½¿ç”¨ThreadPoolExecutorè¿›è¡Œå¹¶å‘å¤„ç†
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_id = {
            executor.submit(process_single_id, data_item, output_file): data_item["id"] 
            for data_item in test_data
        }
        
        # å¤„ç†å®Œæˆçš„ä»»åŠ¡
        for i, future in enumerate(as_completed(future_to_id)):
            id = future_to_id[future]
            try:
                result = future.result()
                if result["status"] == "success":
                    successful_count += 1
                elif result["status"] == "no_queries":
                    successful_count += 1
                else:
                    error_count += 1
                    
            except Exception as exc:
                error_count += 1
                print(f"ğŸ’¥ ID {id}: å¤„ç†å¼‚å¸¸ - {exc}")
    
    total_time = time.time() - start_time
    
    print("=" * 60)
    print(f"ğŸ æµ‹è¯•å®Œæˆ!")
    print(f"â±ï¸  æ€»å¤„ç†æ—¶é—´: {total_time:.2f}s")
    print(f"âœ… æˆåŠŸå¤„ç†: {successful_count}")
    print(f"âŒ å¤„ç†å¤±è´¥: {error_count}")
    print(f"âš¡ å¹³å‡å¤„ç†æ—¶é—´: {total_time/len(test_data):.2f}s")
    print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

def show_results(output_file):
    """
    æ˜¾ç¤ºå¤„ç†ç»“æœ
    """
    print(f"\nğŸ“Š ç»“æœè¯¦æƒ…:")
    try:
        with open(output_file, 'r', encoding='utf-8') as f:
            results = [json.loads(line) for line in f if line.strip()]
        
        for result in results:
            print(f"\nID {result['id']}:")
            print(f"  çŠ¶æ€: {result['status']}")
            print(f"  å¤„ç†æ—¶é—´: {result.get('processing_time', 0):.2f}s")
            
            if result.get('queries'):
                print(f"  ç”ŸæˆæŸ¥è¯¢æ•°: {len(result['queries'])}")
                for i, query in enumerate(result['queries']):
                    print(f"    æŸ¥è¯¢ {i+1}:")
                    print(f"      é”™è¯¯ID: {query.get('error_ids', [])}")
                    print(f"      APIè·¯å¾„: {query.get('target_api_path', 'N/A')}")
                    print(f"      æŸ¥è¯¢å†…å®¹: {query.get('query_content', 'N/A')[:100]}...")
            elif result.get('error'):
                print(f"  é”™è¯¯: {result['error']}")
        
    except Exception as e:
        print(f"æ˜¾ç¤ºç»“æœæ—¶å‡ºé”™: {e}")

if __name__ == "__main__":
    # åŠ è½½æ•°æ®
    with open("data/temp/combined_errors_vscc_with_ids.json","r") as f:
        combined_errors_vscc = json.load(f)
    
    # æµ‹è¯•å‰5ä¸ªæ•°æ®é¡¹
    test_count = 5
    test_data = combined_errors_vscc[:test_count]
    
    # è¿è¡Œæµ‹è¯•
    output_file = "data/temp/query_results_test.jsonl"
    max_workers = 2  # è¾ƒå°çš„å¹¶å‘æ•°ç”¨äºæµ‹è¯•
    
    run_test_concurrent_processing(test_data, max_workers, output_file)
    
    # æ˜¾ç¤ºç»“æœ
    show_results(output_file) 