#!/usr/bin/env python3
"""
æµ‹è¯•LoRAè®­ç»ƒè®¾å¤‡æ˜ å°„åŠŸèƒ½
"""
import sys
import os
import torch
import argparse
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from utils.loraTrain.loraTrainUtils import (
    create_balanced_device_map,
    create_dynamic_device_map,
    analyze_device_balance,
    create_layer_balanced_mapping
)

def test_balanced_device_map():
    """æµ‹è¯•å‡è¡¡è®¾å¤‡æ˜ å°„"""
    print("ğŸ§ª æµ‹è¯•å‡è¡¡è®¾å¤‡æ˜ å°„...")
    
    # ä½¿ç”¨ä¸€ä¸ªå·²çŸ¥çš„æ¨¡å‹è·¯å¾„è¿›è¡Œæµ‹è¯•
    model_path = "/datanfs2/chenrongyi/models/Llama-3.1-8B"
    
    try:
        device_map = create_balanced_device_map(
            model_path,
            force_balance=True,
            exclude_cpu=True
        )
        
        if device_map == "auto":
            print("âœ… å›é€€åˆ°autoç­–ç•¥")
        elif device_map == "cpu":
            print("âœ… ä½¿ç”¨CPUç­–ç•¥")
        elif isinstance(device_map, dict):
            print(f"âœ… åˆ›å»ºå‡è¡¡è®¾å¤‡æ˜ å°„æˆåŠŸ: {len(device_map)} ä¸ªç»„ä»¶")
            
            # åˆ†æå‡è¡¡æ€§
            balance_info = analyze_device_balance(device_map)
            print(f"   ä¸å‡è¡¡ç³»æ•°: {balance_info['imbalance_ratio']:.3f}")
            print(f"   è®¾å¤‡åˆ†å¸ƒ: {balance_info['device_distribution']}")
        else:
            print("âŒ æœªçŸ¥çš„è®¾å¤‡æ˜ å°„ç±»å‹")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def test_dynamic_device_map():
    """æµ‹è¯•åŠ¨æ€è®¾å¤‡æ˜ å°„"""
    print("\nğŸ§ª æµ‹è¯•åŠ¨æ€è®¾å¤‡æ˜ å°„...")
    
    # ä½¿ç”¨ä¸€ä¸ªå·²çŸ¥çš„æ¨¡å‹è·¯å¾„è¿›è¡Œæµ‹è¯•
    model_path = "/datanfs2/chenrongyi/models/Llama-3.1-8B"
    
    try:
        model, tokenizer, device_map_info = create_dynamic_device_map(
            model_path,
            balance_threshold=0.3,
            force_balance=True,
            exclude_cpu=True
        )
        
        if model is None:
            print("âŒ åŠ¨æ€è®¾å¤‡æ˜ å°„å¤±è´¥")
            print(f"   ç­–ç•¥: {device_map_info['strategy']}")
            print(f"   åŸå› : {device_map_info['reason']}")
        else:
            print(f"âœ… åŠ¨æ€è®¾å¤‡æ˜ å°„æˆåŠŸ")
            print(f"   ç­–ç•¥: {device_map_info['strategy']}")
            print(f"   åŸå› : {device_map_info['reason']}")
            
            # æ£€æŸ¥æ¨¡å‹è®¾å¤‡æ˜ å°„
            if hasattr(model, 'hf_device_map'):
                balance_info = analyze_device_balance(model.hf_device_map)
                print(f"   ä¸å‡è¡¡ç³»æ•°: {balance_info['imbalance_ratio']:.3f}")
                print(f"   è®¾å¤‡åˆ†å¸ƒ: {balance_info['device_distribution']}")
            
            # æ¸…ç†å†…å­˜
            del model, tokenizer
            torch.cuda.empty_cache()
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def test_analyze_device_balance():
    """æµ‹è¯•è®¾å¤‡å‡è¡¡æ€§åˆ†æ"""
    print("\nğŸ§ª æµ‹è¯•è®¾å¤‡å‡è¡¡æ€§åˆ†æ...")
    
    # æ¨¡æ‹Ÿautoç­–ç•¥çš„ä¸å‡è¡¡åˆ†é…
    auto_device_map = {
        "model.embed_tokens": 0,
        "model.layers.0": 0, "model.layers.1": 0, "model.layers.2": 0,
        "model.layers.3": 0, "model.layers.4": 0, "model.layers.5": 0,
        "model.layers.6": 0, "model.layers.7": 0,
        "model.layers.8": 1, "model.layers.9": 1, "model.layers.10": 1,
        "model.layers.11": 1, "model.layers.12": 1, "model.layers.13": 1,
        "model.layers.14": 1, "model.layers.15": 1, "model.layers.16": 1,
        "model.layers.17": 1, "model.layers.18": 1, "model.layers.19": 1,
        "model.layers.20": 1,
        "model.layers.21": 2, "model.layers.22": 2, "model.layers.23": 2,
        "model.layers.24": 2, "model.layers.25": 2, "model.layers.26": 2,
        "model.layers.27": 2, "model.layers.28": 2, "model.layers.29": 2,
        "model.layers.30": 2, "model.layers.31": 2,
        "model.norm": 2, "lm_head": 2,
    }
    
    print("ğŸ“Š åˆ†æAutoç­–ç•¥åˆ†é…:")
    balance_info = analyze_device_balance(auto_device_map)
    print(f"   ä¸å‡è¡¡ç³»æ•°: {balance_info['imbalance_ratio']:.3f}")
    print(f"   è®¾å¤‡åˆ†å¸ƒ: {balance_info['device_distribution']}")
    print(f"   å±‚åˆ†å¸ƒ: {balance_info['layer_distribution']}")
    
    # æ¨¡æ‹Ÿbalancedç­–ç•¥çš„å‡è¡¡åˆ†é…
    balanced_device_map = {
        "model.embed_tokens": 0,
        "model.layers.0": 0, "model.layers.1": 0, "model.layers.2": 0,
        "model.layers.3": 0, "model.layers.4": 0, "model.layers.5": 0,
        "model.layers.6": 0, "model.layers.7": 0, "model.layers.8": 0,
        "model.layers.9": 0, "model.layers.10": 0,
        "model.layers.11": 1, "model.layers.12": 1, "model.layers.13": 1,
        "model.layers.14": 1, "model.layers.15": 1, "model.layers.16": 1,
        "model.layers.17": 1, "model.layers.18": 1, "model.layers.19": 1,
        "model.layers.20": 1, "model.layers.21": 1,
        "model.layers.22": 2, "model.layers.23": 2, "model.layers.24": 2,
        "model.layers.25": 2, "model.layers.26": 2, "model.layers.27": 2,
        "model.layers.28": 2, "model.layers.29": 2, "model.layers.30": 2,
        "model.layers.31": 2,
        "model.norm": 2, "lm_head": 2,
    }
    
    print("\nğŸ“Š åˆ†æBalancedç­–ç•¥åˆ†é…:")
    balance_info = analyze_device_balance(balanced_device_map)
    print(f"   ä¸å‡è¡¡ç³»æ•°: {balance_info['imbalance_ratio']:.3f}")
    print(f"   è®¾å¤‡åˆ†å¸ƒ: {balance_info['device_distribution']}")
    print(f"   å±‚åˆ†å¸ƒ: {balance_info['layer_distribution']}")

def test_layer_balanced_mapping():
    """æµ‹è¯•å±‚çº§å‡è¡¡æ˜ å°„"""
    print("\nğŸ§ª æµ‹è¯•å±‚çº§å‡è¡¡æ˜ å°„...")
    
    # æ¨¡æ‹ŸGPUä¿¡æ¯
    available_gpus = [
        {"device_id": 0, "free_memory_gb": 20.0},
        {"device_id": 1, "free_memory_gb": 20.0},
        {"device_id": 2, "free_memory_gb": 20.0},
    ]
    
    # æ¨¡æ‹Ÿæ¨¡å‹é…ç½®
    class MockConfig:
        def __init__(self):
            self.architectures = ["LlamaForCausalLM"]
    
    model_config = MockConfig()
    
    try:
        device_map = create_layer_balanced_mapping(32, available_gpus, model_config)
        
        if device_map:
            print(f"âœ… å±‚çº§å‡è¡¡æ˜ å°„åˆ›å»ºæˆåŠŸ: {len(device_map)} ä¸ªç»„ä»¶")
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«é¢„æœŸçš„ç»„ä»¶
            expected_components = ["model.embed_tokens", "lm_head", "model.norm"]
            for component in expected_components:
                if component in device_map:
                    print(f"   âœ“ æ‰¾åˆ°ç»„ä»¶: {component} -> GPU {device_map[component]}")
                else:
                    print(f"   âœ— ç¼ºå°‘ç»„ä»¶: {component}")
            
            # åˆ†æå‡è¡¡æ€§
            balance_info = analyze_device_balance(device_map)
            print(f"   ä¸å‡è¡¡ç³»æ•°: {balance_info['imbalance_ratio']:.3f}")
            print(f"   è®¾å¤‡åˆ†å¸ƒ: {balance_info['device_distribution']}")
        else:
            print("âŒ å±‚çº§å‡è¡¡æ˜ å°„åˆ›å»ºå¤±è´¥")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def test_gpu_environment():
    """æµ‹è¯•GPUç¯å¢ƒ"""
    print("\nğŸ§ª æµ‹è¯•GPUç¯å¢ƒ...")
    
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        num_gpus = torch.cuda.device_count()
        print(f"GPUæ•°é‡: {num_gpus}")
        
        for i in range(num_gpus):
            try:
                props = torch.cuda.get_device_properties(i)
                total_memory = props.total_memory / (1024**3)
                allocated_memory = torch.cuda.memory_allocated(i) / (1024**3)
                free_memory = total_memory - allocated_memory
                
                print(f"GPU {i} ({props.name}):")
                print(f"   æ€»å†…å­˜: {total_memory:.1f}GB")
                print(f"   å·²ç”¨å†…å­˜: {allocated_memory:.1f}GB")
                print(f"   å¯ç”¨å†…å­˜: {free_memory:.1f}GB")
            except Exception as e:
                print(f"GPU {i}: æ— æ³•è·å–ä¿¡æ¯ - {e}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æµ‹è¯•LoRAè®­ç»ƒè®¾å¤‡æ˜ å°„åŠŸèƒ½")
    parser.add_argument("--test", type=str, default="all", 
                       choices=["all", "balanced", "dynamic", "analyze", "layer", "gpu"],
                       help="é€‰æ‹©è¦è¿è¡Œçš„æµ‹è¯•")
    parser.add_argument("--model_path", type=str, 
                       default="/datanfs2/chenrongyi/models/Llama-3.1-8B",
                       help="æµ‹è¯•ç”¨çš„æ¨¡å‹è·¯å¾„")
    
    args = parser.parse_args()
    
    print("=" * 60)
    print(f"LoRAè®­ç»ƒè®¾å¤‡æ˜ å°„åŠŸèƒ½æµ‹è¯• - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    try:
        if args.test in ["all", "gpu"]:
            test_gpu_environment()
        
        if args.test in ["all", "analyze"]:
            test_analyze_device_balance()
        
        if args.test in ["all", "layer"]:
            test_layer_balanced_mapping()
        
        if args.test in ["all", "balanced"]:
            test_balanced_device_map()
        
        if args.test in ["all", "dynamic"]:
            test_dynamic_device_map()
        
        print("\n" + "=" * 60)
        print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
        print("=" * 60)
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 