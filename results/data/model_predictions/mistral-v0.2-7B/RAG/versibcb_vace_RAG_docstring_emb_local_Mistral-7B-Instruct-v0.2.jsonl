{"id": 0, "answer": " <start>\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(list_of_pairs):\n    df = pd.DataFrame(list_of_pairs, columns=[\"Category\", \"Value\"])\n    ax = sns.barplot(x=\"Category\", y=\"Value\", data=df)\n    ax.set_title(\"Category vs Value\")\n    return df, ax\n<end>"}
{"id": 1, "answer": " <start>\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(list_of_pairs):\n    df = pd.DataFrame(list_of_pairs, columns=[\"Category\", \"Value\"])\n\n    # Use itertuples instead of DataFrame.iterrows for better performance\n    # with large number of columns (>254)\n    ax = df.itertuples(index=False, name=None)\n\n    plt.figure(figsize=(10, 5))\n    sns.barplot(x=\"Category\", y=\"Value\", data=df)\n    plt.title(\"Category vs Value\")\n\n    return df, ax\n<end>"}
{"id": 2, "answer": " <start>\nimport subprocess\nimport psutil\nimport time\n\ndef task_func(process_name: str) -> str:\n    # Get the process by name\n    process = psutil.Process(psutil.find_process_by_name(process_name))\n\n    # If the process is running, terminate it\n    if process.is_running():\n        process.terminate()\n        time.sleep(5)\n        # Start the process again\n        subprocess.Popen(process_name)\n        return \"Process found. Restarting {}.\".format(process_name)\n    else:\n        # Start the process\n        subprocess.Popen(process_name)\n        return \"Process not found. Starting {}.\".format(process_name)\n<end>"}
{"id": 3, "answer": " <start>\nimport subprocess\nimport psutil\nimport time\n\ndef task_func(process_name: str) -> str:\n    # Check if the process is running\n    processes = psutil.process_iter()\n    is_running = any([proc.name() == process_name for proc in processes])\n\n    # If the process is running, terminate it\n    if is_running:\n        for proc in psutil.process_iter():\n            if proc.name() == process_name:\n                proc.terminate()\n                time.sleep(5)\n        psutil.Popen(psutil.Popen(process_name).as_dict())\n        return f\"Process found. Restarting {process_name}.\"\n    else:\n        psutil.Popen(psutil.Popen(process_name).as_dict())\n        return f\"Process not found. Starting {process_name}.\"\n<end>"}
{"id": 4, "answer": " <start>\nimport platform\nfrom psutil import virtual_memory, Process\n\ndef task_func():\n    system_info = {}\n\n    # \u83b7\u53d6\u64cd\u4f5c\u7cfb\u7edf\u540d\u79f0\n    system_info['OS'] = platform.system()\n\n    # \u83b7\u53d6\u7cfb\u7edf\u67b6\u6784\n    system_info['Architecture'] = platform.architecture()[0]\n\n    # \u83b7\u53d6\u5185\u5b58\u4fe1\u606f\uff08psutil 5.9.5 \u4f7f\u7528 virtual_memory\uff09\n    memory_info = virtual_memory()\n    total_memory = memory_info.total\n    used_memory = memory_info.used\n\n    # \u8ba1\u7b97\u5185\u5b58\u4f7f\u7528\u767e\u5206\u6bd4\n    system_info['Memory Usage'] = f'{used_memory / total_memory * 100:.2f}%'\n\n    # \u83b7\u53d6\u5f53\u524d\u8fdb\u7a0b\u7684\u5185\u5b58\u4f7f\u7528\u60c5\u51b5\uff08\u53ef\u9009\uff09\n    current_process = Process(os.getpid())\n    current_process_memory_info = current_process.memory_info()\n    current_process_memory_usage = current_process_memory_info.rss / total_memory * 100\n    system_info['Current Process Memory Usage'] = f'{current_process_memory_usage:.2f}%'\n\n    return system_info\n<end>"}
{"id": 5, "answer": " <start>\nimport psutil\nimport platform\n\ndef get_system_info():\n    system_info = {}\n\n    system_info['OS'] = platform.system()\n    system_info['Architecture'] = platform.architecture()[0]\n\n    system_info['Memory Usage'] = psutil.Process(os.getpid()).memory_info().percent\n\n    return system_info\n<end>"}
{"id": 6, "answer": " <start>\nimport nltk\nfrom string import punctuation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    words = nltk.word_tokenize(text)\n    dollar_words = [\n        word\n        for word in words\n        if word.startswith(\"$\")\n        and not any(c in PUNCTUATION for c in word)\n        and len(word) > 1\n    ]\n    freq = nltk.FreqDist(dollar_words)\n    if not freq:\n        return None\n\n    fig, ax = plt.subplots(figsize=(10, 5))\n    sns.barplot(x=freq.keys(), y=freq.values(), ax=ax)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of words beginning with the \"$\" character')\n    ax.grid(True)\n\n    return ax\n<end>"}
{"id": 7, "answer": " <start>\nimport nltk\nfrom string import punctuation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    words = nltk.word_tokenize(text)\n    dollar_words = [\n        word\n        for word in words\n        if word.startswith(\"$\")\n        and not any(c in PUNCTUATION for c in word)\n        and len(word) > 1\n    ]\n    freq = nltk.FreqDist(dollar_words)\n    if not freq:\n        return None\n\n    fig, ax = plt.subplots(figsize=(10, 5))\n    sns.barplot(x=freq.keys(), y=freq.values(), ax=ax)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of words beginning with the \"$\" character')\n    ax.grid(True)\n\n    return ax\n<end>"}
{"id": 8, "answer": " <start>\nimport nltk\nfrom string import punctuation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    words = nltk.word_tokenize(text)\n    dollar_words = [\n        word\n        for word in words\n        if word.startswith(\"$\")\n        and not any(c in PUNCTUATION for c in word)\n        and len(word) > 1\n    ]\n    freq = nltk.FreqDist(dollar_words)\n    if not freq:\n        return None\n\n    fig, ax = plt.subplots(figsize=(10, 5))\n    sns.barplot(x=freq.keys(), y=freq.values(), ax=ax)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of words beginning with the \"$\" character')\n    ax.set_xticklabels(freq.keys(), rotation=45)\n    return ax\n<end>"}
{"id": 9, "answer": " <start>\nimport nltk\nfrom string import punctuation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    words = nltk.word_tokenize(text)\n    dollar_words = [\n        word\n        for word in words\n        if word.startswith(\"$\")\n        and not any(c in PUNCTUATION for c in word)\n        and len(word) > 1\n    ]\n    freq = nltk.FreqDist(dollar_words)\n    if not freq:\n        return None\n\n    fig, ax = plt.subplots(figsize=(10, 5))\n    sns.barplot(x=freq.keys(), y=freq.values(), ax=ax)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of Words Beginning with the \"$\" Character')\n    ax.set_xticklabels(freq.keys(), rotation=45)\n    return ax\n<end>"}
{"id": 10, "answer": " <start>\nimport nltk\nfrom string import punctuation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    words = nltk.word_tokenize(text)\n    dollar_words = [\n        word\n        for word in words\n        if word.startswith(\"$\")\n        and not any(c in PUNCTUATION for c in word)\n        and len(word) > 1\n    ]\n    freq = nltk.FreqDist(dollar_words)\n    if not freq:\n        return None\n\n    fig, ax = plt.subplots(figsize=(10, 5))\n    sns.barplot(x=list(freq.keys()), y=list(freq.values()), ax=ax)\n    ax.set_title(\"Frequency of words beginning with '$' character\")\n    ax.set_xlabel(\"Words\")\n    ax.set_ylabel(\"Frequency\")\n\n    return ax\n<end>"}
{"id": 11, "answer": " <start>\nimport nltk\nfrom string import punctuation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    words = nltk.word_tokenize(text)\n    dollar_words = [\n        word\n        for word in words\n        if word.startswith(\"$\")\n        and not any(c in PUNCTUATION for c in word)\n        and len(word) > 1\n    ]\n    freq = nltk.FreqDist(dollar_words)\n    if not freq:\n        return None\n\n    fig, ax = plt.subplots(figsize=(10, 5))\n    sns.barplot(x=list(freq.keys()), y=list(freq.values()), ax=ax)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of Words Beginning with \"$\" Character')\n    return ax\n<end>"}
{"id": 12, "answer": " <start>\nimport nltk\nfrom string import punctuation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    words = text.split()\n    dollar_words = [\n        word\n        for word in words\n        if word.startswith(\"$\")\n        and not any(c in PUNCTUATION for c in word)\n        and len(word) > 1\n    ]\n    freq = nltk.FreqDist(dollar_words)\n    if not freq:\n        return None\n\n    plt.figure(figsize=(10, 5))\n    sns.barplot(x=freq.keys(), y=freq.values())\n    return plt.gca()\n<end>"}
{"id": 13, "answer": " <start>\nimport nltk\nfrom string import punctuation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    words = text.split()\n    dollar_words = [\n        word\n        for word in words\n        if word.startswith(\"$\")\n        and not any(c in PUNCTUATION for c in word)\n        and len(word) > 1\n    ]\n    freq = nltk.FreqDist(dollar_words)\n    if not freq:\n        return None\n\n    fig, ax = plt.subplots(figsize=(10, 5))\n    sns.barplot(x=freq.keys(), y=freq.values(), ax=ax)\n    return ax\n<end>"}
{"id": 14, "answer": " <start>\nimport numpy as np\nfrom scipy.stats import mode, entropy\n\ndef task_func(numbers):\n    if not numbers:\n        raise ValueError\n\n    array = np.array(numbers)\n    mode_value = mode(array, keepdims=True).mode[0]\n    ent = entropy(array, base=2)\n\n    result_dict = {'mode': mode_value, 'entropy': ent}\n    return result_dict\n<end>"}
{"id": 15, "answer": " <start>\nimport numpy as np\nfrom scipy.stats import stats as scipy_stats\n\ndef task_func(numbers):\n    if len(numbers) == 0:\n        raise ValueError\n\n    array = np.array(numbers)\n    mode_value = scipy_stats.mode(array)[0]\n    ent = scipy_stats.entropy(array, base=2)\n\n    result_dict = {'mode': mode_value, 'entropy': ent}\n    result_dict['array'] = array\n\n    return result_dict\n<end>"}
{"id": 16, "answer": " <start>\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef task_func(url, tag):\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser', parse_only=SoupStrainer(tag))\n    tag_content = soup.find()\n\n    return tag_content.string if tag_content else None\n<end>"}
{"id": 17, "answer": " <start>\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef task_func(url, tag):\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    return soup.find(tag, recursive=True).text if soup.find(tag, recursive=True) else None\n</end>"}
{"id": 18, "answer": " <start>\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df, target_values=[1, 3, 4]):\n    df = df.apply(lambda x: x.replace(x.dropna() - target_values, 0))\n    plt.figure(figsize=(10, 5))\n    for column in df.columns:\n        sns.kdeplot(df[column], label=column, warn_singular=False)\n    plt.legend()\n    return df, plt.gca()\n<end>"}
{"id": 19, "answer": " <start>\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df, target_values=[1, 3, 4]):\n    df = df.mask(df.isin(target_values), other=0, axis=0)\n    plt.figure(figsize=(10, 5))\n    for column in df.columns:\n        if not df[column].eq(0).all():\n            sns.kdeplot(df[column], label=column)\n    plt.legend()\n    return df, plt.gca()\n<end>"}
{"id": 20, "answer": " <start>\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df, target_values=[1, 3, 4]):\n    # Replace elements not in target_values with 0\n    df = df.mask(df.isin(target_values).all(axis=1), other=0)\n\n    # Plot the distribution of each column\n    plt.figure(figsize=(10, 5))\n    for column in df.columns:\n        # Check if the column has non-zero values to avoid KDE error\n        if df[column].nunique() > 1:  # Ensure there is variation in the data\n            sns.kdeplot(df[column], label=column)\n        else:\n            # If all values are zero, plot a vertical line at zero\n            plt.axvline(0, label=column, linestyle='--', color='gray')\n\n    plt.legend()\n    return df, plt.gca()\n<end>"}
{"id": 21, "answer": " <start>\nimport numpy as np\nfrom scipy.stats import ttest_1samp\nimport matplotlib.pyplot as plt\n\n# Constants\nALPHA = 0.05\n\ndef task_func(data_matrix):\n    means = np.mean(data_matrix, axis=1)\n    population_mean = np.mean(data_matrix)\n\n    _, p_value = ttest_1samp(means, population_mean)\n    significant_indices = np.where(p_value < ALPHA)[0]\n\n    fig, ax = plt.subplots(figsize=(10, 5))\n    ax.plot(means, \"ro\", label=\"Means\")\n    ax.plot(\n        significant_indices, means[significant_indices], \"bo\", label=\"Significant Means\"\n    )\n    ax.axhline(y=population_mean, color=\"g\", linestyle=\"-\", label=\"Population Mean\")\n    ax.legend()\n    return significant_indices, ax\n<end>"}
{"id": 22, "answer": " <start>\nimport numpy as np\nfrom scipy.stats import ttest_1samp\nimport matplotlib.pyplot as plt\n\n# Constants\nALPHA = 0.05\n\ndef task_func(data_matrix):\n    means = np.mean(data_matrix, axis=1)\n    population_mean = np.mean(data_matrix)\n\n    _, p_value = ttest_1samp(means, population_mean)\n    significant_indices = np.where(p_value < ALPHA)[0]\n\n    fig, ax = plt.subplots(figsize=(10, 5))\n\n    ax.plot(means, \"ro\", label=\"Means\")\n    ax.plot(\n        significant_indices, means[significant_indices], \"bo\", label=\"Significant Means\"\n    )\n    ax.axhline(y=population_mean, color=\"g\", linestyle=\"-\", label=\"Population Mean\")\n    ax.legend()\n\n    return significant_indices.tolist(), ax\n<end>"}
{"id": 23, "answer": " <start>\nfrom datetime import datetime\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nDATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n\ndef task_func(timestamps):\n    if not timestamps:\n        raise ValueError(\"Input list of timestamps is empty.\")\n\n    # Convert Unix timestamps to datetime objects\n    datetimes = [datetime.utcfromtimestamp(t) for t in timestamps]\n\n    # Create DataFrame\n    df = pd.DataFrame({\"Timestamp\": timestamps, \"Datetime\": datetimes})\n\n    # Plot histogram\n    ax = plt.hist(df[\"Datetime\"], bins=10)\n\n    # Close the current figure\n    plt.close()\n\n    return df, ax\n<end>"}
{"id": 24, "answer": " <start>\n```python\nfrom datetime import datetime\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nDATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n\ndef task_func(timestamps):\n    if not timestamps:\n        raise ValueError(\"Input list of timestamps is empty.\")\n\n    df = pd.DataFrame({\"Timestamp\": timestamps, \"Datetime\": [datetime.fromtimestamp(t).strftime(DATE_FORMAT) for t in timestamps]})\n\n    df[\"Datetime\"] = pd.to_datetime(df[\"Datetime\"])\n\n    ax = df[\"Datetime\"].hist()\n\n    plt.close()\n\n    return df, ax\n```\n<end>"}
{"id": 25, "answer": " <start>\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(mu, sigma, num_samples):\n    rng = np.random.default_rng()\n    samples = rng.normal(mu, sigma, num_samples)\n\n    fig, ax = plt.subplots()\n    ax.hist(samples, bins=30, normed=True, alpha=0.6, color='g')\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, sigma)\n    ax.plot(x, p, 'k', linewidth=2)\n\n    ax.set_title('Normal Distribution')\n    plt.show()\n\n    return fig\n<end>"}
{"id": 26, "answer": " <start>\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(mu, sigma, num_samples):\n    samples = np.random.normal(mu, sigma, num_samples)\n    fig, ax = plt.subplots()\n\n    # Calculate and plot the probability density function (pdf) of the normal distribution\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, sigma)\n    ax.plot(x, p, 'k', linewidth=2)\n\n    # Plot the histogram of the samples\n    ax.hist(samples, bins=30, density=True, alpha=0.6, color='g')\n\n    ax.set_title('Normal Distribution')\n    plt.show()\n    return fig\n<end>"}
{"id": 27, "answer": " <start>\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(result, colors=['b', 'g', 'r', 'c', 'm', 'y', 'k']):\n    from_user_values = [d['from_user'] for d in result if 'from_user' in d]\n    color = random.choice(colors)\n    fig, ax = plt.subplots()\n    sns.histplot(from_user_values, color=color, kde=False, ax=ax)\n    plt.show()\n<end>"}
{"id": 28, "answer": " <start>\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(result, colors=['b', 'g', 'r', 'c', 'm', 'y', 'k']):\n    from_user_values = [d['from_user'] for d in result if 'from_user' in d]\n    color = random.choice(colors)\n\n    # Create a palette using seaborn's blend_palette function\n    palette = sns.blend_palette(color, n_colors=len(from_user_values))\n\n    plt.figure()\n    sns.histplot(from_user_values, color=palette)\n    plt.show()\n<end>"}
{"id": 29, "answer": " <start>\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.cbook import index_of\n\nCOLUMNS = ['col1', 'col2', 'col3']\n\ndef task_func(data):\n    df = pd.DataFrame(data, columns=COLUMNS)\n    analyzed_df = df.groupby(COLUMNS[:-1])[COLUMNS[-1]].nunique().reset_index()\n\n    # Adjusting the plotting logic\n    fig, ax = plt.subplots()\n    x_labels = index_of(analyzed_df[COLUMNS[:-1]], axis=0)\n    ax.plot(x_labels, analyzed_df[COLUMNS[-1]], marker='o')\n    ax.set_xticks(x_labels)\n    ax.set_xticklabels(analyzed_df[COLUMNS[:-1]].astype(str), rotation=45)\n    ax.set_xlabel('-'.join(COLUMNS[:-1]))\n    ax.set_ylabel(COLUMNS[-1])\n\n    return analyzed_df, ax\n<end>"}
{"id": 30, "answer": " <start>\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nCOLUMNS = ['col1', 'col2', 'col3']\n\ndef task_func(data):\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    # Refactored using the new version of pandas DataFrame.rename method\n    df = df.rename(columns={COLUMNS[-1]: 'value'})\n    analyzed_df = df.groupby(df.columns[:-1]).value.nunique().reset_index()\n\n    # Adjusting the plotting logic\n    fig, ax = plt.subplots()\n    ax.plot(analyzed_df[df.columns[:-1]].astype(str).agg('-'.join, axis=1), analyzed_df['value'])\n    ax.set_xlabel('-'.join(df.columns[:-1]))\n    ax.set_ylabel('value')\n\n    return analyzed_df, ax\n<end>"}
{"id": 31, "answer": " <start>\nimport random\nimport string\nfrom django.http import HttpResponse\nfrom django.utils import timezone\n\ndef task_func(request, session_expire_time):\n    # Generate a random session key with letters and digits\n    session_key = ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(20))\n\n    # Check if the session key contains both letters and digits\n    has_digit = any(char.isdigit() for char in session_key)\n    has_letter = any(char.isalpha() for char in session_key)\n    if not (has_digit and has_letter):\n        raise ValueError(\"Session key should contain both letters and digits\")\n\n    # Set the session key expiration time\n    expires = timezone.now() + timezone.timedelta(seconds=session_expire_time)\n\n    # Set the session key in a cookie and return the response\n    response = HttpResponse('Session key generated successfully.')\n    response.set_cookie('session_key', session_key, max_age=session_expire_time, expires=expires)\n    return response\n<end>"}
{"id": 32, "answer": " <start>\nimport random\nimport string\nfrom django.http import HttpResponse, HttpResponseCookie\n\ndef task_func(request, session_expire_time):\n    session_key = ''.join(random.choices(string.ascii_letters + string.digits, k=20))\n\n    has_digit = any(char.isdigit() for char in session_key)\n    has_letter = any(char.isalpha() for char in session_key)\n    if not (has_digit and has_letter or len(session_key)!=20):\n        raise ValueError(\"Session key should contain both letters and digits\")\n\n    response = HttpResponse('Session key generated successfully.')\n    response.set_cookie(key='session_key', value=session_key, max_age=session_expire_time, expires=None)\n    return response\n<end>"}
{"id": 33, "answer": " <start>\nimport csv\nfrom django.http import HttpResponse\nfrom io import StringIO\n\ndef task_func(request, header, csv_data):\n    csv_io = StringIO()\n    writer = csv.writer(csv_io)\n    writer.writerow(header)\n    writer.writerows(csv_data)\n    csv_io.seek(0)\n\n    response = HttpResponse(csv_io, content_type='text/csv')\n    response['Content-Disposition'] = 'attachment; filename=\"data.csv\"'\n\n    return response\n<end>"}
{"id": 34, "answer": " <start>\nimport csv\nfrom django.http import HttpResponse\n\ndef task_func(request, header, csv_data):\n    response = HttpResponse(content_type='text/csv')\n    response['Content-Disposition'] = 'attachment; filename=\"data.csv\"'\n\n    csv_io = io.StringIO()\n    writer = csv.writer(csv_io)\n    writer.writerow(header)\n    writer.writerows(csv_data)\n    csv_io.seek(0)\n    response.write(csv_io.getvalue())\n\n    return response\n<end>"}
{"id": 35, "answer": " <start>\nimport zipfile\nimport io\nfrom django.http import FileResponse\nfrom django.utils.encoding import force_text\nfrom django.conf import settings\n\ndef task_func(request, file_paths):\n    zip_io = io.BytesIO()\n\n    with zipfile.ZipFile(zip_io, 'w', zipfile.ZIP_DEFLATED) as zip_file:\n        for file_path in file_paths:\n            with open(file_path, 'rb') as f:\n                zip_file.writestr(file_path, f.read())\n\n    zip_io.seek(0)  # Reset the file pointer to the start of the stream\n    response = FileResponse(zip_io, content_type='application/zip')\n    response['Content-Disposition'] = f'attachment; filename=\"files.zip\"'\n\n    return response\n<end>"}
{"id": 36, "answer": " <start>\nimport zipfile\nimport io\nfrom django.http import FileResponse, HttpResponse\n\ndef task_func(file_paths):\n    zip_io = io.BytesIO()\n\n    with zipfile.ZipFile(zip_io, 'w') as zip_file:\n        for file_path in file_paths:\n            zip_file.writestr(file_path, 'This is the content of {}.'.format(file_path))\n\n    zip_io.seek(0)  # Reset the file pointer to the start of the stream\n    response = FileResponse(zip_io, content_type='application/zip', filename='files.zip')\n\n    return response\n<end>"}
{"id": 37, "answer": " <start>\nfrom flask import Flask, render_template, request, jsonify, logging\n\nlogging.basicConfig(filename=\"out.log\", level=logging.INFO)\n\ndef task_func(template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n\n    @app.route('/', methods=['POST'])\n    def handle_post():\n        data = request.get_json()\n        logging.info(json.dumps(data))\n        return render_template('index.html', data=data)\n\n    @app.after_request\n    def add_response_headers(response):\n        response.headers['Content-Type'] = 'application/json'\n        return response\n\n    return app\n<end>"}
{"id": 38, "answer": " <start>\n```python\nfrom flask import Flask, render_template, request, jsonify\nfrom logging import getLogger\n\nlogger = getLogger(__name__)\n\ndef task_func(template_folder):\n\n    app = Flask(__name__, template_folder=template_folder)\n\n    @app.route('/', methods=['POST'])\n    def handle_post():\n        data = request.get_json()\n        logger.info(json.dumps(data))\n        return render_template('index.html', data=data)\n\n    return app\n```\n<end>"}
{"id": 41, "answer": " <start>\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef task_func(df, n_clusters=3, random_state=0):\n    if df.empty or not all(col in df.columns for col in ['group', 'date', 'value']):\n        raise ValueError(\"DataFrame must be non-empty and contain 'group', 'date', and 'value' columns.\")\n\n    df['date'] = pd.to_datetime(df['date']).dt.ordinal\n    X = df[['date', 'value']]\n\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n    kmeans.set_output(transform='pandas')\n    kmeans.fit(X)\n    y_kmeans = kmeans.labels_\n\n    fig, ax = plt.subplots()\n    ax.scatter(X['date'], X['value'], c=y_kmeans, cmap='viridis')\n    ax.set_title('KMeans Clustering of Value vs Date')\n    ax.set_xlabel('Date (ordinal)')\n    ax.set_ylabel('Value')\n\n    return ax\n<end>"}
{"id": 42, "answer": " <start>\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef task_func(df, n_clusters=3, random_state=0):\n    if df.empty or not all(col in df.columns for col in ['group', 'date', 'value']):\n        raise ValueError(\"DataFrame must be non-empty and contain 'group', 'date', and 'value' columns.\")\n\n    if not pd.api.types.is_datetime64_any_dtype(df['date']):\n        raise ValueError(\"'date' column must be in datetime format.\")\n\n    df['date'] = df['date'].dt.ordinal\n    X = df[['date', 'value']]\n\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n    kmeans.fit(X)\n    y_kmeans = kmeans.predict(X)\n\n    fig, ax = plt.subplots()\n    ax.scatter(X['date'], X['value'], c=y_kmeans, cmap='viridis')\n    ax.set_title('KMeans Clustering of Value vs Date')\n    ax.set_xlabel('Date (ordinal)')\n    ax.set_ylabel('Value')\n\n    return ax\n<end>"}
{"id": 47, "answer": " <start>\nimport numpy as np\nimport pandas as pd\nfrom collections import OrderedDict\n\ndef task_func(country_dict):\n    COUNTRIES = ['USA', 'UK', 'China', 'Japan', 'Australia']\n    country_gdp = OrderedDict()\n\n    for country, name in country_dict.items():\n        if name in COUNTRIES:\n            country_gdp[country] = np.random.randint(1000000000, 100000000000, dtype=np.int64)\n\n    gdp_df = pd.DataFrame.from_dict(country_gdp, orient='index').reset_index().rename(columns={'index': 'Country', 0: 'GDP'})\n\n    return gdp_df\n<end>"}
{"id": 49, "answer": " <start>\nimport numpy as np\nimport pandas as pd\n\ndef task_func(country_dict):\n    COUNTRIES = ['USA', 'UK', 'China', 'Japan', 'Australia']\n    country_gdp = {country: np.random.randint(1000000000, 100000000000) for country in COUNTRIES if\n                   country in country_dict.values()}\n\n    # Use pandas.DataFrame.from_dict method to create DataFrame\n    gdp_df = pd.DataFrame.from_dict(country_gdp, orient='index')\n\n    return gdp_df\n<end>"}
{"id": 53, "answer": " <start>\n```python\nfrom django.http import JsonResponse\nimport random\nimport time\nimport json\n\ndef task_func(data, min_delay, max_delay):\n    # Generate a random delay\n    delay = random.uniform(min_delay, max_delay)\n\n    # Wait for the delay\n    time.sleep(delay)\n\n    # Ensure the data is in JSON format\n    json_data = json.dumps(data) if isinstance(data, str) else json.dumps(data, default=str)\n\n    # Return the response\n    return JsonResponse(json_data, content_type='application/json')\n```\n<end>"}
{"id": 54, "answer": " <start>\n```python\nfrom django.http import JsonResponse\nimport random\nimport time\n\ndef task_func(data, min_delay, max_delay):\n\n    # Generate a random delay\n    delay = random.uniform(min_delay, max_delay)\n\n    # Wait for the delay\n    time.sleep(delay)\n\n    return JsonResponse(data)\n```\n<end>"}
{"id": 55, "answer": " <start>\nimport pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Constants\nSTOPWORDS = pd.read_csv(\"stopwords.txt\", engine=\"python\", header=None, squeeze=True)\n\ndef task_func(dataframe, text_column):\n\n    def preprocess_text(text):\n        text = text.lower()\n        text = re.sub(r'\\d+', '', text)\n        text = re.sub(r'\\W+', ' ', text)\n        text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n        return text\n\n    dataframe[text_column] = dataframe[text_column].apply(preprocess_text)\n    vectorizer = CountVectorizer()\n    vectorized_data = vectorizer.fit_transform(dataframe[text_column])\n\n    return pd.DataFrame(vectorized_data.toarray(), columns=vectorizer.get_feature_names())\n<end>"}
{"id": 56, "answer": " <start>\nimport pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Constants\nSTOPWORDS = pd.read_csv(\"https://raw.githubusercontent.com/stopwords-iso/stopwords/master/stopwords/en.txt\")\n\ndef task_func(dataframe, text_column):\n\n    def preprocess_text(text):\n        text = text.lower()\n        text = re.sub(r'\\d+', '', text)\n        text = re.sub(r'\\W+', ' ', text)\n        return ' '.join([word for word in text.split() if word not in STOPWORDS])\n\n    dataframe[text_column] = dataframe[text_column].apply(preprocess_text)\n    vectorizer = CountVectorizer()\n    vectorized_data = vectorizer.fit_transform(dataframe[text_column])\n\n    return pd.DataFrame(vectorized_data.toarray(), columns=vectorizer.get_feature_names_out())\n<end>"}
{"id": 57, "answer": " <start>\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(mu=0, sigma=1, sample_size=1000, seed=0):\n    np.random.seed(seed)\n    rng = np.random.default_rng(seed)\n    sample = rng.normal(mu, sigma, sample_size)\n\n    fig, ax = plt.subplots()\n    ax.hist(sample, bins=30, normed=True, alpha=0.5, label='Sample Histogram')\n\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, sigma)\n    ax.plot(x, p, 'k', linewidth=2, label='Normal PDF')\n\n    ax.set_title(\"Normal Distribution with $\\\\mu = %0.2f, \\\\sigma = %0.2f$\" % (mu, sigma))\n    ax.legend()\n\n    return ax, np.mean(sample), np.std(sample)\n<end>"}
{"id": 58, "answer": " <start>\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(mu=0, sigma=1, sample_size=1000, seed=0):\n    np.random.seed(seed)\n    sample = np.random.normal(mu, sigma, sample_size)\n\n    fig, ax = plt.subplots()\n    ax.hist(sample, bins=30, density=True, alpha=0.5, label='Sample Histogram')\n\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, sigma)\n    ax.plot(x, p, 'k', linewidth=2, label='Normal PDF')\n\n    ax.set_title(f\"Normal Distribution with $\\\\mu = {mu:.2f}, \\\\sigma = {sigma:.2f}$\")\n    ax.legend()\n\n    return ax, np.mean(sample), np.std(sample)\n<end>"}
{"id": 61, "answer": " <start>\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\n\ndef task_func(range_start=-10, range_end=10, step=0.1):\n    if range_start > range_end:\n        raise ValueError(\"range_start cannot be smaller than range_end.\")\n\n    x_values = np.arange(range_start, range_end, step)\n\n    data = (x, np.sin(x), np.cos(x), abs(np.sin(x) - np.cos(x))) for x in x_values\n\n    fig, ax = plt.subplots()\n\n    for x, sin_x, cos_x, abs_x in data:\n        ax.scatter(x, sin_x, color='b')\n        ax.scatter(x, cos_x, color='r')\n        ax.scatter(x, abs_x, color='g')\n\n    fft_values = fft([abs(np.sin(x) - np.cos(x)) for x in x_values])\n\n    return data, fig, abs(np.mean(fft_values)), abs(np.median(fft_values))\n<end>"}
{"id": 62, "answer": " <start>\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fftpack import fft\n\ndef task_func(range_start=-10, range_end=10, step=0.1):\n    if range_start > range_end:\n        raise ValueError(\"range_start cannot be smaller than range_end.\")\n\n    x_values = np.arange(range_start, range_end, step)\n\n    def generate_data():\n        for x in x_values:\n            yield (x, np.sin(x), np.cos(x), abs(np.sin(x) - np.cos(x)))\n\n    fft_values = fft([abs(np.sin(x) - np.cos(x)) for x in x_values])\n\n    _, ax = plt.subplots()\n    for x, sin_x, cos_x, abs_x in generate_data():\n        ax.scatter(x, sin_x, color='b')\n        ax.scatter(x, cos_x, color='r')\n        ax.scatter(x, abs_x, color='g')\n\n    return generate_data(), ax, abs(np.mean(fft_values)), abs(np.median(fft_values))\n<end>"}
{"id": 63, "answer": " <start>\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\n\ndef task_func(range_start=-10, range_end=10, step=0.1):\n    if range_start > range_end:\n        raise ValueError(\"range_start cannot be smaller than range_end.\")\n\n    x_values = np.arange(range_start, range_end, step)\n\n    # Generate data\n    data = ((x, np.sin(x), np.cos(x), abs(np.sin(x) - np.cos(x))) for x in x_values)\n\n    # Plot data\n    fig, ax = plt.subplots()\n    for x, sin_x, cos_x, abs_x in data:\n        ax.scatter(x, sin_x, color='b')\n        ax.scatter(x, cos_x, color='r')\n        ax.scatter(x, abs_x, color='g')\n\n    # Calculate FFT and get mean and median\n    fft_values = fft([abs(np.sin(x) - np.cos(x)) for x in x_values])\n    fft_mean = abs(np.mean(fft_values))\n    fft_median = abs(np.median(fft_values))\n\n    return data, fig, fft_mean, fft_median\n<end>"}
{"id": 64, "answer": " <start>\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\n\ndef task_func(range_start=-10, range_end=10, step=0.1):\n    if range_start > range_end:\n        raise ValueError(\"range_start cannot be smaller than range_end.\")\n\n    x_values = np.arange(range_start, range_end, step)\n\n    # Generate the data\n    data = ((x, np.sin(x), np.cos(x), abs(np.sin(x) - np.cos(x))) for x in x_values)\n\n    # Calculate the FFT of the absolute difference between sin(x) and cos(x)\n    fft_values = fft([abs(np.sin(x) - np.cos(x)) for x in x_values])\n\n    # Create the plot\n    _, ax = plt.subplots()\n    for x, sin_x, cos_x, abs_x in data:\n        ax.scatter(x, sin_x, color='b')\n        ax.scatter(x, cos_x, color='r')\n        ax.scatter(x, abs_x, color='g')\n\n    # Return the data, plot, and FFT results\n    return data, ax, abs(np.mean(fft_values)), abs(np.median(fft_values))\n<end>"}
{"id": 65, "answer": " <start>\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fftpack import fft\n\ndef task_func(range_start=-10, range_end=10, step=0.1):\n    if range_start > range_end:\n        raise ValueError(\"range_start cannot be smaller than range_end.\")\n\n    x_values = np.arange(range_start, range_end, step)\n    data = ((x, np.sin(x), np.cos(x), abs(np.sin(x) - np.cos(x))) for x in x_values)\n\n    fig, ax = plt.subplots()\n    for x, sin_x, cos_x, abs_x in data:\n        ax.scatter(x, sin_x, color='b')\n        ax.scatter(x, cos_x, color='r')\n        ax.scatter(x, abs_x, color='g')\n\n    fft_values = fft([abs(np.sin(x) - np.cos(x)) for x in x_values])\n\n    return data, fig, abs(np.mean(fft_values)), abs(np.median(fft_values))\n<end>"}
{"id": 66, "answer": " <start>\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fftpack import fft\n\ndef task_func(range_start=-10, range_end=10, step=0.1):\n    if range_start > range_end:\n        raise ValueError(\"range_start cannot be smaller than range_end.\")\n\n    x_values = np.arange(range_start, range_end, step)\n\n    data = (x, np.sin(x), np.cos(x), abs(np.sin(x) - np.cos(x))) for x in x_values\n    fft_values = fft([abs(np.sin(x) - np.cos(x)) for x in x_values])\n\n    fig, ax = plt.subplots()\n    for x, sin_x, cos_x, abs_x in data:\n        ax.scatter(x, sin_x, color='b')\n        ax.scatter(x, cos_x, color='r')\n        ax.scatter(x, abs_x, color='g')\n\n    fft_mean = abs(np.mean(fft_values))\n    fft_median = abs(np.median(fft_values))\n\n    return data, fig, fft_mean, fft_median\n<end>"}
{"id": 67, "answer": " <start>\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\n\ndef task_func(range_start=-10, range_end=10, step=0.1):\n    if range_start > range_end:\n        raise ValueError(\"range_start cannot be smaller than range_end.\")\n\n    x_values = np.arange(range_start, range_end, step)\n\n    # Generate data\n    data = ((x, np.sin(x), np.cos(x), abs(np.sin(x) - np.cos(x))) for x in x_values)\n\n    # Plot data\n    fig, ax = plt.subplots()\n    for x, sin_x, cos_x, abs_x in data:\n        ax.scatter(x, sin_x, color='b')\n        ax.scatter(x, cos_x, color='r')\n        ax.scatter(x, abs_x, color='g')\n\n    # Calculate FFT\n    abs_diff = np.abs(np.sin(x_values) - np.cos(x_values))\n    fft_values = fft(abs_diff)\n\n    # Calculate mean and median of FFT\n    fft_mean = np.abs(np.mean(fft_values))\n    fft_median = np.abs(np.median(fft_values))\n\n    return data, fig, fft_mean, fft_median\n<end>"}
{"id": 68, "answer": " <start>\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\n\ndef task_func(range_start=-10, range_end=10, step=0.1):\n    if range_start > range_end:\n        raise ValueError(\"range_start cannot be smaller than range_end.\")\n\n    x_values = np.arange(range_start, range_end, step)\n\n    # Generate data\n    data = ((x, np.sin(x), np.cos(x), abs(np.sin(x) - np.cos(x))) for x in x_values)\n\n    # Plot data\n    fig, ax = plt.subplots()\n    for x, sin_x, cos_x, abs_x in data:\n        ax.scatter(x, sin_x, color='b')\n        ax.scatter(x, cos_x, color='r')\n        ax.scatter(x, abs_x, color='g')\n\n    # Calculate FFT\n    abs_diff = np.abs(np.sin(x_values) - np.cos(x_values))\n    fft_values = fft(abs_diff)\n\n    # Calculate mean and median of FFT\n    fft_mean = np.abs(np.mean(fft_values))\n    fft_median = np.abs(np.median(fft_values))\n\n    return data, fig, fft_mean, fft_median\n<end>"}
{"id": 69, "answer": " <start>\nimport json\nimport random\nfrom datetime import datetime, timedelta\nfrom datetime import UTC\n\nclass CustomTzinfo(datetime.tzinfo):\n    def utcoffset(self, dt):\n        return UTC()\n\n    def tzname(self, dt):\n        return \"Custom Timezone\"\n\n    def dst(self, dt):\n        return timedelta(0)\n\ndef task_func(file_path, num_entries, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    log_entries = []\n    current_time = datetime.now(CustomTzinfo())\n    for _ in range(num_entries):\n        user = random.choice(['Alice', 'Bob', 'Charlie', 'Dave', 'Eve'])\n        action = random.choice(['login', 'logout', 'view_page', 'edit_profile', 'post_message'])\n        timestamp = current_time.strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3]\n        log_entries.append({'user': user, 'action': action, 'timestamp': timestamp})\n        current_time -= timedelta(minutes=random.randint(1, 60))\n\n    with open(file_path, 'w') as json_file:\n        json.dump(log_entries, json_file, indent=4)\n\n    return file_path\n<end>"}
{"id": 70, "answer": " <start>\n```python\nimport json\nimport random\nfrom datetime import int, timedelta\nfrom datetime import datetime as dt\nfrom datetime import timezone\n\nUSERS = ['Alice', 'Bob', 'Charlie', 'Dave', 'Eve']\n\ndef task_func(file_path, num_entries, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    log_entries = []\n    current_time = dt.now(timezone.utc)\n    for _ in range(num_entries):\n        user = random.choice(USERS)\n        action = random.choice(['login', 'logout', 'view_page', 'edit_profile', 'post_message'])\n        timestamp = current_time.strftime('%Y-%m-%dT%H:%M:%S')\n        log_entries.append({'user': user, 'action': action, 'timestamp': timestamp})\n        current_time += timedelta(minutes=-random.randint(1, 60))\n\n    with open(file_path, 'w') as json_file:\n        json.dump(log_entries, json_file, indent=4)\n\n    return file_path\n```\n<end>"}
{"id": 73, "answer": " <start>\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\ndef task_func(original):\n    arr = np.array([b for (a, b) in original])\n\n    computed_stats = {\n        'mean': np.mean(arr),\n        'std': np.std(arr),\n        'min': np.min(arr),\n        'max': np.max(arr)\n    }\n\n    # Plotting histogram and PDF\n    fig, ax = plt.subplots()\n    ax.hist(arr, density=True, alpha=0.6, bins='auto')\n\n    # Adding PDF\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, computed_stats['mean'], computed_stats['std'])\n    ax.plot(x, p, 'k', linewidth=2)\n    ax.set_title('Histogram with PDF')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    plt.tight_layout()\n\n    return arr, computed_stats, ax\n<end>"}
{"id": 74, "answer": " <start>\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import rv_histogram\n\ndef task_func(original):\n    arr = np.array([b for (a, b) in original])\n\n    computed_stats = {\n        'mean': arr.mean(),\n        'std': arr.std(),\n        'min': arr.min(),\n        'max': arr.max()\n    }\n\n    # Create a scipy.stats.rv_histogram instance\n    histogram, _ = np.histogram(arr, bins='auto')\n    hist_dist = rv_histogram(histogram)\n\n    # Plotting histogram and PDF\n    fig, ax = plt.subplots()\n    ax.hist(arr, density=True, alpha=0.6, bins='auto', label='Histogram')\n\n    # Adding PDF\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = hist_dist.pdf(x)\n    ax.plot(x, p, 'k', linewidth=2, label='PDF')\n    ax.set_title('Histogram with PDF')\n    ax.legend()\n    plt.close(fig)  # Close the plot to prevent display here\n\n    return arr, computed_stats, ax\n<end>"}
{"id": 75, "answer": " <start>\nimport numpy as np\nfrom scipy.fft import fft2\nimport matplotlib.pyplot as plt\n\ndef task_func(original):\n    arr = np.array([b for (_, b) in original])\n\n    if arr.size == 0:\n        fft_data = np.array([])\n        return arr, fft_data, None\n\n    fft_data = fft2(arr)\n    _, ax = plt.subplots()\n    ax.hist(np.abs(fft_data.flatten()))\n\n    return arr, fft_data, ax\n<end>"}
{"id": 76, "answer": " <start>\nimport numpy as np\nfrom scipy.fftpack import fft2\nimport matplotlib.pyplot as plt\n\ndef task_func(original):\n    arr = np.array([b for (_, b) in original])\n\n    if arr.size == 0:\n        fft_data = np.array([])\n        return arr, fft_data, None\n\n    fft_data = fft2(arr)\n    _, ax = plt.subplots()\n    ax.hist(np.abs(fft_data.flatten()))\n\n    return arr, fft_data, ax\n<end>"}
{"id": 77, "answer": " <start>\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import sweep_poly\nfrom scipy.fft import fft\n\nANGLES = np.arange(0, 2 * np.pi, 0.01)\n\ndef task_func(n_waves, seed=0):\n    np.random.seed(seed)\n    sine_wave_series = []\n\n    if n_waves < 1:\n        return sine_wave_series, np.array([]), None\n\n    poly = np.poly1d([0, 0, 0])\n    for frequency in range(1, n_waves+1):\n        poly_coeffs = np.polyfit(ANGLES, np.ones(ANGLES.shape), degree=frequency-1)\n        poly += np.poly1d(poly_coeffs)\n        wave = sweep_poly(ANGLES, poly)\n        sine_wave_series.append(wave)\n\n    fft_data = fft(np.sum(sine_wave_series, axis=0))\n    _, ax = plt.subplots()\n    ax.hist(np.abs(fft_data))\n\n    return sine_wave_series, fft_data, ax\n<end>"}
{"id": 78, "answer": " <start>\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fftpack import fft2\nANGLES = np.arange(0, 2 * np.pi, 0.01)\n\ndef task_func(n_waves, seed=0):\n    np.random.seed(seed)\n    sine_wave_series = []\n\n    if n_waves < 1:\n        return sine_wave_series, np.array([]), None\n\n    for frequency in range(1, n_waves + 1):\n        wave = np.sin(frequency * ANGLES)\n        sine_wave_series.append(wave)\n\n    mixed_signal = np.sum(sine_wave_series, axis=0)\n    fft_data = np.abs(fft2(mixed_signal))\n    _, ax = plt.subplots()\n    ax.hist(fft_data)\n\n    return sine_wave_series, fft_data, ax\n<end>"}
{"id": 81, "answer": " <start>\nimport numpy as np\nfrom scipy.stats import norm, skew, kurtosis\nimport matplotlib.pyplot as plt\n\ndef task_func(matrix):\n    max_values = np.max(matrix, axis=0)\n\n    fig, ax = plt.subplots()\n    ax.hist(max_values, bins=10, density=True, alpha=0.6, color='g')\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, np.mean(max_values), np.std(max_values))\n    ax.plot(x, p, 'k', linewidth=2)\n\n    # Handle cases where all values are identical\n    if np.std(max_values) == 0:\n        skewness = 0.0  # Skewness is 0 for a perfectly symmetric distribution\n        kurtosis = -1.5  # Kurtosis is -1.5 for a uniform distribution\n    else:\n        skewness = skew(max_values)\n        kurtosis = kurtosis(max_values)\n\n    return skewness, kurtosis, ax\n<end>"}
{"id": 82, "answer": " <start>\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(matrix):\n    max_values = np.max(matrix, axis=0)\n\n    fig, ax = plt.subplots()\n    ax.hist(max_values, bins=10, density=True, alpha=0.6, color='g')\n\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, np.mean(max_values), np.std(max_values))\n    ax.plot(x, p, 'k', linewidth=2)\n\n    # Handle cases where all values are identical\n    if np.std(max_values) == 0:\n        skewness = 0.0  # Skewness is 0 for a perfectly symmetric distribution\n        kurtosis = -1.5  # Kurtosis is -1.5 for a uniform distribution\n    else:\n        skewness = stats.skew(max_values)\n        kurtosis = stats.kurtosis(max_values)\n\n    return skewness, kurtosis, ax\n<end>"}
{"id": 83, "answer": " <start>\nimport numpy as np\nfrom scipy.stats import norm, skew, kurtosis\nimport matplotlib.pyplot as plt\n\ndef task_func(matrix):\n    max_values = np.max(matrix, axis=1)\n\n    fig, ax = plt.subplots()\n    ax.hist(max_values, bins=10, density=True, alpha=0.6, color='g')\n\n    x = np.linspace(np.min(max_values), np.max(max_values), 100)\n    p = norm.pdf(x, np.mean(max_values), np.std(max_values))\n    ax.plot(x, p, 'k', linewidth=2)\n\n    skewness = skew(max_values)\n    kurtosis = kurtosis(max_values)\n\n    return skewness, kurtosis, ax\n<end>"}
{"id": 84, "answer": " <start>\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nfrom matplotlib.projections import HammerAxes\n\ndef task_func(matrix):\n    max_values = [max(row) for row in matrix]\n\n    fig, ax = plt.subplots(projection=HammerAxes())\n    ax.hist(max_values, bins=10, normed=True, alpha=0.6, color='g')\n\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, np.mean(max_values), np.std(max_values))\n    ax.plot(x, p, 'k', linewidth=2)\n\n    skewness = stats.skew(max_values)\n    kurtosis = stats.kurtosis(max_values)\n\n    return skewness, kurtosis, ax\n<end>"}
{"id": 85, "answer": " <start>\nimport numpy as np\nfrom scipy.stats import norm, skew, kurtosis\nimport matplotlib.pyplot as plt\n\ndef task_func(matrix):\n    max_values = np.max(matrix, axis=1)\n\n    fig, ax = plt.subplots()\n    ax.hist(max_values, bins=10, density=True, alpha=0.6, color='g')\n\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, np.mean(max_values), np.std(max_values))\n    ax.plot(x, p, 'k', linewidth=2)\n\n    skewness = skew(max_values)\n    kurtosis = kurtosis(max_values)\n\n    return skewness, kurtosis, ax\n<end>"}
{"id": 86, "answer": " <start>\nimport numpy as np\nfrom scipy.stats import rv_histogram, norm\nimport matplotlib.pyplot as plt\n\ndef task_func(matrix):\n    max_values = np.max(matrix, axis=1)\n\n    fig, ax = plt.subplots()\n    hist, bins, _ = rv_histogram(max_values, bins=10, density=True, fwhm=0.75)\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    mean, std = norm.fit(max_values)\n    p = norm.pdf(x, mean, std)\n    ax.plot(x, p, 'k', linewidth=2)\n\n    skewness = stats.skew(max_values)\n    kurtosis = stats.kurtosis(max_values)\n\n    return skewness, kurtosis, ax\n<end>"}
{"id": 87, "answer": " <start>\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import rfft  # \u4f7f\u7528 scipy.fft \u66ff\u4ee3 scipy.fftpack\n\ndef task_func(signal, precision=2, seed=777):\n    np.random.seed(seed)\n    transformed_signal = rfft(signal)\n    transformed_signal_rounded = np.round(transformed_signal, precision)\n\n    fig, ax = plt.subplots(2, 1, figsize=(10, 6))  # \u8bbe\u7f6e\u56fe\u5f62\u5927\u5c0f\n    ax[0].plot(signal)\n    ax[0].set_title('Original Signal')\n    ax[1].plot(transformed_signal_rounded.real)  # \u53ea\u7ed8\u5236\u5b9e\u90e8\n    ax[1].set_title('Transformed Signal')\n    plt.tight_layout()  # Adjust layout to avoid overlap\n\n    return transformed_signal_rounded, ax\n<end>"}
{"id": 88, "answer": " <start>\nimport numpy as np\nimport matplotlib.pylab as pylab\nfrom numpy.fft.fftpack import fft\n\ndef task_func(signal, precision=2, seed=777):\n    np.random.seed(seed)\n    transformed_signal = fft(signal)\n    transformed_signal_rounded = np.round(transformed_signal, precision).tolist()\n\n    fig, ax = pylab.subplots(2, 1)\n    ax[0].plot(signal)\n    ax[0].set_title('Original Signal')\n    ax[1].plot(transformed_signal_rounded)\n    ax[1].set_title('Transformed Signal')\n    pylab.tight_layout()  # Adjust layout to avoid overlap\n\n    return np.array(transformed_signal_rounded), ax\n<end>"}
{"id": 89, "answer": " <start>\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df):\n    # Check if the required columns are present\n    if not all(col in df.columns for col in ['id', 'age', 'income']):\n        raise ValueError(\"DataFrame must contain 'id', 'age', and 'income' columns.\")\n\n    # Initialize the scaler\n    scaler = StandardScaler()\n\n    # Standardize 'age' and 'income' columns for each group by 'id'\n    df_standardized = df.groupby('id')['age', 'income'].apply(scaler.transform).reset_index(drop=True)\n\n    return df_standardized\n<end>"}
{"id": 90, "answer": " <start>\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df):\n    scaler = StandardScaler()\n\n    # Use the `assign` method to create and standardize the 'age' and 'income' columns for each group by 'id'\n    df_grouped = df.groupby('id').apply(lambda x: x.assign(age=scaler.transform(x['age']), income=scaler.transform(x['income'])))\n\n    return df_grouped\n<end>"}
{"id": 91, "answer": " <start>\nimport itertools\nimport statistics\n\ndef task_func(elements, subset_size):\n    combinations = list(itertools.combinations(elements, subset_size))\n    sums = [sum(combination) for combination in combinations]\n\n    # Calculate mode using statistics.mode\n    mode = statistics.mode(sums)\n\n    return {\n        'mean': statistics.mean(sums),\n        'median': statistics.median(sums),\n        'mode': mode\n    }\n<end>"}
{"id": 92, "answer": " <start>\nimport itertools\nimport statistics\nfrom collections.abc import Iterable\nfrom typing import Counter\n\ndef task_func(elements: Iterable, subset_size: int) -> dict:\n    combinations = list(itertools.combinations(elements, subset_size))\n    sums = [sum(combination) for combination in combinations]\n\n    count = Counter(sums)\n    mode = count.most_common(1)[0][0]\n\n    return {\n        'mean': statistics.mean(sums),\n        'median': statistics.median(sums),\n        'mode': mode\n    }\n<end>"}
{"id": 93, "answer": " <start>\nimport itertools\nimport statistics\n\nfrom faker import Faker\n\ndef task_func(elements, subset_size):\n    faker = Faker()\n    weighted_elements = {faker.random_element(OrderedDict([\n        (\"{{elements}}\", weight)\n    ])): weight for weight in [len(elements) / subset_size] * subset_size}\n\n    combinations = list(itertools.combinations(weighted_elements, subset_size))\n    sums = [sum(combination) for combination in combinations]\n    return {\n        'mean': statistics.mean(sums),\n        'median': statistics.median(sums),\n        'mode': statistics.mode(sums)\n    }\n<end>"}
{"id": 94, "answer": " <start>\nimport itertools\nimport statistics\nfrom faker.utils.datasets import Counter\n\ndef task_func(elements, subset_size):\n    counter = Counter(elements)\n    combinations = list(itertools.combinations(elements, subset_size))\n    sums = [counter.update(combination).elements() for combination in combinations]\n    sums = [sum(combination) for combination in sums]\n    return {\n        'mean': statistics.mean(sums),\n        'median': statistics.median(sums),\n        'mode': statistics.mode(sums)\n    }\n<end>"}
{"id": 95, "answer": " <start>\nimport itertools\nimport statistics\nfrom collections.abc import Iterable\nfrom typing import Counter\n\ndef task_func(elements: Iterable, subset_size: int):\n    combinations = list(itertools.combinations(elements, subset_size))\n    sums = [sum(combination) for combination in combinations]\n\n    # Calculate mode using built-in Counter class\n    count = Counter(sums)\n    mode = count.most_common(1)[0][0]\n\n    return {\n        'mean': statistics.mean(sums),\n        'median': statistics.median(sums),\n        'mode': mode\n    }\n<end>"}
{"id": 96, "answer": " <start>\nimport itertools\nimport statistics\nfrom collections.abc import Iterable\nfrom typing import Counter\n\ndef task_func(elements: Iterable, subset_size: int):\n    combinations = list(itertools.combinations(elements, subset_size))\n    sums = [sum(combination) for combination in combinations]\n\n    # Calculate mode using built-in Counter class\n    count = Counter(sums)\n    mode = count.most_common(1)[0][0]\n\n    return {\n        'mean': statistics.mean(sums),\n        'median': statistics.median(sums),\n        'mode': mode\n    }\n<end>"}
{"id": 97, "answer": " <start>\nimport itertools\nimport statistics\n\ndef task_func(elements, subset_size):\n    combinations = list(itertools.combinations(elements, subset_size))\n    sums = [sum(combination) for combination in combinations]\n\n    # Calculate mode using statistics.mode\n    mode = statistics.mode(sums)\n\n    return {\n        'mean': statistics.mean(sums),\n        'median': statistics.median(sums),\n        'mode': mode\n    }\n<end>"}
{"id": 98, "answer": " <start>\nimport itertools\nimport statistics\n\ndef task_func(elements, subset_size):\n    combinations = list(itertools.combinations(elements, subset_size))\n    sums = [sum(combination) for combination in combinations]\n\n    # Calculate mode using statistics.mode\n    mode = statistics.mode(sums)\n\n    return {\n        'mean': statistics.mean(sums),\n        'median': statistics.median(sums),\n        'mode': mode\n    }\n<end>"}
{"id": 99, "answer": " <start>\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['Date', 'Value']\n\ndef task_func(df, plot=False):\n    # Check if the required columns are present\n    if not all(col in df.columns for col in COLUMNS):\n        raise KeyError(f\"DataFrame must contain the columns: {COLUMNS}\")\n\n    # Convert 'Date' column to datetime\n    df['Date'] = pd.to_datetime(df['Date'])\n\n    # Split the 'Value' column into multiple columns\n    value_df = df['Value'].apply(lambda x: pd.Series(x))\n\n    # Fill NaN values with 0 (or any other appropriate value)\n    value_df = value_df.fillna(0)\n\n    # Scale the values using StandardScaler\n    scaler = StandardScaler()\n    scaled_values = scaler.fit_transform(value_df)\n\n    # Create a new DataFrame with the scaled values\n    scaled_df = pd.DataFrame(scaled_values, columns=[f'Scaled_{i}' for i in range(scaled_values.shape[1])])\n\n    # Combine the 'Date' column with the scaled values\n    result_df = pd.concat([df['Date'], scaled_df], axis=1)\n\n    if plot:\n        plt.figure()\n        result_df.set_index('Date').plot(kind='bar', stacked=True)\n        plt.title('Scaled Values Over Time')\n        plt.xlabel('Date')\n        plt.ylabel('Scaled Value')\n\n    return result_df\n<end>"}
{"id": 100, "answer": " <start>\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['Date', 'Value']\n\ndef task_func(df, plot=False):\n    df['Date'] = pd.to_datetime(df['Date'])\n\n    # Extract the 'Value' column and convert it to a list of DataFrames\n    value_dfs = df['Value'].apply(pd.DataFrame.from_records)\n\n    # Concatenate the 'Date' DataFrame and the list of 'Value' DataFrames\n    df = pd.concat([df['Date'], pd.concat(value_dfs, axis=1)], axis=1)\n\n    # Scale the columns using StandardScaler\n    scaler = StandardScaler()\n    df.iloc[:,1:] = scaler.fit_transform(df.iloc[:,1:])\n\n    if plot:\n        plt.figure()\n        ax = df.set_index('Date').plot(kind='bar', stacked=True)\n        plt.title('Scaled Values Over Time')\n        plt.xlabel('Date')\n        plt.ylabel('Scaled Value')\n\n    return df, ax\n<end>"}
{"id": 101, "answer": " <start>\nimport itertools\nimport math\nfrom pandas import Series\n\ndef task_func(elements, subset_size, top_n=2):\n    if subset_size > len(elements) or subset_size <= 0:\n        return 1, []\n\n    combinations = list(itertools.combinations(elements, subset_size))\n    sums = [sum(combination) for combination in combinations if len(combination) != 0]\n\n    # Use pandas.core.arrays.sparse.scipy_sparse.Series.agg to calculate the product of sums\n    product = math.prod(Series(sums).agg('product'))\n\n    # Use pandas.core.arrays.sparse.scipy_sparse.Series.sort_values to sort the sums and get the top_n sums\n    top_sums = Series(sums).sort_values(ascending=False)[:top_n]\n\n    return product, top_sums\n<end>"}
{"id": 102, "answer": " <start>\nimport itertools\nimport math\nimport pandas as pd\n\ndef task_func(elements, subset_size, top_n=2):\n    if subset_size > len(elements) or subset_size <= 0:\n        return 1, []\n\n    combinations = list(itertools.combinations(elements, subset_size))\n    sums = pd.Series([sum(combination) for combination in combinations if len(combination) != 0])\n    product = math.prod(sums.sum())\n    top_sums = sums.sort_values(ascending=False).head(top_n)\n    return product, top_sums.to_list()\n<end>"}
{"id": 103, "answer": " <start>\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nDISTRIBUTION_SIZE = 1000\n\ndef task_func(bins=30):\n    distribution = [random.gauss(0, 1) for _ in range(DISTRIBUTION_SIZE)]\n    fig, ax = plt.subplots()\n    ax.hist(distribution, bins=bins, edgecolor='black')\n    return distribution, ax.patches\n<end>"}
{"id": 104, "answer": " <start>\nimport random\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import Subplot\n\ndef task_func(bins=30):\n    distribution = [random.gauss(0, 1) for _ in range(DISTRIBUTION_SIZE)]\n    ax = Subplot().hist(distribution, bins=bins, edgecolor='black')[2]\n    return distribution, ax\n<end>"}
{"id": 105, "answer": " <start>\nimport re\nimport os\nimport shutil\nfrom datetime import datetime\nfrom faker import Faker\n\ndef task_func(directory):\n    DATE_FORMAT = '%Y%m%d%H%M%S'\n    moved_files = {}\n    fake = Faker()\n\n    for filename in os.listdir(directory):\n        file_path = os.path.join(directory, filename)\n        if os.path.isfile(file_path):  # Ensure it's a file\n            with open(file_path, 'r') as file:\n                content = file.read()\n                match = re.search(r'([^\\[\\]]+)\\[.*?\\]', content)\n                if match:\n                    subdirectory = match.group(1).strip()\n\n                    if not os.path.exists(os.path.join(directory, subdirectory)):\n                        os.makedirs(os.path.join(directory, subdirectory))\n\n                    name, ext = os.path.splitext(filename)\n                    new_filename = \"{}_{}{}\".format(name, datetime.now().strftime(DATE_FORMAT), ext)\n                    shutil.move(file_path, os.path.join(directory, subdirectory, new_filename))\n\n                    if subdirectory not in moved_files:\n                        moved_files[subdirectory] = []\n                    moved_files[subdirectory].append(new_filename)\n                else:\n                    subdirectory = fake.random_element(elements=['random_subdir1', 'random_subdir2'])\n\n                    if not os.path.exists(os.path.join(directory, subdirectory)):\n                        os.makedirs(os.path.join(directory, subdirectory))\n\n                    name, ext = os.path.splitext(filename)\n                    new_filename = \"{}_{}{}\".format(name, datetime.now().strftime(DATE_FORMAT), ext)\n                    shutil.move(file_path, os.path.join(directory, subdirectory, new_filename))\n\n                    if subdirectory not in moved_files:\n                        moved_files[subdirectory] = []\n                    moved_files[subdirectory].append(new_filename)\n\n    return directory, moved_files\n<end>"}
{"id": 106, "answer": " <start>\nimport re\nimport os\nimport shutil\nfrom datetime import datetime\nfrom faker.utils.datasets import Counter\n\ndef task_func(directory):\n    DATE_FORMAT = '%Y%m%d%H%M%S'\n    moved_files = Counter()\n\n    for filename in os.listdir(directory):\n        with open(os.path.join(directory, filename), 'r') as file:\n            content = file.read()\n            match = re.search('(.*?)\\[.*?\\]', content)\n            if match:\n                subdirectory = match.group(1).strip()\n\n                if not os.path.exists(os.path.join(directory, subdirectory)):\n                    os.makedirs(os.path.join(directory, subdirectory))\n\n                new_filename = f\"{filename.split('.')[0]}_{datetime.now().strftime(DATE_FORMAT)}.{filename.split('.')[1]}\"\n                shutil.move(os.path.join(directory, filename), os.path.join(directory, subdirectory, new_filename))\n                moved_files[subdirectory] += 1\n\n    return directory, dict(moved_files)\n<end>"}
{"id": 107, "answer": " <start>\nimport os\nfrom sendgrid import SendGridAPIClient\nfrom sendgrid.helpers.mail import Mail\nfrom python_http_client.client import HTTPResponse\nfrom sendgrid.helpers.mail.content.ValidateApiKey import ValidateApiKey\n\ndef task_func(dir, api_key, recipient_email):\n    try:\n        file_list = os.listdir(dir)\n    except FileNotFoundError:\n        return False\n\n    validate_api_key = ValidateApiKey()\n    message = Mail(\n        from_email='from_email@example.com',\n        to_emails=recipient_email,\n        subject=f'Directory Listing for {dir}',\n        plain_text_content=validate_api_key.validate_message_text(', '.join(file_list)))\n\n    sg = SendGridAPIClient(api_key)\n    response = sg.send(message)\n\n    if response.status_code >= 200 and response.status_code < 300:\n        return True\n    else:\n        raise HTTPResponse(response)\n<end>"}
{"id": 108, "answer": " <start>\nimport os\nfrom sendgrid import SendGridAPIClient\nfrom sendgrid.helpers.mail import Mail\nfrom python_http_client.client import HTTPResponse\nfrom sendgrid.helpers.mail.content.ValidateApiKey import ValidateApiKey\n\ndef task_func(dir, api_key, recipient_email):\n    try:\n        file_list = os.listdir(dir)\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"Directory '{dir}' does not exist.\")\n\n    file_list_str = ', '.join(file_list)\n\n    message = Mail(\n        from_email='from_email@example.com',\n        to_emails=recipient_email,\n        subject=f'Directory Listing for {dir}',\n        plain_text_content=file_list_str)\n\n    ValidateApiKey().validate_message_dict(message.get_content())\n\n    sg = SendGridAPIClient(api_key)\n    response = sg.send(message)\n\n    if response.status_code >= 200 and response.status_code < 300:\n        return True\n    else:\n        raise HTTPError(response)\n<end>"}
{"id": 111, "answer": " <start>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(points_count=1000, radius=1):\n    points = np.random.uniform(low=-radius, high=radius, size=(points_count, 2))\n\n    fig, ax = plt.subplots()\n    ax.scatter(points[:, 0], points[:, 1])\n    ax.set_aspect('equal', adjustable='box')\n    ax.set_xlim(ax.get_xlim())\n    ax.set_ylim(ax.get_ylim())\n    return ax\n<end>"}
{"id": 112, "answer": " <start>\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import Axes\nfrom matplotlib.random_state import RandomState\n\ndef task_func(points_count=1000, radius=1):\n\n    rng = RandomState(0)\n    points = rng.uniform(size=(points_count, 2), low=-radius, high=radius)\n\n    fig, ax = plt.subplots()\n    ax.scatter(*points.T)\n    ax.set_aspect('equal', adjustable='box')\n    return ax\n<end>"}
{"id": 113, "answer": " <start>\nimport re\nimport matplotlib.pyplot as plt\nfrom nltk.draw.dispersion import dispersion_plot\nfrom nltk.corpus.reader.crubadan import FreqDist\n\ndef task_func(example_str, top_n=30):\n    # Extract text not enclosed in square brackets\n    text = ' '.join(re.findall('(.*?)(?:\\\\[.*?\\\\]|$)', example_str))\n    words = text.split()\n\n    # Create frequency distribution\n    fdist = FreqDist(words)\n\n    # Plot the frequency distribution\n    ax = dispersion_plot(words, fdist, ignore_case=False, title='Lexical Dispersion Plot')\n\n    # Get the top_n most common words\n    top_n_words = dict(fdist.most_common(top_n))\n    return ax, top_n_words\n<end>"}
{"id": 114, "answer": " <start>\nimport re\nimport matplotlib.pyplot as plt\nfrom matplotlib.font_manager import dict\nfrom nltk.corpus.reader.crubadan import FreqDist\n\ndef task_func(example_str, top_n=30):\n    text = ' '.join(re.findall('(.*?)\\\\[.*?\\\\]', example_str))\n    words = text.split()\n    fdist = FreqDist(words)\n\n    if top_n > len(fdist):\n        top_n = len(fdist)\n    # Initialize a fresh plot for the frequency distribution but do not show it\n    plt.figure()\n    ax = fdist.plot(top_n, cumulative=False, show=False)\n    plt.close()\n\n    top_n_words = dict(fdist.most_common(top_n))\n    return ax, top_n_words\n<end>"}
{"id": 118, "answer": " <start>\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pandas import DataFrame\n\n# Constants\nCOLORS = ['r', 'g', 'b']\n\ndef task_func(df, group_col, value_col):\n    # Check if the value column contains non-numeric values\n    if not np.issubdtype(df[value_col].dtype, np.number):\n        raise TypeError(f\"The column '{value_col}' contains non-numeric values.\")\n\n    # Group the data and calculate mean and standard deviation\n    grouped_data = df.groupby(group_col)\n    group_mean = grouped_data[value_col].mean()\n    group_std = grouped_data[value_col].std()\n\n    # Get the number of groups and generate x locations for the bars\n    num_groups = len(group_mean)\n    index = np.arange(num_groups)\n\n    # Create the bar chart with error bars\n    ax = plt.errorbar(index, group_mean, yerr=group_std, color=COLORS, capsize=4, label=group_mean.index)\n\n    # Set labels and title\n    plt.xlabel(group_col)\n    plt.ylabel(value_col)\n    plt.title(f'Bar chart of {value_col} by {group_col}')\n    plt.xticks(index, group_mean.index)  # Set x-axis labels to group names\n    plt.legend()\n\n    # Return the axes object\n    return ax\n<end>"}
{"id": 121, "answer": " <start>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLORS = ['r', 'g', 'b']\n\ndef task_func(df, group_col, value_col):\n    # Check if the value column contains non-numeric values\n    if not np.issubdtype(df[value_col].dtype, np.number):\n        raise TypeError(f\"The column '{value_col}' contains non-numeric values.\")\n\n    # Calculate mean and standard deviation for each group\n    grouped = df.groupby(group_col)\n    group_mean = grouped[value_col].mean()\n    group_std = grouped[value_col].std()\n\n    # Get the number of groups and generate x locations for the bars\n    num_groups = len(group_mean)\n    index = np.arange(num_groups)\n\n    # Create the bar chart with error bars\n    for i, (mean, std) in enumerate(zip(group_mean, group_std)):\n        plt.bar(index[i], mean, yerr=std, color=COLORS[i % len(COLORS)], capsize=4, label=f'Group {i+1}')\n\n    # Set labels and title\n    plt.xlabel(group_col)\n    plt.ylabel(value_col)\n    plt.title(f'Bar chart of {value_col} by {group_col}')\n    plt.xticks(index, group_mean.index)  # Set x-axis labels to group names\n    plt.legend()\n\n    # Return the axes object\n    return plt.gca()\n<end>"}
{"id": 122, "answer": " <start>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLORS = ['r', 'g', 'b']\n\ndef task_func(df, group_col, value_col):\n    # Check if the value column contains non-numeric values\n    if not np.issubdtype(df[value_col].dtype, np.number):\n        raise TypeError(f\"The column '{value_col}' contains non-numeric values.\")\n\n    # Calculate mean and standard deviation for each group\n    grouped = df.groupby(group_col)\n    group_mean = grouped[value_col].mean()\n    group_std = grouped[value_col].std()\n\n    # Get the number of groups and generate x locations for the bars\n    num_groups = len(group_mean)\n    index = np.arange(num_groups)\n\n    # Create the bar chart with error bars\n    for i, (mean, std) in enumerate(zip(group_mean, group_std)):\n        plt.bar(index[i], mean, yerr=std, color=COLORS[i % len(COLORS)], capsize=4, label=f'Group {i+1}')\n\n    # Set labels and title\n    plt.xlabel(group_col)\n    plt.ylabel(value_col)\n    plt.title(f'Bar chart of {value_col} by {group_col}')\n    plt.xticks(index, group_mean.index)  # Set x-axis labels to group names\n    plt.legend()\n\n    # Return the axes object\n    return plt.gca()\n<end>"}
{"id": 127, "answer": " <start>\nimport numpy as np\nfrom scipy.stats import norm\nfrom numpy.random import Generator\n\ndef task_func(x):\n    # Type check for x\n    if not isinstance(x, np.ndarray):\n        raise TypeError(\"x must be numpy.ndarray\")\n\n    # Create a random number generator\n    rng = np.random.Generator()\n\n    # Generate random samples for the real and imaginary parts\n    real_part = rng.multivariate_normal([0, 0], [[1, 0], [0, 1]])\n    imag_part = rng.multivariate_normal([2, 0], [[1, 0], [0, 4]])\n\n    # Combine the real and imaginary parts to create the complex distribution\n    complex_dist = real_part + 1j * imag_part\n\n    plt.plot(x, complex_dist.real, label='Real part')\n    plt.plot(x, complex_dist.imag, label='Imaginary part')\n    plt.legend()\n    plt.grid()\n    plt.show()\n    return complex_dist\n<end>"}
{"id": 128, "answer": " <start>\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(x):\n    # Type check for x\n    if not isinstance(x, np.ndarray):\n        raise TypeError(\"x must be numpy.ndarray\")\n\n    # Generate real and imaginary parts of the complex distribution\n    real_dist = norm.pdf(x, loc=0, scale=1)\n    imag_dist = norm.pdf(x, loc=2, scale=2)\n\n    # Combine real and imaginary parts into a complex distribution\n    complex_dist = real_dist + 1j * imag_dist\n\n    # Plot the real and imaginary parts of the complex distribution\n    plt.plot(x, real_dist, label='Real part')\n    plt.plot(x, imag_dist, label='Imaginary part')\n    plt.legend()\n    plt.grid()\n    plt.show()\n\n    return complex_dist\n<end>"}
{"id": 129, "answer": " <start>\n```python\nimport datetime\nfrom collections.abc import Iterable\nimport matplotlib.pyplot as plt\nfrom matplotlib.dates import WeekdayLocator, DateFormatter\n\ndef task_func(activities):\n    if not all(isinstance(activity, datetime.datetime) for activity in activities):\n        raise TypeError('All activities must be datetime objects')\n\n    activity_counts = {}\n\n    # Count the activities for each day of the week\n    for activity in activities:\n        day = activity.weekday()\n        if day not in activity_counts:\n            activity_counts[day] = 0\n        activity_counts[day] += 1\n\n    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n    counts = [activity_counts[day] for day in sorted(activity_counts.keys())]\n\n    fig, ax = plt.subplots()\n    loc = WeekdayLocator(byweekday=[0, 1, 2, 3, 4, 5])  # Monday through Sunday\n    ax.bar(loc.tick_values(len(days)), counts)\n    ax.xaxis_date()\n    ax.xaxis.set_major_locator(loc)\n    ax.xaxis.set_major_formatter(DateFormatter('%A'))\n    ax.set_xticks(loc.tick_values(len(days)))\n    ax.set_xticklabels(days)\n    ax.set_xlabel('Day of the Week')\n    ax.set_ylabel('Number of Activities')\n    ax.set_title('Weekly Activity')\n\n    return ax\n```\n<end>"}
{"id": 130, "answer": " <start>\n```python\nimport datetime\nfrom collections.abc import Iterable\nimport matplotlib.pyplot as plt\nfrom matplotlib.dates import is_busday\n\ndef task_func(activities):\n    if not isinstance(activities, Iterable):\n        raise TypeError('Activities must be an iterable')\n\n    if not all(isinstance(activity, datetime.datetime) for activity in activities):\n        raise TypeError('All activities must be datetime objects')\n\n    activity_counts = defaultdict(int)\n\n    # Count the activities for each day of the week\n    for activity in activities:\n        if is_busday([activity], weekmask='1111100'):\n            day = activity.strftime('%A')\n            activity_counts[day] += 1\n\n    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n    counts = [activity_counts[day] for day in days]\n\n    plt.figure()\n    fig, ax = plt.subplots()\n    ax.bar(days, counts)\n    ax.set_xlabel('Day of the Week')\n    ax.set_ylabel('Number of Activities')\n    ax.set_title('Weekly Activity')\n\n    return ax\n```\n<end>"}
{"id": 131, "answer": " <start>\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(l):\n    fig, ax = plt.subplots()\n    ax.hist(l, bins='auto', density=True, alpha=0.6, color='g')\n\n    mu, std = norm.fit(l)\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, std)\n    ax.plot(x, p, 'k', linewidth=2)\n    title = f\"Fit results: mu = {round(mu, 2)},  std = {round(std, 2)}\"\n    ax.set_title(title)\n    return ax\n<end>"}
{"id": 132, "answer": " <start>\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(l):\n    fig, ax = plt.subplots()\n    ax.hist(l, bins='auto', density=True, alpha=0.6, color='g')\n\n    mu, std = stats.norm.fit(l)\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, std)\n    ax.plot(x, p, 'k', linewidth=2)\n    title = f\"Fit results: mu = {round(mu, 2)},  std = {round(std, 2)}\"\n    ax.set_title(title)\n    return ax\n<end>"}
{"id": 133, "answer": " <start>\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(length):\n    MU = 0\n    SIGMA = 1\n\n    rng = np.random.default_rng()\n    distribution = rng.normal(MU, SIGMA, length)\n    fig, ax = plt.subplots()\n    ax.hist(distribution, 30, density=None, bins=None, alpha=1, color='c')\n    ax.plot(np.sort(distribution), norm.pdf(np.sort(distribution), MU, SIGMA), linewidth=2, color='r')\n\n    return distribution, ax\n<end>"}
{"id": 134, "answer": " <start>\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(length):\n    MU = 0\n    SIGMA = 1\n\n    distribution = np.random.normal(MU, SIGMA, length)\n    fig, ax = plt.subplots()\n    ax.hist(distribution, density=True, bins=30, label='Histogram')\n    ax.plot(np.sort(distribution), norm.pdf(np.sort(distribution), MU, SIGMA), linewidth=2, color='r', label='PDF')\n    ax.legend()\n\n    return distribution, ax\n<end>"}
{"id": 135, "answer": " <start>\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(length):\n\n    distribution = norm.rvs(loc=0, scale=1, size=length)\n    fig, ax = plt.subplots()\n    ax.hist(distribution, 30, density=True, label='Histogram')\n    ax.plot(np.sort(distribution), norm.pdf(np.sort(distribution), 0, 1), linewidth=2, color='r', label='PDF')\n    ax.legend()\n\n    return distribution, ax\n<end>"}
{"id": 136, "answer": " <start>\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(length):\n\n    MU = 0\n    SIGMA = 1\n\n    distribution = norm.rvs(MU, SIGMA, size=length)\n    fig, ax = plt.subplots()\n    ax.hist(distribution, 30, density=True, label='Histogram')\n    ax.plot(distribution, norm.pdf(distribution, MU, SIGMA), linewidth=2, color='r', label='PDF')\n    ax.legend()\n\n    return distribution, ax\n<end>"}
{"id": 137, "answer": " <start>\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(length):\n    MU = 0\n    SIGMA = 1\n\n    distribution = np.random.normal(loc=MU, scale=SIGMA, size=length)\n    fig, ax = plt.subplots()\n    ax.hist(distribution, bins=30, density=True, label='Histogram')  # \u4f7f\u7528density\u66ff\u4ee3normed\n    ax.plot(np.sort(distribution), norm.pdf(np.sort(distribution), MU, SIGMA), linewidth=2, color='r', label='PDF')\n    ax.legend()\n\n    return distribution, ax\n<end>"}
{"id": 138, "answer": " <start>\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(length):\n    MU = 0\n    SIGMA = 1\n\n    rng = np.random.default_rng()\n    distribution = rng.normal(MU, SIGMA, length)\n    fig, ax = plt.subplots()\n    ax.hist(distribution, 30, density=None, bins=None, alpha=1, color='c')\n    ax.plot(np.sort(distribution), norm.pdf(np.sort(distribution), MU, SIGMA), linewidth=2, color='r')\n\n    return distribution, ax\n<end>"}
{"id": 139, "answer": " <start>\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import probplot\n\ndef task_func(mu, sigma, num_samples=1000, seed=77):\n    np.random.seed(seed)\n    samples = np.random.normal(mu, sigma, num_samples)\n\n    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n    axes[0].hist(samples, bins=30, density=True, alpha=0.6, color='g')\n\n    axes[1] = probplot(samples, dist=\"norm\", plot=axes[1])\n\n    return fig\n<end>"}
{"id": 140, "answer": " <start>\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm, probplot\n\ndef task_func(mu, sigma, num_samples=1000, seed=77):\n    np.random.seed(seed)\n    samples = np.random.normal(mu, sigma, num_samples)\n\n    fig = plt.figure(figsize=(12, 6))\n    plt.subplot(1, 2, 1)\n    plt.hist(samples, bins=30, density=True, alpha=0.6, color='g')\n\n    plt.subplot(1, 2, 2)\n    probplot(samples, dist=norm, plot=plt)\n\n    return fig\n<end>"}
{"id": 141, "answer": " <start>\nimport cv2\nimport numpy as np\nimport os\n\ndef task_func(img_path):\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(f\"No file found at {img_path}\")\n\n    img = cv2.imread(img_path)\n    if img is None:\n        raise ValueError(f\"Unable to read image at {img_path}\")\n\n    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n    # Apply binary thresholding to ensure the image is binary before finding contours\n    _, binary_img = cv2.threshold(gray_img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n\n    # Find contours\n    contours, _ = cv2.findContours(binary_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\n    return np.array(img), contours\n<end>"}
{"id": 142, "answer": " <start>\nimport cv2\nimport numpy as np\nimport os\nfrom PIL import Image\n\ndef task_func(img_path):\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(f\"No file found at {img_path}\")\n\n    # Open the image using PIL\n    img = Image.open(img_path)\n\n    # Convert the image to RGB (PIL uses BGR by default)\n    rgb_img = img.convert('RGB')\n\n    # Convert the RGB image to grayscale using OpenCV\n    gray_img = cv2.cvtColor(np.array(rgb_img), cv2.COLOR_RGB2GRAY)\n\n    # Apply binary thresholding to ensure the image is binary before finding contours\n    _, binary_img = cv2.threshold(gray_img, 127, 255, cv2.THRESH_BINARY)\n\n    # Find contours\n    contours, _ = cv2.findContours(binary_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\n    return np.array(rgb_img), contours\n<end>"}
{"id": 143, "answer": " <start>\nimport cv2\nimport numpy as np\nimport os\nfrom PIL import Image\n\ndef task_func(img_path):\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(f\"No file found at {img_path}\")\n\n    # Open the image using PIL\n    img = Image.open(img_path)\n\n    # Convert the image to RGB (PIL images are in RGB by default)\n    rgb_img = img.convert(\"RGB\")\n\n    # Convert the RGB image to grayscale using OpenCV\n    gray_img = cv2.cvtColor(np.array(rgb_img), cv2.COLOR_RGB2GRAY)\n\n    # Apply binary thresholding to ensure the image is binary before finding contours\n    _, binary_img = cv2.threshold(gray_img, 127, 255, cv2.THRESH_BINARY)\n\n    # Find contours\n    contours, hierarchy = cv2.findContours(binary_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\n    return np.array(rgb_img), contours\n<end>"}
{"id": 144, "answer": " <start>\nimport cv2\nimport numpy as np\nimport os\nfrom PIL import Image\n\ndef task_func(img_path):\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(f\"No file found at {img_path}\")\n\n    # Open the image using PIL\n    img = Image.open(img_path)\n\n    # Convert the image to grayscale using PIL\n    gray_img = img.convert('L')\n\n    # Apply binary thresholding to ensure the image is binary before finding contours\n    _, binary_img = cv2.threshold(np.array(gray_img), 127, 255, cv2.THRESH_BINARY)\n\n    # Find contours\n    contours, hierarchy = cv2.findContours(binary_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\n    return np.array(img), contours\n<end>"}
{"id": 145, "answer": " <start>\nimport cv2\nimport numpy as np\nimport os\nimport PIL.Image\n\ndef task_func(img_path):\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(f\"No file found at {img_path}\")\n\n    # Open the image using PIL\n    img = PIL.Image.open(img_path)\n\n    # Convert the image to RGB (PIL images are BGR by default)\n    img = img.convert(\"RGB\")\n\n    # Convert the image to grayscale using OpenCV\n    gray_img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2GRAY)\n\n    # Find contours\n    contours, _ = cv2.findContours(gray_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\n    return np.array(img), contours\n<end>"}
{"id": 146, "answer": " <start>\nimport cv2\nimport numpy as np\nimport os\n\nimport numpy.lib.format as format\n\ndef read_array(fp):\n    return format.read_array(fp, allow_pickle=False)\n\ndef task_func(img_path):\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(f\"No file found at {img_path}\")\n\n    img = read_array(open(img_path, 'rb'))\n    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n    # Find contours\n    contours, _ = cv2.findContours(gray_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\n    return np.array(img), contours\n<end>"}
{"id": 147, "answer": " <start>\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndef task_func(df, target_column, column_to_remove=\"c\", test_size=0.2):\n    # Ensure the input is a DataFrame\n    df = pd.DataFrame(df)\n\n    # Drop the specified column if it exists in the dataframe\n    if column_to_remove in df.columns:\n        df = df.drop(columns=column_to_remove)\n\n    # Split the dataframe into training and test datasets\n    X = df.drop(columns=target_column)\n    y = df[target_column]\n\n    try:\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n    except ValueError as e:\n        raise ValueError(f\"Invalid test_size parameter: {e}\")\n\n    return X_train, X_test, y_train, y_test\n<end>"}
{"id": 148, "answer": " <start>\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndef task_func(df, target_column, column_to_remove=\"c\", test_size=0.2):\n    # Drop the specified column if it exists in the dataframe\n    df = df.drop(columns=[column_to_remove]) if column_to_remove in df.columns else df\n\n    # Split the dataframe into training and test datasets\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n\n    return X_train, X_test, y_train, y_test\n<end>"}
{"id": 149, "answer": " <start>\nimport numpy as np\nimport cv2\nimport os\nfrom PIL import Image\n\ndef task_func(image_path='image.jpg', threshold=128):\n    if not isinstance(threshold, int) or not (0 <= threshold <= 255):\n        raise ValueError(\"Threshold must be an integer between 0 and 255.\")\n\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"No image found at {image_path}\")\n\n    img = Image.open(image_path).convert('L')\n    img_array = np.array(img)\n\n    binary_img = np.where(img_array > threshold, 255, 0).astype('uint8')\n\n    return img_array, binary_img\n<end>"}
{"id": 150, "answer": " <start>\nimport numpy as np\nimport cv2\nimport os\nimport PIL.Image\n\ndef task_func(image_path='image.jpg', threshold=128):\n\n    if not isinstance(threshold, int) or not (0 <= threshold <= 255):\n        raise ValueError(\"Threshold must be an integer between 0 and 255.\")\n\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"No image found at {image_path}\")\n\n    img = PIL.Image.open(image_path).convert('L')\n    img_array = np.array(img)\n    binary_img = np.where(img_array > threshold, 255, 0).astype('uint8')\n\n    return img_array, binary_img\n<end>"}
{"id": 153, "answer": " <start>\nimport cv2\nimport os\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\ndef task_func(image_path='image.jpg', histogram_path='histogram.png'):\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"No image found at {image_path}\")\n\n    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n    img_pil = Image.fromarray(img)\n    hist, bins = np.histogram(img_pil, bins=256, range=[0, 256])\n\n    fig, ax = plt.subplots()\n    ax.hist(hist, bins=bins)\n    ax.set_title(\"Grayscale Histogram\")\n    ax.set_xlabel(\"Bins\")\n    ax.set_ylabel(\"# of Pixels\")\n    fig.savefig(histogram_path)\n    return ax\n<end>"}
{"id": 154, "answer": " <start>\nimport cv2\nimport os\nfrom matplotlib import pyplot as plt\nfrom matplotlib.image import imread\n\ndef task_func(image_path='image.jpg', histogram_path='histogram.png'):\n\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"No image found at {image_path}\")\n\n    img = imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n    hist = cv2.calcHist([img], [0], None, [256], [0, 256])\n\n    plt.figure()\n    plt.title(\"Grayscale Histogram\")\n    plt.xlabel(\"Bins\")\n    plt.ylabel(\"# of Pixels\")\n    axes = plt.plot(hist)[0].axes\n    plt.savefig(histogram_path)\n    return axes\n<end>"}
{"id": 155, "answer": " <start>\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef generate_normal_samples(mean, std_dev, n):\n    rng = np.random.default_rng()\n    samples = rng.normal(mean, std_dev, size=n)\n    return samples\n\ndef visualize_normal_distribution(mean, std_dev, samples):\n    plt.figure(figsize=(10, 6))\n    plt.hist(samples, bins=30, normed=True, alpha=0.6, color='g')\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mean, std_dev)\n    plt.plot(x, p, 'k', linewidth=2)\n\n    title = f'Normal Distribution: Mean = {mean}, Std Dev = {std_dev}'\n    plt.title(title)\n    plt.xlabel('Value')\n    plt.ylabel('Density')\n    plt.show()\n\ndef task_func(mean, std_dev, n):\n    samples = generate_normal_samples(mean, std_dev, n)\n    visualize_normal_distribution(mean, std_dev, samples)\n    return samples\n<end>"}
{"id": 156, "answer": " <start>\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(mean, std_dev, n):\n    samples = np.random.normal(mean, std_dev, n)\n\n    plt.figure(figsize=(10, 6))\n    plt.hist(samples, bins=30, normed=True, alpha=0.6, color='g')\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mean, std_dev)\n    plt.plot(x, p, 'k', linewidth=2)\n\n    title = f'Normal Distribution: Mean = {mean}, Std Dev = {std_dev}'\n    plt.title(title)\n    plt.xlabel('Value')\n    plt.ylabel('Density')\n    plt.show()\n\n    return samples\n<end>"}
{"id": 157, "answer": " <start>\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(mean, std_dev, n):\n    samples = norm.rvs(mean, std_dev, size=n)\n\n    plt.figure(figsize=(10, 6))\n    plt.hist(samples, bins=30, density=True, alpha=0.6, color='g')\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mean, std_dev)\n    plt.plot(x, p, 'k', linewidth=2)\n\n    title = f'Normal Distribution: Mean = {mean}, Std Dev = {std_dev}'\n    plt.title(title)\n    plt.xlabel('Value')\n    plt.ylabel('Density')\n    plt.show()\n\n    return samples\n<end>"}
{"id": 158, "answer": " <start>\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef generate_normal_samples(mean, std_dev, n):\n    samples = np.random.normal(mean, std_dev, n)\n    return samples\n\ndef plot_normal_distribution(samples):\n    plt.figure(figsize=(10, 6))\n    plt.hist(samples, bins=30, density=True, alpha=0.6, color='g')\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, samples.mean(), samples.std())\n    plt.plot(x, p, 'k', linewidth=2)\n\n    title = f'Normal Distribution: Mean = {samples.mean()}, Std Dev = {samples.std()}'\n    plt.title(title)\n    plt.xlabel('Value')\n    plt.ylabel('Density')\n    plt.show()\n\ndef task_func(mean, std_dev, n):\n    samples = generate_normal_samples(mean, std_dev, n)\n    plot_normal_distribution(samples)\n    return samples\n<end>"}
{"id": 159, "answer": " <start>\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(mean, std_dev, n):\n    samples = norm.rvs(mean, std_dev, size=n)\n\n    plt.figure(figsize=(10, 6))\n    plt.hist(samples, bins=30, normed=True, alpha=0.6, color='g')\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mean, std_dev)\n    plt.plot(x, p, 'k', linewidth=2)\n\n    title = f'Normal Distribution: Mean = {mean}, Std Dev = {std_dev}'\n    plt.title(title)\n    plt.xlabel('Value')\n    plt.ylabel('Density')\n    plt.show()\n\n    return samples\n<end>"}
{"id": 160, "answer": " <start>\nimport numpy as np\nfrom numpy.random import default_rng\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef generate_normal_samples(mean, std_dev, n):\n    rng = default_rng()\n    samples = rng.standard_normal(size=n)\n    return samples\n\ndef visualize_normal_distribution(mean, std_dev, samples):\n    plt.figure(figsize=(10, 6))\n    plt.hist(samples, bins=30, normed=True, alpha=0.6, color='g')\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mean, std_dev)\n    plt.plot(x, p, 'k', linewidth=2)\n\n    title = 'Normal Distribution: Mean = {0}, Std Dev = {1}'.format(mean, std_dev)\n    plt.title(title)\n    plt.xlabel('Value')\n    plt.ylabel('Density')\n    plt.show()\n\ndef task_func(mean, std_dev, n):\n    samples = generate_normal_samples(mean, std_dev, n)\n    visualize_normal_distribution(mean, std_dev, samples)\n    return samples\n<end>"}
{"id": 161, "answer": " <start>\nimport subprocess\nimport os\nimport time\nfrom datetime import datetime, timedelta\n\ndef task_func(script_dir, scripts, delay):\n    if delay < 0:\n        raise ValueError(\"delay cannot be negative.\")\n    if not scripts:\n        raise ValueError(\"No scripts provided.\")\n    start_times = []\n\n    for script in scripts:\n        script_path = os.path.join(script_dir, script)\n        if not os.path.exists(script_path):\n            raise FileNotFoundError(\"Script not found: {}\".format(script_path))\n\n        start_time = datetime.now()\n        start_times.append(start_time)\n\n        result = subprocess.call(script_path, shell=True)\n        if result != 0:\n            raise RuntimeError(\"Script execution failed: {}\".format(script_path))\n\n        time.sleep(delay)\n        start_time += timedelta(seconds=delay)\n        start_times.append(start_time)\n\n    return [start_time.strftime(\"%Y-%m-%d %H:%M:%S\") for start_time in start_times]\n<end>"}
{"id": 162, "answer": " <start>\nimport subprocess\nimport os\nimport time\nfrom datetime import datetime, timezone\n\ndef task_func(script_dir, scripts, delay):\n    if delay < 0:\n        raise ValueError(\"delay cannot be negative.\")\n    if not scripts:\n        raise ValueError(\"No scripts provided.\")\n\n    start_times = []\n    for script in scripts:\n        script_path = os.path.join(script_dir, script)\n        start_time = datetime.now(timezone.utc)\n        start_times.append(start_time)\n\n        result = subprocess.call(script_path, shell=True)\n        if result != 0:\n            raise FileNotFoundError(f\"Script not found: {script_path}\")\n\n        time.sleep(delay)\n    return [start_time.strftime(\"%Y-%m-%d %H:%M:%S\") for start_time in start_times]\n<end>"}
{"id": 163, "answer": " <start>\nimport subprocess\nimport psutil\nimport time\nimport os\n\ndef task_func(script_path: str, timeout=10) -> dict:\nif not os.path.exists(script_path):\nraise FileNotFoundError(f\"'{script_path}' does not exist.\")\n\n# Start the bash script process\np = subprocess.Popen([\"bash\", script_path])\npid = p.pid\n\n# Initialize resources\ntotal_cpu = 0.0\ntotal_memory = 0\n\nstart_time = time.time()\n\ntry:\n# Fetch the process using psutil\nprocess = psutil.Process(p)\n\n# Continuously fetch the process statistics\nwhile process.is_running():\n# Get the CPU and memory usage\ncpu_percent = process.cpu_percent(interval=0.05)\ntotal_cpu += cpu_percent\ntotal_memory += process.memory_info().rss\ntime.sleep(0.05)\n\n# Check for timeout\nif time.time() - start_time > timeout:\nbreak\nexcept (psutil.NoSuchProcess, psutil.ZombieProcess):\npass\nfinally:\nif p.poll() is None:\np.terminate()\np.wait()\n\nreturn {\"CPU Usage\": total_cpu, \"Memory Usage\": total_memory}\n</end>"}
{"id": 164, "answer": " <start>\nimport subprocess\nimport psutil\nimport time\nimport os\n\ndef task_func(script_path: str, timeout=10) -> dict:\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"'{script_path}' does not exist.\")\n\n    # Start the bash script process\n    p = subprocess.Popen([\"bash\", script_path])\n    pid = p.pid\n\n    # Initialize resources\n    total_cpu = 0.0\n    total_memory = 0\n\n    start_time = time.time()\n\n    try:\n        # Fetch the process using psutil\n        process = psutil.Process(pid)\n\n        # Continuously fetch the process statistics\n        while process.is_running():\n            # Get the CPU and memory usage\n            cpu_percent, memory_full_info = process.cpu_percent(interval=0.05), process.memory_full_info()\n            total_cpu += cpu_percent\n            total_memory += memory_full_info.rss\n            time.sleep(0.05)\n\n            # Check for timeout\n            if time.time() - start_time > timeout:\n                break\n    except (psutil.NoSuchProcess, psutil.ZombieProcess):\n        pass\n    finally:\n        if p.poll() is None:\n            p.terminate()\n            p.wait()\n\n    return {\"CPU Usage\": total_cpu, \"Memory Usage\": total_memory}\n<end>"}
{"id": 165, "answer": " <start>\nimport subprocess\nimport psutil\nimport time\nimport os\n\ndef task_func(script_path: str, timeout=10) -> dict:\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(\"'{}' does not exist.\".format(script_path))\n\n    # Start the bash script process\n    p = subprocess.Popen([\"bash\", script_path])\n    pid = p.pid\n\n    # Initialize resources\n    total_cpu = 0.0\n    total_memory = 0\n\n    start_time = time.time()\n\n    try:\n        # Fetch the process using psutil\n        process = psutil.Process(pid)\n\n        # Continuously fetch the process statistics\n        while process.is_running():\n            # Get the CPU and memory usage\n            cpu_percent, memory_info = process.cpu_full_info(), process.memory_full_info()\n            total_cpu += cpu_percent.cpu_percent\n            total_memory += memory_info.rss\n            time.sleep(0.05)\n\n            # Check for timeout\n            if time.time() - start_time > timeout:\n                break\n    except (psutil.NoSuchProcess, psutil.ZombieProcess):\n        pass\n    finally:\n        if p.poll() is None:\n            p.terminate()\n            p.wait()\n\n    return {\"CPU Usage\": total_cpu, \"Memory Usage\": total_memory}\n<end>"}
{"id": 166, "answer": " <start>\nimport subprocess\nimport time\nimport psutil\n\ndef task_func(script_path: str, timeout=10) -> dict:\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"'{script_path}' does not exist.\")\n\n    # Start the bash script process\n    p = subprocess.Popen([\"bash\", script_path])\n    pid = p.pid\n\n    # Initialize resources\n    total_cpu = 0.0\n    total_memory = 0\n\n    start_time = time.time()\n\n    try:\n        # Fetch the process using psutil\n        process = psutil.Process(pid)\n\n        # Continuously fetch the process statistics\n        while process.is_running():\n            # Get the CPU and memory usage\n            cpu_percent, memory_info = process.cpu_full_info(), process.memory_full_info()\n            total_cpu += cpu_percent.cpu_percent\n            total_memory += memory_info.rss\n            time.sleep(0.05)\n\n            # Check for timeout\n            if time.time() - start_time > timeout:\n                break\n    except (psutil.NoSuchProcess, psutil.ZombieProcess):\n        pass\n    finally:\n        if p.poll() is None:\n            p.terminate()\n            p.wait()\n\n    return {\"CPU Usage\": total_cpu, \"Memory Usage\": total_memory}\n<end>"}
{"id": 167, "answer": " <start>\nimport subprocess\nimport psutil\nimport time\nimport os\n\ndef task_func(script_path: str, timeout=10) -> dict:\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"'{script_path}' does not exist.\")\n\n    # Start the bash script process\n    p = subprocess.Popen([\"bash\", script_path])\n    pid = p.pid\n\n    # Initialize resources\n    total_cpu = 0.0\n    total_memory = 0\n\n    start_time = time.time()\n\n    try:\n        # Fetch the process using psutil\n        process = psutil.Process(pid)\n\n        # Continuously fetch the process statistics\n        while process.is_running():\n            # Get the CPU and memory usage\n            cpu_percent, mem_info = process.cpu_full_info(), process.memory_full_info()\n            total_cpu += cpu_percent.cpu_percent\n            total_memory += mem_info.rss\n            time.sleep(0.05)\n\n            # Check for timeout\n            if time.time() - start_time > timeout:\n                break\n    except (psutil.NoSuchProcess, psutil.ZombieProcess):\n        pass\n    finally:\n        if p.poll() is None:\n            p.terminate()\n            p.wait()\n\n    return {\"CPU Usage\": total_cpu, \"Memory Usage\": total_memory}\n<end>"}
{"id": 168, "answer": " <start>\nimport subprocess\nimport psutil\nimport time\nimport os\n\ndef task_func(script_path: str, timeout=10) -> dict:\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"'{script_path}' does not exist.\")\n\n    # Start the bash script process\n    p = subprocess.Popen([\"bash\", script_path])\n    pid = p.pid\n\n    # Initialize resources\n    total_cpu = 0.0\n    total_memory = 0\n\n    start_time = time.time()\n\n    try:\n        # Fetch the process using psutil\n        process = psutil.Process(pid)\n\n        # Continuously fetch the process statistics\n        while process.is_running():\n            # Get the CPU and memory usage\n            cpu_percent, memory_info = process.cpu_percent(interval=0.05), process.memory_info()\n            total_cpu += cpu_percent\n            total_memory += memory_info.rss\n            time.sleep(0.05)\n\n            # Check for timeout\n            if time.time() - start_time > timeout:\n                break\n    except (psutil.NoSuchProcess, psutil.ZombieProcess):\n        pass\n    finally:\n        if p.poll() is None:\n            p.terminate()\n            p.wait()\n\n    # Get additional memory metrics\n    memory_full_info = process.memory_full_info()\n    total_memory += memory_full_info.uss + memory_full_info.pss\n\n    return {\"CPU Usage\": total_cpu, \"Memory Usage\": total_memory}\n<end>"}
{"id": 169, "answer": " <start>\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections.abc import Iterable\nfrom matplotlib.axes import Subplot\n\ndef task_func(student_grades: Iterable, possible_grades: Iterable = [\"A\", \"B\", \"C\", \"D\", \"F\"]):\n    if not student_grades:\n        raise ValueError(\"student_grades cannot be empty\")\n\n    # Ensure grades are case-insensitive and unique\n    possible_grades = {g.upper() for g in possible_grades}\n\n    # Count the grades, ignoring those not in possible_grades\n    grade_counts = {grade: sum(g.upper() == grade.upper() for g in student_grades) for grade in possible_grades}\n\n    # Create a dictionary with counts for each possible grade\n    report_data = {grade: count for grade, count in grade_counts.items()}\n\n    # Create DataFrame without using 'columns' argument\n    report_df = pd.DataFrame(list(report_data.items()), index=report_data.keys())\n\n    # Plot the bar chart\n    fig, ax = plt.subplots(figsize=(8, 6))\n    report_df.plot(kind=\"bar\", ax=ax, legend=False, title=\"Grade Distribution\")\n    ax.set_ylabel(\"Number of Students\")\n    ax.set_xlabel(\"Grade\")\n\n    plt.tight_layout()\n\n    return report_df, ax\n<end>"}
{"id": 170, "answer": " <start>\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections.abc import Iterable\n\ndef task_func(student_grades, possible_grades=None):\n    if not student_grades:\n        raise ValueError(\"student_grades cannot be empty\")\n\n    if possible_grades is None:\n        possible_grades = [\"A\", \"B\", \"C\", \"D\", \"F\"]\n    else:\n        possible_grades = [g.upper() for g in possible_grades]\n\n    grade_counts = Counter(student_grades)\n    report_data = {grade: grade_counts[grade] for grade in possible_grades}\n\n    report_df = pd.DataFrame.from_dict(report_data, orient=\"index\")\n    report_df.index.name = \"Grade\"\n    report_df[\"Count\"] = report_df.index\n\n    ax = report_df.plot(kind=\"bar\", legend=False, title=\"Grade Distribution\")\n    ax.set_ylabel(\"Number of Students\")\n    ax.set_xlabel(\"Grade\")\n\n    plt.tight_layout()\n\n    return report_df, ax\n<end>"}
{"id": 171, "answer": " <start>\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\ndef task_func(n_samples=1000, mu=0, sigma=1, random_seed=0):\n    if n_samples <= 0 or sigma <= 0:\n        raise ValueError(\"Invalid n_samples or sigma\")\n    np.random.seed(random_seed)\n    plt.figure()\n    samples = np.random.normal(mu, sigma, n_samples)\n    _, _, patches = plt.hist(samples, bins=30, density=False)\n    ax = plt.gca()\n    ax.plot(\n        np.linspace(mu - 4 * sigma, mu + 4 * sigma, 1000),\n        norm.pdf(np.linspace(mu - 4 * sigma, mu + 4 * sigma, 1000), mu, sigma),\n        linewidth=2,\n        color=\"r\",\n    )\n    return ax, samples\n<end>"}
{"id": 172, "answer": " <start>\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\ndef generate_normal_histogram(n_samples=1000, mu=0, sigma=1, random_seed=0):\n    if n_samples <= 0 or sigma <= 0:\n        raise ValueError(\"Invalid n_samples or sigma\")\n\n    np.random.seed(random_seed)\n    samples = norm.rvs(mu, sigma, size=n_samples)\n\n    fig, ax = plt.subplots()\n    ax.hist(samples, bins=30, density=True)\n\n    ax.plot(\n        norm.ppf(np.linspace(0, 1, 1000), loc=mu, scale=sigma),\n        norm.pdf(np.linspace(0, 1, 1000), loc=mu, scale=sigma),\n        linewidth=2,\n        color=\"r\",\n    )\n\n    return ax, samples\n<end>"}
{"id": 173, "answer": " <start>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    np.random.seed(seed)\n\n    if N == 0:\n        df = pd.DataFrame(columns=[\"x\", \"y\", \"category\"])\n        fig, ax = plt.subplots()\n        return df, ax\n\n    if N < len(CATEGORIES):\n        all_categories = np.random.choice(CATEGORIES, N, replace=False)\n    else:\n        all_categories = np.array(CATEGORIES)\n        np.random.shuffle(all_categories)\n\n    df = pd.DataFrame({\"x\": np.random.rand(N), \"y\": np.random.rand(N), \"category\": all_categories})\n\n    fig, ax = plt.subplots()\n    df[\"category\"].astype(str).apply(pd.Series).stack().reset_index().groupby(\"level_1\").mean().plot(kind=\"scatter\", ax=ax)\n    ax.legend()\n\n    return df, ax\n<end>"}
{"id": 175, "answer": " <start>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    np.random.seed(seed)\n\n    if N == 0:\n        df = pd.DataFrame(columns=[\"x\", \"y\", \"category\"])\n        fig, ax = plt.subplots()\n        return df, ax\n\n    if N < len(CATEGORIES):\n        all_categories = np.random.choice(CATEGORIES, N, replace=False)\n    else:\n        all_categories = np.concatenate([np.array(CATEGORIES), np.random.choice(CATEGORIES, N - len(CATEGORIES), replace=True)])\n        np.random.shuffle(all_categories)\n\n    df = pd.DataFrame(\n        {\"x\": np.random.rand(N), \"y\": np.random.rand(N), \"category\": all_categories},\n        columns=[\"x\", \"y\", \"category\"]  # Ensure the order of columns\n    )\n\n    fig, ax = plt.subplots()\n    df[\"category\"].astype(str).apply(lambda x: ax.scatter(df[df[\"category\"] == x][\"x\"], df[df[\"category\"] == x][\"y\"], label=x))\n\n    ax.legend()\n\n    return df, ax\n<end>"}
{"id": 183, "answer": " <start>\nimport xlwt\nimport os\nfrom collections import OrderedDict\n\n# Constants\nFIELDS = ['ID', 'Name', 'Age']\n\ndef task_func(values, filename):\n    book = xlwt.Workbook()\n    sheet1 = book.add_sheet(\"persons\")\n\n    # Write header\n    for col_index, col in enumerate(FIELDS):\n        sheet1.write(0, col_index, col)\n\n    # Write data rows\n    for row_index, row_data in enumerate(values, 1):\n        row = sheet1.add_row(row_data, style=xlwt.XFStyle())\n\n    book.save(filename)\n\n    return os.path.abspath(filename)\n<end>"}
{"id": 184, "answer": " <start>\nimport xlwt\nimport os\nfrom collections import OrderedDict\n\n# Constants\nFIELDS = ['ID', 'Name', 'Age']\n\ndef task_func(values, filename):\n    book = xlwt.Workbook()\n    sheet1 = book.add_sheet(\"persons\")\n\n    # Write header\n    for col_index, col in enumerate(FIELDS):\n        sheet1.write(0, col_index, col, style=xlwt.easyxf('font: bold on'))\n\n    # Write data rows\n    for row_index, row_values in enumerate(values, 1):\n        row = sheet1.add_row(row_values, style=xlwt.easyxf('align: wrap on, vert center'))\n\n    book.save(filename)\n\n    return os.path.abspath(filename)\n<end>"}
{"id": 187, "answer": " <start>\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.cbook import index_of\n\ndef task_func(data):\n    # Check if input is a list of dictionaries\n    if not isinstance(data, list) or not all(isinstance(item, dict) for item in data):\n        raise TypeError(\"Input must be a list of dictionaries.\")\n\n    # Check if all values in dictionaries are integers\n    for item in data:\n        for key, value in item.items():\n            if not isinstance(value, (int, float)):\n                raise TypeError(\"All values in dictionaries must be integers or floats.\")\n\n    df = pd.DataFrame(data)\n    df.fillna(0, inplace=True)\n\n    # Generate x values using matplotlib.cbook.index_of\n    x = index_of(df.index)\n\n    # Plot data\n    df.plot(x='', y='', kind='line', x=x, legend=True)\n\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Sales Quantity\")\n    plt.title(\"Fruit Sales over Time\")\n    plt.legend()\n    return plt.gca()\n<end>"}
{"id": 188, "answer": " <start>\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    df = pd.DataFrame(data).fillna(0)\n    df.plot(x=\"index\", y=\"Sales Quantity\")\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Sales Quantity\")\n    plt.title(\"Fruit Sales over Time\")\n    plt.legend()\n    return plt.gca()\n<end>"}
{"id": 191, "answer": " <start>\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections.abc import Iterable\n\ndef task_func(data):\n    if not data:\n        raise ValueError(\"Input data is empty.\")\n    if not isinstance(data, Iterable):\n        raise TypeError(\"Input must be a list or other iterable.\")\n    for d in data:\n        if not isinstance(d, dict):\n            raise TypeError(\"All elements in the iterable must be dictionaries.\")\n        if not all(isinstance(value, (int, float)) for value in d.values()):\n            raise TypeError(\"All values in the dictionaries must be numeric.\")\n\n    stats = {k: [] for k in set(itertools.chain.from_iterable(data.values()))}\n    for d in data:\n        for key, value in d.items():\n            stats[value].append(key)\n\n    result = {k: {\"mean\": np.mean(v), \"std\": np.std(v)} for k, v in stats.items()}\n\n    # Visualization\n    axes = []\n    for key in result:\n        fig, ax = plt.subplots()\n        ax.bar([\"mean\", \"std\"], [result[key][\"mean\"], result[key][\"std\"]])\n        ax.set_title(f\"Statistics of {key}\")\n        ax.set_ylabel(\"Value\")\n        axes.append(ax)\n\n    return result, axes\n```\n<end>"}
{"id": 192, "answer": " <start>\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections.defaultdict import defaultdict\n\ndef task_func(data):\n    if not data:\n        raise ValueError(\"Input data is empty.\")\n    if not all(isinstance(d, dict) for d in data):\n        raise TypeError(\"Input must be a list of dictionaries.\")\n    for d in data:\n        if not all(isinstance(value, (int, float)) for value in d.values()):\n            raise TypeError(\"All values in the dictionaries must be numeric.\")\n\n    stats = defaultdict(list)\n    for d in data:\n        for key, value in d.items():\n            stats[key].append(value)\n\n    result = {k: {\"mean\": np.mean(v), \"std\": np.std(v)} for k, v in stats.items()}\n\n    # Visualization\n    fig, axes = plt.subplots(len(result))\n    for i, key in enumerate(result):\n        ax = axes[i]\n        ax.bar([\"mean\", \"std\"], [result[key][\"mean\"], result[key][\"std\"]])\n        ax.set_title(f\"Statistics of {key}\")\n        ax.set_ylabel(\"Value\")\n\n    return result, axes\n```\n<end>"}
{"id": 193, "answer": " <start>\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections.defaultdict import defaultdict\n\ndef task_func(data):\n    if not data:\n        raise ValueError(\"Input data is empty.\")\n    if not all(isinstance(d, dict) for d in data):\n        raise TypeError(\"Input must be a list of dictionaries.\")\n    for d in data:\n        if not all(isinstance(value, (int, float)) for value in d.values()):\n            raise TypeError(\"All values in the dictionaries must be numeric.\")\n\n    stats = defaultdict(list)\n    for d in data:\n        for key, value in d.items():\n            stats[key].append(value)\n\n    result = {k: {\"mean\": np.mean(v), \"std\": np.std(v)} for k, v in stats.items()}\n\n    # Visualization\n    fig, axes = plt.subplots(len(result))\n    for i, key in enumerate(result):\n        ax = axes[i]\n        ax.bar([0, 1], [result[key][\"mean\"], result[key][\"std\"]], tick_label=[\"mean\", \"std\"])\n        ax.set_title(f\"Statistics of {key}\")\n        ax.set_ylabel(\"Value\")\n\n    return result, axes\n```\n<end>"}
{"id": 194, "answer": " <start>\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections.defaultdict import defaultdict\n\ndef task_func(data):\n    if not data:\n        raise ValueError(\"Input data is empty.\")\n    if not all(isinstance(d, dict) for d in data):\n        raise TypeError(\"Input must be a list of dictionaries.\")\n    for d in data:\n        if not all(isinstance(value, (int, float)) for value in d.values()):\n            raise TypeError(\"All values in the dictionaries must be numeric.\")\n\n    stats = defaultdict(list)\n    for d in data:\n        for key, value in d.items():\n            stats[key].append(value)\n\n    result = {k: {\"mean\": np.mean(v), \"std\": np.std(v)} for k, v in stats.items()}\n\n    # Visualization\n    axes = []\n    for key in result:\n        fig, ax = plt.subplots()\n        ax.bar(**matplotlib.projections.HammerAxes.bar_label(\n            container=matplotlib.pyplot.Axes.bar(left=np.array([0, 1]), height=[result[key][\"mean\"], result[key][\"std\"]]),\n            labels=[\"mean\", \"std\"],\n            fmt=\"%g\",\n            label_type=\"edge\",\n            padding=0\n        ))\n        ax.set_title(f\"Statistics of {key}\")\n        ax.set_ylabel(\"Value\")\n        axes.append(ax)\n\n    return result, axes\n```\n<end>"}
{"id": 195, "answer": " <start>\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections.defaultdict import defaultdict\n\ndef task_func(data):\n    if not data:\n        raise ValueError(\"Input data is empty.\")\n    if not all(isinstance(d, dict) for d in data):\n        raise TypeError(\"Input must be a list of dictionaries.\")\n\n    stats = defaultdict(list)\n    for d in data:\n        if not all(isinstance(value, (int, float)) for value in d.values()):\n            raise TypeError(\"All values in the dictionaries must be numeric.\")\n\n        for key, value in d.items():\n            stats[key].append(value)\n\n    result = {k: {\"mean\": np.mean(v), \"std\": np.std(v)} for k, v in stats.items()}\n\n    # Visualization\n    fig, axes = plt.subplots(len(result))\n    for i, key in enumerate(result):\n        ax = axes[i]\n        ax.bar(x=[\"mean\", \"std\"], height=result[key].values())\n        ax.set_title(f\"Statistics of {key}\")\n        ax.set_ylabel(\"Value\")\n\n    return result, axes\n```\n<end>"}
{"id": 196, "answer": " <start>\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections.abc import Iterable\n\ndef task_func(data):\n    if not data:\n        raise ValueError(\"Input data is empty.\")\n    if not isinstance(data, Iterable):\n        raise TypeError(\"Input must be a list or other iterable.\")\n    for d in data:\n        if not isinstance(d, dict):\n            raise TypeError(\"All elements in the iterable must be dictionaries.\")\n        if not all(isinstance(value, (int, float)) for value in d.values()):\n            raise TypeError(\"All values in the dictionaries must be numeric.\")\n\n    stats = {k: [] for k in set(itertools.chain.from_iterable(data.values()))}\n    for d in data:\n        for key, value in d.items():\n            stats[value].append(key)\n\n    result = {k: {\"mean\": np.mean(v), \"std\": np.std(v)} for k, v in stats.items()}\n\n    # Visualization\n    axes = []\n    for key in result:\n        fig, ax = plt.subplots()\n        ax.bar(x=[\"mean\", \"std\"], height=result[key].values())\n        ax.set_xticks(np.arange(len(result[key].keys())))\n        ax.set_xticklabels(result[key].keys())\n        ax.set_title(f\"Statistics of {key}\")\n        ax.set_ylabel(\"Value\")\n        axes.append(ax)\n\n    return result, axes\n```\n</end>"}
{"id": 197, "answer": " <start>\nimport json\nimport numpy as np\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\nfrom matplotlib.pylab import Subplot\n\ndef task_func(input_file):\n    with open(input_file, \"r\") as f:\n        data = json.load(f)\n\n    stats = defaultdict(list)\n    for d in data:\n        for key, value in d.items():\n            stats[key].append(value)\n\n    result = {k: {\"mean\": np.mean(v), \"median\": np.median(v)} for k, v in stats.items()}\n\n    plots = []\n    for key, values in result.items():\n        fig, ax = plt.subplots()\n        ax = Subplot(fig, 110)\n        fig.wcs.set_subimage(ax)\n        ax.bar([0, 1], [values[\"mean\"], values[\"median\"]], tick_label=[\"mean\", \"median\"])\n        ax.set_title(f\"Statistics of {key}\")\n        plots.append(ax)\n    return result, plots\n<end>"}
{"id": 198, "answer": " <start>\nimport json\nimport numpy as np\nfrom collections import defaultdict\nimport matplotlib.pylab as pylab\n\ndef task_func(input_file):\n    with open(input_file, \"r\") as f:\n        data = json.load(f)\n\n    stats = defaultdict(list)\n    for d in data:\n        for key, value in d.items():\n            stats[key].append(value)\n\n    result = {k: {\"mean\": np.mean(v), \"median\": np.median(v)} for k, v in stats.items()}\n\n    plots = []\n    for key, values in result.items():\n        _, ax = pylab.subplots()\n        ax.bar([\"mean\", \"median\"], [values[\"mean\"], values[\"median\"]])\n        ax.set_title(f\"Statistics of {key}\")\n        plots.append(ax)\n\n    pylab.show()\n    return result\n<end>"}
{"id": 199, "answer": " <start>\nimport csv\nfrom collections.abc import Iterable\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.cbook import index_of\n\ndef task_func(file_path):\n    # Strip the file_path and then check its extension\n    file_path = file_path.strip()\n    if not file_path.lower().endswith(\".csv\"):\n        raise ValueError(\"Invalid file format. Only .csv files are accepted.\")\n\n    # Read the CSV file\n    with open(file_path, \"r\") as f:\n        reader = csv.reader(f)\n        rows = list(reader)\n\n    # Use Counter to get duplicates\n    duplicates = {}\n    for row in rows:\n        if rows.count(row) > 1:\n            if row in duplicates:\n                duplicates[row] += 1\n            else:\n                duplicates[row] = 1\n\n    # Plot the duplicates using matplotlib\n    ax = None\n    if duplicates:\n        df = pd.DataFrame(list(duplicates.items()), columns=['Row', 'Count'])\n        x, y = index_of(df['Row']) if isinstance(df['Row'], Iterable) else None, df['Count']\n        ax = df.plot(x=x, y=y, kind=\"bar\", legend=False, title=\"Duplicate Entries\")\n        ax.set_ylabel(\"Count\")\n        plt.tight_layout()\n\n    return duplicates, ax\n<end>"}
{"id": 200, "answer": " <start>\nimport csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(file_path):\n    # Strip the file_path and then check its extension\n    file_path = file_path.strip()\n    if not file_path.lower().endswith(\".csv\"):\n        raise ValueError(\"Invalid file format. Only .csv files are accepted.\")\n\n    # Read the CSV file\n    with open(file_path, \"r\") as f:\n        reader = csv.reader(f)\n        rows = list(reader)\n\n    # Use pandas DataFrame to get duplicates\n    df = pd.DataFrame(rows)\n    duplicates = df.duplicated(keep=False)\n    duplicates = duplicates[duplicates].groupby(duplicates).size()\n\n    # Plot the duplicates using matplotlib\n    ax = None\n    if duplicates:\n        ax = duplicates.plot(kind=\"bar\", legend=False, title=\"Duplicate Entries\")\n        ax.set_ylabel(\"Count\")\n        plt.tight_layout()\n\n    return duplicates.to_dict(), ax\n<end>"}
{"id": 201, "answer": " <start>\nfrom collections import Counter\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef task_func(df, n_clusters=3, random_state=None, n_init=10):\n    # Identify duplicates\n    duplicates = df[df.duplicated(subset=[\"x\", \"y\"], keep=False)]\n    duplicates_counter = Counter(duplicates[[\"x\", \"y\"]].values.tolist())\n\n    # Remove duplicates and perform KMeans clustering on unique points\n    unique_df = df.drop_duplicates(subset=[\"x\", \"y\"]).copy()\n\n    # Adjust n_clusters if unique data points are fewer than desired clusters\n    n_clusters = min(n_clusters, len(unique_df))\n\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    unique_df[\"cluster\"] = kmeans.fit_transform(unique_df[[\"x\", \"y\"]])\n\n    # Plot clustered data\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(unique_df[\"x\"], unique_df[\"y\"], c=unique_df[\"cluster\"])\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n    ax.set_title(\"KMeans Clusters\")\n\n    return duplicates_counter, unique_df, ax\n<end>"}
{"id": 202, "answer": " <start>\nfrom collections import Counter\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef task_func(df, n_clusters=3, random_state=None, n_init=10):\n    # Identify duplicates\n    duplicates = df[df.duplicated(subset=[\"x\", \"y\"], keep=False)]\n    duplicates_counter = Counter(map(tuple, duplicates[[\"x\", \"y\"]].values))\n\n    # Remove duplicates and perform KMeans clustering on unique points\n    unique_df = df.drop_duplicates(subset=[\"x\", \"y\"]).copy()\n\n    # Adjust n_clusters if unique data points are fewer than desired clusters\n    n_clusters = min(n_clusters, len(unique_df))\n\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    unique_df[\"cluster\"] = kmeans.fit_predict(unique_df[[\"x\", \"y\"]])\n\n    # Plot clustered data\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(unique_df[\"x\"], unique_df[\"y\"], c=unique_df[\"cluster\"])\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n    ax.set_title(\"KMeans Clusters\")\n\n    return duplicates_counter, unique_df, ax\n<end>"}
{"id": 203, "answer": " <start>\nimport sqlite3\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(db_name=\"test.db\", table_name=\"People\"):\n    conn = sqlite3.connect(db_name)\n    df = pd.read_sql_table(table_name, conn)\n\n    if df.empty:\n        # Return an empty plot if the table is empty\n        fig, ax = plt.subplots()\n        ax.set_xlabel(\"age\")\n        return ax\n\n    if df[\"age\"].min() < 0:\n        raise ValueError(\"Data contains negative age values.\")\n\n    ax = sns.distplot(df[\"age\"], bins=30, kde=True)\n    ax.set_xlabel(\"age\")\n    return ax\n<end>"}
{"id": 204, "answer": " <start>\nimport sqlite3\nimport pandas as pd\nimport seaborn as sns\nfrom pandas.plotting._matplotlib.converter import Series\n\ndef task_func(db_name=\"test.db\", table_name=\"People\"):\n    conn = sqlite3.connect(db_name)\n    df = pd.read_sql_query(f\"SELECT age from {table_name}\", conn)\n\n    if (df[\"age\"] < 0).any():\n        raise ValueError(\"Data contains negative age values.\")\n\n    series = Series(df)\n    series.rename_axis(\"age\")\n    ax = sns.histplot(data=series, x=\"age\", bins=30, kde=True)\n    ax.set_xlabel(\"age\")\n    return ax\n<end>"}
{"id": 205, "answer": " <start>\nimport numpy as np\nfrom difflib import SequenceMatcher\nimport matplotlib.fontconfig_pattern as mfcp\nimport matplotlib.pyplot as plt\n\ndef task_func(s_list, plot_path=None):\n    if not isinstance(s_list, list) or not all(isinstance(item, str) for item in s_list):\n        raise ValueError(\"All items in s_list must be strings.\")\n\n    if len(s_list) == 1:\n        return [np.nan]\n\n    avg_scores = []\n    for s in s_list:\n        scores = [mfcp.SequenceMatcher(None, s, other_s).run_tests(str(other_s), parse_all=True).results for other_s in s_list if s != other_s]\n        matches = [result[1] for result in scores if result[0]]\n        avg_score = np.mean([match.aslist() for match in matches]) if matches else np.nan\n        avg_scores.append(avg_score)\n\n    if plot_path:\n        plt.figure()\n        plt.bar(range(len(s_list)), avg_scores, tick_label=s_list)\n        plt.xlabel('Strings')\n        plt.ylabel('Average Similarity Score')\n        plt.title('Average Similarity Scores of Strings')\n        plt.savefig(plot_path)\n        plt.close()\n\n    return avg_scores\n<end>"}
{"id": 206, "answer": " <start>\nimport numpy as np\nfrom difflib import SequenceMatcher\nimport matplotlib.pyplot as plt\n\ndef task_func(s_list, plot_path=None):\n    if not all(isinstance(item, str) for item in s_list):\n        raise ValueError(\"All items in s_list must be strings.\")\n\n    avg_scores = []\n    for s in s_list:\n        scores = [SequenceMatcher(None, s, other_s).ratio() for other_s in s_list if s != other_s]\n        avg_score = np.mean(scores)\n        avg_scores.append(avg_score)\n\n    if plot_path:\n        plt.bar(s_list, avg_scores)\n        plt.savefig(plot_path)\n\n    return avg_scores\n<end>"}
{"id": 207, "answer": " <start>\n```python\nimport datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom matplotlib.dates import MonthLocator, DateFormatter\n\ndef task_func(data):\n    # Handle empty data\n    if not data.strip():\n        raise ValueError(\"The provided data string is empty.\")\n\n    # Parse the data and convert to DataFrame\n    df = pd.read_csv(pd.compat.StringIO(data), sep=',', parse_dates=['Month'], dayfirst=True)\n    df = df.set_index('Month')\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n\n    # Set the x-axis to display months\n    ax.xaxis.set_major_locator(MonthLocator())\n    ax.xaxis.set_major_formatter(DateFormatter('%b'))\n\n    # Plot the data\n    ax.bar(df.index, df['Value'])\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Value')\n    ax.set_title(f\"Monthly Data for {df.index[0].year}\")\n    plt.xticks(rotation='vertical')\n\n    return ax\n```\n<end>"}
{"id": 208, "answer": " <start>\n```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(data):\n    # Handle empty data\n    if not data.strip():\n        raise ValueError(\"The provided data string is empty.\")\n\n    # Convert data to DataFrame\n    df = pd.DataFrame([list(map(lambda x: (x.split('-')[0], int(x.split('-')[1])), data.split(',')))], columns=['Year', 'Month', 'Value'])\n    df = df.set_index(['Year', 'Month']).resample('A').sum().reset_index()\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax = df.plot(x='Month', y='Value', kind='bar')\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Value')\n    ax.set_title(f\"Monthly Data for {df['Year'].iloc[0]}\")\n    ax.xaxis.set_ticklabels(ax.xaxis.get_ticklabels(), rotation=90)\n\n    return ax\n```\n<end>"}
{"id": 211, "answer": " <start>\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\n\ndef task_func(size=1000):\n    data = np.random.normal(size=size)\n\n    rng = np.random.default_rng()\n    triangular_data = rng.triangular(0, 0, 1, size=size)\n\n    bin_edges = np.histogram_bin_edges(data, bins='auto')\n    number_of_bins = len(bin_edges) - 1\n\n    fig, ax = plt.subplots()\n    ax.hist(data, bins=number_of_bins, density=True, alpha=0.6, color='g')\n\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, size)\n    p = norm.pdf(x, np.mean(data), np.std(data))\n    ax.plot(x, p, 'k', linewidth=2)\n\n    ax.fill_between(triangular_data, np.zeros_like(triangular_data), np.ones_like(triangular_data), alpha=0.2, color='r')\n\n    return fig\n<end>"}
{"id": 212, "answer": " <start>\nimport numpy as np\nfrom scipy.stats import rv_histogram, norm\nimport matplotlib.pyplot as plt\n\ndef task_func(size=1000):\n    data = np.random.randn(size)\n    histogram, _ = np.histogram(data, bins='auto')\n    hist_dist = rv_histogram(histogram)\n\n    fig, ax = plt.subplots()\n    ax.hist(data, bins=histogram.size, density=True, alpha=0.6, color='g')\n\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, size)\n    p = hist_dist.pdf(x)\n    ax.plot(x, p, 'k', linewidth=2)\n\n    return fig\n<end>"}
{"id": 217, "answer": " <start>\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nGOALS_RANGE = (-10, 10)\n\ndef calculate_scores(goals, penalties):\n    scores = []\n    for team in TEAMS:\n        score = goals.get(team, 0) - penalties.get(team, 0)\n        scores.append(min(max(score, *GOALS_RANGE), *GOALS_RANGE))\n    return scores\n\ndef visualize_scores(scores):\n    df = pd.DataFrame(list(zip(TEAMS, scores)), columns=['Team', 'Score'])\n\n    plt.figure(figsize=(10, 6))\n    plt.bar(range(len(scores)), scores, color='skyblue')\n    plt.xticks(range(len(scores)), TEAMS)\n    plt.xlabel('Team')\n    plt.ylabel('Score')\n    plt.title('Team Scores Distribution')\n    plt.ylim(GOALS_RANGE[0] - 1, GOALS_RANGE[1] + 1)\n    plt.grid(axis='y', linestyle='--')\n    plt.show()\n\ndef task_func(goals, penalties):\n    scores = calculate_scores(goals, penalties)\n    scores_df = pd.DataFrame(scores, columns=['Score'])\n    scores_df['Team'] = TEAMS\n    return scores_df\n<end>"}
{"id": 218, "answer": " <start>\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nGOALS_RANGE = (-10, 10)\n\ndef task_func(goals, penalties):\n\n    scores_data = []\n\n    for team in TEAMS:\n        team_goals = goals.get(team, 0)\n        team_penalties = penalties.get(team, 0)\n        score = team_goals - team_penalties\n        scores_data.append([team, score])\n\n    scores_df = pd.DataFrame(scores_data, columns=['Team', 'Score'])\n    scores_df['Score'] = scores_df['Score'].clip(*GOALS_RANGE)\n\n    #Plotting\n    plt.figure(figsize=(10, 6))\n    scores_df.plot(x='Team', y='Score', kind='bar', color='skyblue')\n    plt.xlabel('Team')\n    plt.ylabel('Score')\n    plt.title('Team Scores Distribution')\n    plt.ylim(GOALS_RANGE[0] - 1, GOALS_RANGE[1] + 1)\n    plt.grid(axis='y', linestyle='--')\n    plt.show()\n\n    return scores_df\n<end>"}
{"id": 219, "answer": " <start>\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import norm\n\ndef task_func(L):\n    data = np.concatenate(L)\n    mu, std = norm.fit(data)\n\n    fig, ax = plt.subplots()\n    ax.hist(data, bins=30, density=True, alpha=0.6, color='g')\n\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, std)\n    ax.plot(x, p, 'k', linewidth=2)\n    title = f\"Fit results: mu = {mu:.2f},  std = {std:.2f}\"\n    ax.set_title(title)\n\n    return ax\n<end>"}
{"id": 220, "answer": " <start>\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import rv_histogram as Histogram\nfrom scipy.stats.norm import NormRV\n\ndef task_func(L):\n    data = np.concatenate(L)\n    hist, bins = np.histogram(data, bins=30)\n    hist_dist = Histogram(hist, bins)\n\n    fig, ax = plt.subplots()\n    ax.hist(data, bins=bins, density=True, alpha=0.6, color='g')\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    pdf = hist_dist.pdf(x)\n    cdf = hist_dist.cdf(x)\n\n    mean, std = hist_dist.stats('mv')\n    mu, std = mean, std\n\n    norm_dist = NormRV(loc=mu, scale=std)\n    p = norm_dist.pdf(x)\n\n    ax.plot(x, p, 'k', linewidth=2)\n    title = f\"Fit results: mu = {mu:.2f},  std = {std:.2f}\"\n    ax.set_title(title)\n\n    return ax\n<end>"}
{"id": 223, "answer": " <start>\nimport pandas as pd\nfrom sklearn.cluster import KMeans\n\ndef task_func(x_list, y_list, n_clusters=2, random_state=0):\n    df = pd.DataFrame({'x': x_list, 'y': y_list})\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init='auto').fit(df)\n    return kmeans.labels_, kmeans.cluster_centers_\n<end>"}
{"id": 224, "answer": " <start>\nimport pandas as pd\nfrom sklearn.cluster import KMeans\n\ndef task_func(x, y, n_clusters=2, random_state=0):\n    df = pd.DataFrame({'x': x, 'y': y})\n    kmeans = KMeans(n_clusters=n_clusters, init='k-means++', n_init='auto', random_state=random_state)\n    kmeans.fit(df)\n    return kmeans.labels_, kmeans.cluster_centers_\n<end>"}
{"id": 225, "answer": " <start>\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df, age, weight):\n    # Filter the DataFrame based on the given criteria\n    selected_df = df.query(\"Age < @age & Weight > @weight\")\n\n    # Check if the selected DataFrame is empty\n    if selected_df.empty:\n        return selected_df\n\n    # Standardizing the selected data\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(selected_df)\n    selected_df = pd.DataFrame(standardized_data, columns=selected_df.columns, index=selected_df.index)\n\n    return selected_df\n<end>"}
{"id": 226, "answer": " <start>\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df, age, weight):\n    # Filter the DataFrame based on the given criteria\n    selected_df = df.query(\"Age < @age & Weight > @weight\")\n\n    # Check if the selected DataFrame is empty\n    if selected_df.empty:\n        return selected_df\n\n    # Standardizing the selected data\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(selected_df)\n    selected_df = pd.DataFrame(standardized_data, columns=selected_df.columns, index=selected_df.index)\n\n    return selected_df\n<end>"}
{"id": 227, "answer": " <start>\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df, age, weight):\n    selected_df = df.loc[(df['Age'] < age) & (df['Weight'] > weight)]\n\n    if selected_df.empty:\n        return selected_df\n\n    scaler = StandardScaler()\n    selected_df = pd.DataFrame(scaler.fit_transform(selected_df), columns=selected_df.columns)\n\n    return selected_df\n<end>"}
{"id": 228, "answer": " <start>\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df, age, weight):\n    # Filtering the rows based on age and weight criteria\n    selected_df = df[(df['Age'] < age) & (df['Weight'] > weight)]\n\n    # Check if the selected DataFrame is empty\n    if selected_df.empty:\n        return selected_df\n\n    # Standardizing the selected data\n    scaler = StandardScaler()\n    selected_df = pd.DataFrame(scaler.fit_transform(selected_df), columns=selected_df.columns)\n\n    return selected_df\n<end>"}
{"id": 229, "answer": " <start>\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df, age, weight):\n    # Filter the DataFrame based on the given criteria\n    selected_df = df.query(\"Age < @age & Weight > @weight\")\n\n    # Check if the selected DataFrame is empty\n    if selected_df.empty:\n        return selected_df\n\n    # Standardizing the selected data\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(selected_df)\n    selected_df = pd.DataFrame(standardized_data, columns=selected_df.columns, index=selected_df.index)\n\n    return selected_df\n<end>"}
{"id": 230, "answer": " <start>\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df, age, weight):\n    # Filter the DataFrame based on the given criteria\n    selected_df = df.query(\"Age < @age & Weight > @weight\")\n\n    # Check if the selected DataFrame is empty\n    if selected_df.empty:\n        return selected_df\n\n    # Standardizing the selected data\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(selected_df)\n    selected_df = pd.DataFrame(standardized_data, columns=selected_df.columns, index=selected_df.index)\n\n    return selected_df\n<end>"}
{"id": 231, "answer": " <start>\nfrom collections.abc import Iterable\nfrom typing import Dict, List, Union\nfrom operator import itemgetter\nfrom itertools import groupby\n\ndef task_func(news_articles: Union[List[Dict], Iterable[Dict]]) -> Dict[str, List[Dict]]:\n    if not all(isinstance(article, dict) and set(article.keys()) == {'category', 'id', 'title', 'title_url'} for article in news_articles):\n        raise ValueError(\"input dictionaries must contain the following keys: 'category', 'id', 'title', 'title_url'\")\n\n    news_articles.sort(key=lambda article: (article['category'], article['title']))\n\n    grouped_articles = {category: list(group) for category, group in groupby(news_articles, key=itemgetter('category'))}\n\n    return grouped_articles\n<end>"}
{"id": 232, "answer": " <start>\n```python\nfrom collections.abc import Iterable\nfrom typing import DefaultDict\nfrom operator import itemgetter\nfrom itertools import groupby\n\ndef task_func(news_articles: Iterable):\n    if not all(isinstance(item, dict) and set(item.keys()) == {'category', 'id', 'title', 'title_url'} for item in news_articles):\n        raise ValueError(\"input items must be dictionaries with keys 'category', 'id', 'title', 'title_url'\")\n\n    news_articles = sorted(news_articles, key=itemgetter('category', 'title'))\n\n    grouped_articles: DefaultDict[str, list] = DefaultDict(list)\n    for category, group in groupby(news_articles, key=itemgetter('category')):\n        grouped_articles[category] = list(group)\n\n    return grouped_articles\n```\n<end>"}
{"id": 233, "answer": " <start>\nimport os\nfrom datetime import datetime as dt\n\ndef task_func(filepath: str) -> dict:\n    try:\n        size = os.path.getsize(filepath)\n        mtime = dt.fromtimestamp(os.path.getmtime(filepath))\n    except OSError as e:\n        raise Exception(\"Error: {}\".format(e))\n\n    return {'size': f\"{size} bytes\", 'last_modified': mtime.strftime('%Y-%m-%d %H:%M:%S')}\n<end>"}
{"id": 234, "answer": " <start>\nimport os\nfrom datetime import int\n\ndef task_func(filepath: str) -> dict:\n    try:\n        size = os.path.getsize(filepath)\n        mtime = int(os.path.getmtime(filepath))\n        mtime = datetime.fromtimestamp(mtime).strftime('%Y-%m-%d %H:%M:%S')\n    except OSError as e:\n        raise Exception(f\"Error: {e}\")\n\n    return {'size': f\"{size} bytes\", 'last_modified': mtime}\n<end>"}
{"id": 239, "answer": " <start>\nimport re\nimport pandas as pd\n\ndef task_func(df: pd.DataFrame) -> int:\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"df should be a DataFrame.\")\n\n    # Constants\n    BRACKETS_PATTERN = '[(){}[\\\\]]'\n\n    # If DataFrame is empty, return 0\n    if df.empty:\n        return 0\n\n    return df.pipe(\n        lambda s: s.str.len(re.findall(BRACKETS_PATTERN, s))\n    ).sum().sum()\n<end>"}
{"id": 237, "answer": " <start>\nimport pandas as pd\nimport csv\nimport random\nfrom pandas.core.resample import GroupBy\nfrom pandas.core.groupby import DataFrameGroupBy\n\ndef task_func(\n    n,\n    countries=['USA', 'UK', 'China', 'India', 'Germany'],\n    products=['Product A', 'Product B', 'Product C', 'Product D', 'Product E'],\n    output_path=None,\n    random_seed=None):\n\n    random.seed(random_seed)\n\n    sales_data = pd.DataFrame({'Country': countries, 'Product': products})\n\n    # Generate random sales for each combination of Country and Product\n    sales_data = sales_data.groupby(['Country', 'Product']).cumprod().apply(lambda x: x.sample(n=1, replace=False, random_state=random_seed).iloc[0, -1])\n\n    # If an output path is provided, save the data to a CSV file\n    if output_path:\n        sales_data.to_csv(output_path, index=False)\n\n    return sales_data\n<end>"}
{"id": 240, "answer": " <start>\nimport re\nimport pandas as pd\n\ndef task_func(df: pd.DataFrame) -> int:\n\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"df should be a DataFrame.\")\n\n    # Constants\n    BRACKETS_PATTERN = '[(){}[\\]]'\n\n    return df.apply(\n        lambda row: sum(len(re.findall(BRACKETS_PATTERN, str(cell))) for cell in row),\n        axis=1\n    ).sum()\n<end>"}
{"id": 242, "answer": " <start>\nimport pandas as pd\nfrom random import seed, choices\n\ndef task_func(L, num_dataframes=5, random_seed=None):\n    if random_seed is not None:\n        seed(random_seed)\n\n    if len(L) == 0:\n        return pd.DataFrame(), []\n\n    # Ensure all rows in L have the same length\n    if not all(len(row) == len(L[0]) for row in L):\n        raise ValueError(\"All rows in L must have the same length.\")\n\n    LETTERS = list('abcdefghijklmnopqrstuvwxyz')\n    max_cols = min(len(LETTERS), len(L[0]))\n    col_names = choices(LETTERS, k=max_cols)\n\n    dataframes = []\n    for _ in range(num_dataframes):\n        # Randomly sample rows from L for each DataFrame\n        sampled_rows = choices(L, k=3)\n        dataframe = pd.DataFrame(sampled_rows, columns=col_names)\n        dataframes.append(dataframe)\n\n    # Finding common rows across all DataFrames\n    # Concatenate all DataFrames and find common rows\n    combined_df = pd.concat(dataframes, ignore_index=True)\n    common_rows = combined_df[combined_df.duplicated(keep=False)]\n\n    return common_rows.drop_duplicates(), dataframes\n<end>"}
{"id": 241, "answer": " <start>\nimport pandas as pd\nfrom random import seed, choices\n\ndef task_func(L, num_dataframes=5, random_seed=None):\n    if random_seed is not None:\n        seed(random_seed)\n\n    if len(L) == 0:\n        return pd.DataFrame(), []\n\n    # Ensure all rows in L have the same length\n    if not all(len(row) == len(L[0]) for row in L):\n        raise ValueError(\"All rows in L must have the same length.\")\n\n    LETTERS = list('abcdefghijklmnopqrstuvwxyz')\n    max_cols = min(len(LETTERS), len(L[0]))\n    col_names = choices(LETTERS, k=max_cols)\n\n    dataframes = []\n    for _ in range(num_dataframes):\n        sampled_rows = choices(L, k=3)\n        dataframe = pd.DataFrame(sampled_rows, columns=col_names)\n        dataframes.append(dataframe)\n\n    # Finding common rows across all DataFrames\n    # Concatenate all DataFrames and find common rows\n    combined_df = pd.concat(dataframes, ignore_index=True)\n    common_rows = combined_df[combined_df.duplicated(keep=False)]\n\n    return common_rows.drop_duplicates(), dataframes\n<end>"}
{"id": 244, "answer": " <start>\nimport pandas as pd\nfrom random import seed, choices\n\ndef task_func(L, num_dataframes=5, random_seed=None):\n    if random_seed is not None:\n        seed(random_seed)\n\n    if len(L) == 0:\n        return pd.DataFrame(), []\n\n    # Ensure all rows in L have the same length\n    if not all(len(row) == len(L[0]) for row in L):\n        raise ValueError(\"All rows in L must have the same length.\")\n\n    LETTERS = list('abcdefghijklmnopqrstuvwxyz')\n    max_cols = min(len(LETTERS), len(L[0]))\n    col_names = choices(LETTERS, k=max_cols)\n\n    dataframes = []\n    for _ in range(num_dataframes):\n        # Randomly sample rows from L for each DataFrame\n        sampled_rows = choices(L, k=3)\n        dataframe = pd.DataFrame(sampled_rows, columns=col_names)\n        dataframes.append(dataframe)\n\n    # Finding common rows across all DataFrames\n    # Concatenate all DataFrames and find common rows\n    combined_df = pd.concat(dataframes, ignore_index=True)\n    common_rows = combined_df[combined_df.duplicated(keep=False)]\n\n    return common_rows.drop_duplicates(), dataframes\n<end>"}
{"id": 245, "answer": " <start>\nimport pandas as pd\nfrom faker import Faker, Random\nfrom collections import OrderedDict\n\ndef task_func(L, num_dataframes=5, random_seed=None):\n    if random_seed is not None:\n        Random.seed(random_seed)\n\n    if len(L) == 0:\n        return pd.DataFrame(), []\n\n    # Initialize Faker instance\n    faker = Faker()\n\n    # Generate column names using Faker's random_element method\n    col_names = [faker.random_element(OrderedDict([(\"{{\" + letter + \"}}\", 1 / len(LETTERS)) for letter in LETTERS])) for _ in range(min(len(LETTERS), len(L[0]))) ]\n\n    dataframes = []\n\n    for _ in range(num_dataframes):\n        # Randomly sample rows from L for each DataFrame\n        sampled_rows = [faker.random_sample(L, length=3) for _ in range(1)]\n        dataframe = pd.DataFrame(sampled_rows, columns=col_names)\n        dataframes.append(dataframe)\n\n    # Finding common rows across all DataFrames\n    # Concatenate all DataFrames and find common rows\n    combined_df = pd.concat(dataframes, ignore_index=True)\n    common_rows = combined_df[combined_df.duplicated(keep=False)]\n\n    return common_rows.drop_duplicates(), dataframes\n<end>"}
{"id": 246, "answer": " <start>\nimport pandas as pd\nfrom random import seed, choices\n\ndef task_func(L, num_dataframes=5, random_seed=None):\n    if random_seed is not None:\n        seed(random_seed)\n\n    if len(L) == 0:\n        return pd.DataFrame(), []\n\n    LETTERS = list('abcdefghijklmnopqrstuvwxyz')\n    max_cols = min(len(LETTERS), len(L[0]))\n    col_names = choices(LETTERS, k=max_cols)\n\n    dataframes = []\n\n    for _ in range(num_dataframes):\n        # Randomly sample rows from L for each DataFrame\n        sampled_rows = choices(L, k=3)\n        dataframe = pd.DataFrame(sampled_rows, columns=col_names)\n        dataframes.append(dataframe)\n\n    # Finding common rows across all DataFrames\n    # Concatenate all DataFrames and find common rows\n    combined_df = pd.concat(dataframes, ignore_index=True)\n    common_rows = combined_df[combined_df.duplicated(keep=False)]\n\n    return common_rows.drop_duplicates(), dataframes\n<end>"}
{"id": 247, "answer": " <start>\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(file_name: str) -> pd.DataFrame:\n    df = pd.read_csv(file_name, na_filter=False)\n    numeric_columns = df.select_dtypes(include=[np.number]).columns\n\n    if len(numeric_columns) == 0:\n        raise ValueError(\"Input must at least have one numeric column.\")\n\n    scaler = MinMaxScaler()\n    df[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n\n    return df\n<end>"}
{"id": 248, "answer": " <start>\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(file_name: str) -> pd.DataFrame:\n    df = pd.read_csv(file_name)\n\n    # Check if there are any numeric columns\n    if not df.select_dtypes(include='number').any():\n        raise ValueError(\"Input must at least have one numeric column.\")\n\n    # Normalize only the numeric columns\n    scaler = MinMaxScaler()\n    numeric_columns = df.select_dtypes(include='number').columns\n    df[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n\n    return df\n<end>"}
{"id": 249, "answer": " <start>\nimport os\nfrom datetime import datetime, timedelta\n\n# Constants\nLOG_DIR = './logs'\n\ndef task_func(metrics, filename, log_dir=LOG_DIR):\n    if not isinstance(metrics, dict):\n        raise ValueError(\"Metrics must be a dictionary\")\n    if not isinstance(filename, str):\n        raise ValueError(\"Filename must be a string\")\n\n    try:\n        # Ensure the log directory exists\n        if not os.path.exists(log_dir):\n            os.makedirs(log_dir)\n\n        # Get current timestamp\n        timestamp = datetime.now()\n\n        # Write timestamp to the log file\n        with open(os.path.join(log_dir, filename), 'a') as f:\n            f.write('{}\\n'.format(timestamp))\n\n        # Write metrics to the log file\n        for key, value in metrics.items():\n            f.write('{}: {}\\n'.format(key, value))\n        f.write('\\n')\n        return True\n    except Exception as e:\n        print(\"An error occurred: {}\".format(e))\n        return False\n<end>"}
{"id": 250, "answer": " <start>\nimport os\nfrom datetime import int\n\nLOG_DIR = './logs'\n\ndef task_func(metrics, filename, log_dir=LOG_DIR):\n\n    if not isinstance(metrics, dict):\n        raise ValueError(\"Metrics must be a dictionary\")\n    if not isinstance(filename, str):\n        raise ValueError(\"Filename must be a string\")\n\n    try:\n        with open(os.path.join(log_dir, filename), 'a') as f:\n            f.write(f'{datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\\n')\n            for key, value in metrics.items():\n                f.write(f'{key}: {value}\\n')\n            f.write('\\n')\n        return True\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False\n<end>"}
{"id": 251, "answer": " <start>\nimport numpy as np\nfrom scipy.stats import norm\n\ndef task_func(data: np.ndarray, threshold: float = 2.0) -> tuple:\n    # Calculate the z-scores\n    z_scores = (data - norm.fit(data)[0]) / norm.fit(data)[1]\n\n    if np.isclose(norm.fit(data)[1], 0):\n        return [], norm.fit(data)[0], 0\n\n    outliers = np.where(np.abs(z_scores) > threshold)[0]\n\n    return list(outliers), norm.fit(data)[0], norm.fit(data)[1]**2\n<end>"}
{"id": 252, "answer": " <start>\nimport numpy as np\nfrom scipy.stats import norm\n\ndef task_func(data: np.ndarray, threshold: float = 2.0) -> tuple:\n    # Calculate the z-scores\n    z_scores = (data - norm.fit(data)[0]) / norm.fit(data)[1]\n\n    if np.std(z_scores) == 0:\n        return [], norm.fit(data)[0], norm.fit(data)[1]**2\n\n    outliers = np.where(np.abs(z_scores) > threshold)[0]\n\n    return outliers, norm.fit(data)[0], norm.fit(data)[1]**2\n<end>"}
{"id": 253, "answer": " <start>\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.cm import get_cmap\n\ndef task_func(letters, repetitions, colors):\n    if len(letters) != len(repetitions) or len(letters) != len(colors) or len(letters) == 0:\n        raise ValueError(\"All lists must be the same length and non-empty.\")\n\n    # Count the frequency of each letter based on repetitions\n    counts = np.array(repetitions)\n\n    # Create a colormap from the provided colors\n    cmap = get_cmap(colors[0])\n    colors = [cmap(i) for i in np.linspace(0, 1, len(letters))]\n\n    # Create the bar chart\n    fig, ax = plt.subplots()\n    x = np.arange(len(letters))  # Use numerical x-axis values\n    ax.bar(x, counts, color=colors)\n    ax.set_xticks(x)  # Set the x-ticks to be the positions of the bars\n    ax.set_xticklabels(letters)  # Set the x-tick labels to be the letters\n    ax.set_xlabel('Letters')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of Letters')\n\n    return ax\n<end>"}
{"id": 254, "answer": " <start>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(letters, repetitions, colors):\n    if len(letters) != len(repetitions) or len(letters) != len(colors) or len(letters) == 0:\n        raise ValueError(\"All lists must be the same length and non-empty.\")\n\n    # Count the frequency of each letter based on repetitions\n    counts = np.array(repetitions)\n\n    # Create the bar chart\n    fig, ax = plt.subplots()\n    ax.bar(range(len(letters)), counts, tick_label=letters, color=colors)\n    ax.set_xlabel('Letters')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of Letters')\n\n    return ax\n<end>"}
{"id": 255, "answer": " <start>\n```python\nfrom typing import List, Union, Optional\nimport numpy as np\nfrom scipy.fftpack import fft\n\ndef task_func(data: List[Union[int, str]], repetitions: Optional[int] = 1):\n\n    def calculate_mode(data):\n        # Use a dictionary to count occurrences, considering both value and type\n        counts = {}\n        for item in data:\n            key = (item, type(item))  # Distinguish between types\n            counts[key] = counts.get(key, 0) + 1\n\n        # Find the maximum count and corresponding values\n        max_count = max(counts.values())\n        mode_items = [value for (value, value_type), count in counts.items() if count == max_count]\n\n        return mode_items, [max_count] * len(mode_items)\n\n    if not data or repetitions <= 0:  # Handle empty data or no repetitions\n        return {'mode': np.array([], dtype='object'), 'count': np.array([], dtype=int), 'fft': np.array([])}\n\n    # Repeat the data\n    repeated_data = np.repeat(data, repetitions)\n\n    # Calculate mode\n    mode, count = calculate_mode(repeated_data)\n    # using scipy.fftpack to calculate fft\n    return {'mode': np.sort(mode), 'count': count, 'fft': fft(repeated_data)}\n```\n<end>"}
{"id": 256, "answer": " <start>\nfrom typing import List, Union\nimport numpy as np\nimport scipy.stats.mstats_basic\n\ndef task_func(data: List[Union[int, str]], repetitions: int = 1):\n\n    def calculate_mode(data):\n        # Use a dictionary to count occurrences, considering both value and type\n        counts = {}\n        for item in data:\n            key = (item, type(item))  # Distinguish between types\n            counts[key] = counts.get(key, 0) + 1\n\n        # Find the maximum count and corresponding values\n        max_count = max(counts.values())\n        mode_items = [value for (value, value_type), count in counts.items() if count == max_count]\n\n        return mode_items, [max_count] * len(mode_items)\n\n    if not data or repetitions <= 0:  # Handle empty data or no repetitions\n        return {'mode': np.array([], dtype='object'), 'count': np.array([], dtype=int), 'fft': np.array([])}\n\n    # Repeat the data\n    repeated_data = data * repetitions\n\n    # Calculate mode\n    mode, count = calculate_mode(repeated_data)\n\n    # Calculate FFT using scipy.fftpack\n    fft = np.fft.fft(repeated_data)\n\n    return {'mode': np.sort(mode), 'count': count, 'fft': fft}\n<end>"}
{"id": 259, "answer": " <start>\nimport numpy as np\nfrom scipy.integrate import quad\nimport matplotlib.pyplot as plt\n\ndef task_func(func, x_range=(-2, 2), num_points=1000):\n    X = np.linspace(x_range[0], x_range[1], num_points)\n    y = func(X)\n\n    def cumulative_integral(x):\n        return quad(lambda t: y[t], x_range[0], x)[0]\n\n    y_int = [cumulative_integral(x) for x in X]\n\n    fig, ax = plt.subplots()\n    ax.plot(X, y, label=f\"{func.__name__}(x)\")\n    ax.plot(X, y_int, label=f\"Integral of {func.__name__}(x)\")\n    ax.legend()\n\n    return ax\n<end>"}
{"id": 260, "answer": " <start>\nimport numpy as np\nfrom scipy.integrate import quad\nimport matplotlib.pyplot as plt\n\ndef task_func(func, x_range=(-2, 2), num_points=1000):\n    X = np.linspace(x_range[0], x_range[1], num_points)\n    y = func(X)\n\n    def cumulative_integral(y_val, x_val):\n        return quad(lambda x: y_val * x, x_range[0], x_val)[0]\n\n    y_int = np.vectorize(cumulative_integral)(y)\n\n    fig, ax = plt.subplots()\n    ax.plot(X, y, label=f\"{func.__name__}(x)\")\n    ax.plot(X, y_int, label=f\"Integral of {func.__name__}(x)\")\n    ax.legend()\n\n    return ax\n<end>"}
{"id": 238, "answer": " <start>\nimport pandas as pd\nimport csv\nimport random\nfrom faker import Faker\n\nfake = Faker()\n\ndef task_func(\n    n,\n    countries=None,\n    products=None,\n    output_path=None,\n    random_seed=None):\n\n    random.seed(random_seed)\n\n    sales_data = []\n\n    if countries is None:\n        countries = ['USA', 'UK', 'China', 'India', 'Germany']\n\n    if products is None:\n        products = ['Product A', 'Product B', 'Product C', 'Product D', 'Product E']\n\n    for _ in range(n):\n        country = random.choice(countries)\n        product = random.choice(products)\n        sales = random.randint(1, 100)\n        sales_data.append({'Country': country, 'Product': product, 'Sales': sales})\n\n    # If an output path is provided, save the data to a CSV file\n    if output_path:\n        with open(output_path, 'w', newline='') as csvfile:\n            fieldnames = ['Country', 'Product', 'Sales']\n            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n            writer.writeheader()\n            writer.writerows(sales_data)\n\n    return pd.DataFrame(sales_data)\n<end>"}
{"id": 262, "answer": " <start>\nimport numpy as np\nfrom scipy import integrate\nfrom matplotlib.pylab import frompyfunc\n\ndef task_func(func, x_range=(-2, 2), num_points=1000):\n    X = np.linspace(x_range[0], x_range[1], num_points)\n    y = func(X)\n\n    # Create a NumPy ufunc for the function\n    func_ufunc = frompyfunc(func, 1, 1)\n\n    # Calculate the cumulative integral using the ufunc\n    y_int = integrate.cumtrapz(func_ufunc(X), X, initial=0)\n\n    fig, ax = plt.subplots()\n    ax.plot(X, y, label=func.__name__ + \"(x)\")\n    ax.plot(X, y_int, label=\"Integral of \" + func.__name__ + \"(x)\")\n    ax.legend()\n\n    return ax\n<end>"}
{"id": 263, "answer": " <start>\nimport numpy as np\nfrom scipy import integrate\nimport matplotlib.pylab as pylab\n\ndef task_func(func, x_range=(-2, 2), num_points=1000):\n    X = pylab.arange(x_range[0], x_range[1], (x_range[1] - x_range[0]) / num_points)\n    y = func(X)\n    y_int = integrate.cumulative_trapezoid(y, X, initial=0)\n\n    fig, ax = pylab.subplots()\n    ax.plot(X, y, label=f\"{func.__name__}(x)\")\n    ax.plot(X, y_int, label=f\"Integral of {func.__name__}(x)\")\n    ax.legend()\n\n    pylab.xlabel(\"x\")\n    pylab.ylabel(\"y\")\n    pylab.title(f\"Plot of {func.__name__}(x) and its integral\")\n\n    return ax\n<end>"}
{"id": 264, "answer": " <start>\nimport numpy as np\nfrom scipy.integrate import quad\nimport matplotlib.pyplot as plt\n\ndef task_func(func, x_range=(-2, 2), num_points=1000):\n    X = np.linspace(x_range[0], x_range[1], num_points)\n    y = [func(x) for x in X]\n\n    def cumulative_integral(x, y_values):\n        y_cumulative = [0]\n        for i in range(len(y_values)):\n            y_cumulative.append(y_cumulative[-1] + y_values[i])\n        return y_cumulative\n\n    y_cumulative = cumulative_integral(X, y)\n\n    fig, ax = plt.subplots()\n    ax.plot(X, y, label=f\"{func.__name__}(x)\")\n    ax.plot(X, y_cumulative, label=f\"Integral of {func.__name__}(x)\")\n    ax.legend()\n\n    return ax\n<end>"}
{"id": 267, "answer": " <start>\nimport urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nfrom matplotlib.pylab import lookfor\n\ndef task_func(url):\n    with urllib.request.urlopen(url) as response:\n        text = response.read().decode()\n        words = re.findall(r\"\\b\\w+\\b\", text)\n        word_freq = Counter(words)\n        top_words = word_freq.most_common(10)\n\n        _, ax = plt.subplots()\n        labels = [word[0] for word in top_words]\n        fmt = lookfor('frequency')\n        ax.bar(range(len(top_words)), [word[1] for word in top_words], tick_label=labels, label_type='center', fmt=fmt)\n        ax.set_title(\"Top 10 Most Common Words\")\n        ax.set_xlabel(\"Words\")\n        ax.set_ylabel(\"Frequency\")\n\n        return word_freq, ax\n<end>"}
{"id": 243, "answer": " <start>\nimport pandas as pd\nfrom random import seed, choices\n\ndef task_func(L, num_dataframes=5, random_seed=None):\n    if random_seed is not None:\n        seed(random_seed)\n\n    if len(L) == 0:\n        return pd.DataFrame(), []\n\n    # Ensure all rows in L have the same length\n    if not all(len(row) == len(L[0]) for row in L):\n        raise ValueError(\"All rows in L must have the same length.\")\n\n    LETTERS = list('abcdefghijklmnopqrstuvwxyz')\n    max_cols = min(len(LETTERS), len(L[0]))\n    col_names = choices(LETTERS, k=max_cols)\n\n    dataframes = []\n    for _ in range(num_dataframes):\n        # Randomly sample rows from L for each DataFrame\n        sampled_rows = choices(L, k=3)\n        dataframe = pd.DataFrame(sampled_rows, columns=col_names)\n        dataframes.append(dataframe)\n\n    # Finding common rows across all DataFrames\n    # Concatenate all DataFrames and find common rows\n    combined_df = pd.concat(dataframes, ignore_index=True)\n    common_rows = combined_df[combined_df.duplicated(keep=False)]\n\n    return common_rows.drop_duplicates(), dataframes\n<end>"}
{"id": 269, "answer": " <start>\nimport requests\nfrom PIL import Image, ImageQt, ImageFile\n\ndef task_func(url):\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()\n\n        # Register the QImage plugin for PNG images\n        Image.register_open(\"PNG\", ImageFile.Parser(ImageFile.PNG, ImageFile.PNG, ImageFile.IO(response.raw)))\n\n        # Load the QImage from the response content\n        qimage = ImageQt.fromqimage(Image.open(ImageQt.toqimage(Image.open(io.BytesIO(response.content)))).convert(\"RGB\"))\n\n        # Create a PIL Image from the QImage\n        image = Image.fromqimage(qimage)\n\n        return image\n    except Exception as e:\n        raise ValueError(f\"Failed to retrieve image from {url}: {e}\") from e\n<end>"}
{"id": 270, "answer": " <start>\nimport requests\nfrom PIL import Image, ImageQt\n\ndef task_func(url):\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()\n\n        # Load the response content as a PIL Image\n        img_data = response.content\n\n        # Convert the PIL Image to QImage\n        qimage = ImageQt.fromqimage(Image.frombytes(\"RGB\", img_data.size, img_data))\n\n        # Copy the contents of the QImage to a PIL image memory\n        pil_image = Image.fromqimage(qimage)\n\n        return pil_image\n    except Exception as e:\n        raise ValueError(f\"Failed to retrieve image from {url}: {e}\") from e\n<end>"}
{"id": 271, "answer": " <start>\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(csv_file_path, col1_name=\"column1\", col2_name=\"column2\"):\n    df = pd.read_csv(csv_file_path)\n\n    # Check if the column to be aggregated is numeric\n    if not pd.api.types.is_numeric_dtype(df[col2_name]):\n        raise TypeError(f\"The column '{col2_name}' must contain numeric data.\")\n\n    groupby_data = df.groupby(col1_name)[col2_name].mean()\n\n    _, ax = plt.subplots(figsize=(10, 6))\n    ax.barh(groupby_data.index, groupby_data.values)\n    ax.set_title(f\"Mean of {col2_name} Grouped by {col1_name}\")\n    ax.set_xlabel(col1_name)\n    ax.set_ylabel(f\"Mean of {col2_name}\")\n\n    return ax\n<end>"}
{"id": 268, "answer": " <start>\nimport urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nfrom matplotlib.cbook import get_split_ind\n\ndef task_func(url):\n    with urllib.request.urlopen(url) as response:\n        text = response.read().decode()\n        words = re.findall(r\"\\b\\w+\\b\", text)\n        word_freq = Counter(words)\n\n        top_words = word_freq.most_common(10)\n        num_words = len(top_words)\n\n        _, ax = plt.subplots()\n        ax.bar([word[0] for word in top_words[:num_words]],\n               [word[1] for word in top_words[:num_words]])\n        ax.set_title(\"Top 10 Most Common Words\")\n        ax.set_xlabel(\"Words\")\n        ax.set_ylabel(\"Frequency\")\n\n        # Find the index to split the x-axis labels\n        split_index = get_split_ind(top_words[:num_words], 8)\n\n        # Split the x-axis labels\n        x_labels = [top_words[i][0] for i in range(split_index)] + \\\n                   ['...'] + [top_words[i][0] for i in range(split_index + 1, num_words)]\n\n        ax.set_xticks(range(num_words))\n        ax.set_xticklabels(x_labels)\n\n    return word_freq, ax\n<end>"}
{"id": 272, "answer": " <start>\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Subplot\n\ndef task_func(csv_file_path, col1_name=\"column1\", col2_name=\"column2\"):\n    df = pd.read_csv(csv_file_path)\n\n    # Check if the column to be aggregated is numeric\n    if not pd.api.types.is_numeric_dtype(df[col2_name]):\n        raise TypeError(f\"The column '{col2_name}' must contain numeric data.\")\n\n    groupby_data = df.groupby(col1_name)[col2_name].mean()\n\n    _, ax = plt.subplots(figsize=(10, 6))\n    ax: Subplot = ax\n\n    ax.bar(groupby_data.index, groupby_data.values)\n    ax.set_title(f\"Mean of {col2_name} Grouped by {col1_name}\")\n    ax.set_xlabel(col1_name)\n    ax.set_ylabel(f\"Mean of {col2_name}\")\n\n    return ax\n<end>"}
{"id": 274, "answer": " <start>\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Subplot\n\ndef task_func(csv_file_path, col1_name=\"column1\", col2_name=\"column2\"):\n    df = pd.read_csv(csv_file_path)\n\n    # Check if the column to be aggregated is numeric\n    if not pd.api.types.is_numeric_dtype(df[col2_name]):\n        raise TypeError(f\"The column '{col2_name}' must contain numeric data.\")\n\n    groupby_data = df.groupby(col1_name)[col2_name].mean()\n\n    _, ax = plt.subplots(figsize=(10, 6))\n    ax: Subplot = ax\n    ax.bar(range(len(groupby_data.index)), groupby_data.values)\n    ax.set_xticks(range(len(groupby_data.index)))\n    ax.set_xticklabels(groupby_data.index)\n    ax.set_title(f\"Mean of {col2_name} Grouped by {col1_name}\")\n    ax.set_xlabel(col1_name)\n    ax.set_ylabel(f\"Mean of {col2_name}\")\n\n    return ax\n<end>"}
{"id": 261, "answer": " <start>\nimport numpy as np\nfrom scipy import integrate\nimport matplotlib.pyplot as plt\nfrom matplotlib.pylab import frompyfunc\n\ndef task_func(func, x_range=(-2, 2), num_points=1000):\n    X = np.linspace(x_range[0], x_range[1], num_points)\n\n    # Create a NumPy ufunc for the given function\n    func_ufunc = frompyfunc(func, 1, 1)\n\n    # Calculate the integral using the ufunc\n    y = func_ufunc(X)\n    y_int = integrate.cumtrapz(y, X, initial=0)\n\n    fig, ax = plt.subplots()\n    ax.plot(X, y, label=func.__name__ + \"(x)\")\n    ax.plot(X, y_int, label=\"Integral of \" + func.__name__ + \"(x)\")\n    ax.legend()\n\n    return ax\n<end>"}
{"id": 276, "answer": " <start>\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Subplot\n\ndef task_func(csv_file_path, col1_name=\"column1\", col2_name=\"column2\"):\n    df = pd.read_csv(csv_file_path)\n    groupby_data = df.groupby(col1_name)[col2_name].mean()\n\n    _, ax = plt.subplots(figsize=(10, 6), subplot_kw={'projection': 'geo'})\n    ax = Subplot(ax.figure, ax.axes.transData, **ax.axes.spines)\n    ax.barh(groupby_data.index, groupby_data.values)\n    ax.set_title(f\"Mean of {col2_name} Grouped by {col1_name}\")\n    ax.set_xlabel(col1_name)\n    ax.set_ylabel(f\"Mean of {col2_name}\")\n\n    return ax\n<end>"}
{"id": 277, "answer": " <start>\nimport requests\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.image import imread\n\ndef task_func(url: str) -> \"matplotlib.axes._axes.Axes\":\n    response = None  # Initialize response to None\n    # Validate the URL\n    if not isinstance(url, str) or not url:\n        raise ValueError(\"Invalid URL provided.\")\n\n    # Download the image with error handling\n    try:\n        img = imread(requests.get(url, stream=True, timeout=10).raw)\n    except requests.RequestException as e:\n        raise ValueError(f\"Error downloading the image: {e}\") from e\n    except IOError as e:\n        raise IOError(f\"Error processing the image: {e}\") from e\n    finally:\n        if response:  # Check if response is not None before closing\n            response.close()\n\n    # Convert the image to a numpy array\n    img_array = np.array(img)\n\n    # Create the histogram and return the Axes object\n    _, ax = plt.subplots()\n    ax.hist(img_array.ravel(), bins=256, range=(0, 256), color=\"gray\", alpha=0.7)\n    ax.set_title(\"Grayscale Histogram\")\n    ax.set_xlabel(\"Pixel Intensity\")\n    ax.set_ylabel(\"Frequency\")\n    return ax\n<end>"}
{"id": 278, "answer": " <start>\nimport requests\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.image import imread\n\ndef task_func(url: str) -> \"matplotlib.axes._axes.Axes\":\n    # Validate the URL\n    if not isinstance(url, str) or not url:\n        raise ValueError(\"Invalid URL provided.\")\n\n    # Download the image with error handling\n    try:\n        img = imread(url)\n    except requests.RequestException as e:\n        raise ValueError(f\"Error downloading the image: {e}\") from e\n    except IOError as e:\n        raise IOError(f\"Error processing the image: {e}\") from e\n\n    # Convert the image to a numpy array\n    img_array = np.array(img)\n\n    # Create the histogram and return the Axes object\n    _, ax = plt.subplots()\n    ax.hist(img_array.ravel(), bins=256, color=\"gray\", alpha=0.7)\n    ax.set_title(\"Grayscale Histogram\")\n    return ax\n<end>"}
{"id": 279, "answer": " <start>\nimport socket\nimport select\nimport queue\nimport datetime\nfrom datetime import UTC, timezone\n\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n    server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server.setblocking(0)\n    server.bind((server_address, server_port))\n    server.listen(5)\n    inputs = [server]\n    outputs = []\n    message_queues = {}\n    end_time = datetime.now(UTC) + timedelta(seconds=run_duration)\n\n    try:\n        while datetime.now(UTC) < end_time:\n            readable, writable, _ = select.select(inputs, outputs, inputs, 1)\n            for s in readable:\n                if s is server:\n                    connection, _ = s.accept()\n                    connection.setblocking(0)\n                    inputs.append(connection)\n                    message_queues[connection] = queue.Queue()\n                else:\n                    data = s.recv(buffer_size)\n                    if data:\n                        message_queues[s].put(f\"{datetime.now(timezone.utc)}: {data.decode()}\")\n                        if s not in outputs:\n                            outputs.append(s)\n                    else:\n                        if s in outputs:\n                            outputs.remove(s)\n                        inputs.remove(s)\n                        s.close()\n                        del message_queues[s]\n\n            for s in writable:\n                if s not in message_queues:\n                    continue  # Skip if socket's queue has been removed\n\n                try:\n                    next_msg = message_queues[s].get_nowait()\n                except queue.Empty:\n                    outputs.remove(s)\n                else:\n                    s.sendall(next_msg.encode(\"utf-8\"))\n\n    finally:\n        server.close()\n\n    return f\"Server started on {server_address}:{server_port}. Ran for {run_duration} seconds.\"\n<end>"}
{"id": 280, "answer": " <start>\nimport socket\nimport select\nimport queue\nfrom datetime import int\n\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n    server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server.setblocking(0)\n    server.bind((server_address, server_port))\n    server.listen(5)\n    inputs = [server]\n    outputs = []\n    message_queues = {}\n    end_time = int((datetime.now().timestamp() + run_duration))\n\n    try:\n        while int(datetime.now().timestamp()) < end_time:\n            readable, writable, _ = select.select(inputs, outputs, inputs, 1)\n            for s in readable:\n                if s is server:\n                    connection, _ = s.accept()\n                    connection.setblocking(0)\n                    inputs.append(connection)\n                    message_queues[connection] = queue.Queue()\n                else:\n                    data = s.recv(buffer_size)\n                    if data:\n                        message_queues[s].put(f\"{datetime.now()}: {data.decode()}\")\n                        if s not in outputs:\n                            outputs.append(s)\n                    else:\n                        if s in outputs:\n                            outputs.remove(s)\n                        inputs.remove(s)\n                        s.close()\n                        del message_queues[s]\n\n            for s in writable:\n                if s not in message_queues:\n                    continue  # Skip if socket's queue has been removed\n\n                try:\n                    next_msg = message_queues[s].get_nowait()\n                except queue.Empty:\n                    outputs.remove(s)\n                else:\n                    s.sendall(next_msg.encode(\"utf-8\"))\n\n    finally:\n        server.close()\n\n    return f\"Server started on {server_address}:{server_port}. Ran for {run_duration} seconds.\"\n<end>"}
{"id": 281, "answer": " <start>\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\n\n# Constants\nCATEGORIES = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n\ndef task_func(data_list):\n    if not data_list:\n        raise ValueError(\"The data list is empty.\")\n\n    data_series = pd.Series(data_list)\n    category_counts = data_series.value_counts()\n\n    # Prepare data for predefined categories\n    predefined_counts = category_counts.reindex(CATEGORIES, fill_value=0)\n\n    # Check for uniformity in predefined categories\n    if not all(x == predefined_counts.iloc[0] for x in predefined_counts):\n        print(\"The distribution of predefined categories is not uniform.\")\n\n    # Handling extra categories not in predefined list\n    extra_categories = category_counts.drop(CATEGORIES, errors=\"ignore\").index.tolist()\n    all_categories = CATEGORIES + extra_categories\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax: Axes = ax\n\n    ax.bar(\n        range(len(all_categories)),\n        category_counts.reindex(all_categories, fill_value=0),\n        width=0.8,\n        align=\"center\",\n    )\n    ax.set_xticks(range(len(all_categories)))\n    ax.set_xticklabels(all_categories)\n\n    return ax\n<end>"}
{"id": 273, "answer": " <start>\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import PlotAccessor\n\ndef task_func(csv_file_path, col1_name=\"column1\", col2_name=\"column2\"):\n    df = pd.read_csv(csv_file_path)\n\n    # Check if the column to be aggregated is numeric\n    if not pd.api.types.is_numeric_dtype(df[col2_name]):\n        raise TypeError(f\"The column '{col2_name}' must contain numeric data.\")\n\n    groupby_data = df.groupby(col1_name)[col2_name].mean()\n\n    ax = PlotAccessor(groupby_data).barh(x=col1_name, y=col2_name)\n    ax.set_title(f\"Mean of {col2_name} Grouped by {col1_name}\")\n    ax.set_xlabel(col1_name)\n    ax.set_ylabel(f\"Mean of {col2_name}\")\n\n    return ax\n<end>"}
{"id": 275, "answer": " <start>\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import PlotAccessor\n\ndef task_func(csv_file_path, col1_name=\"column1\", col2_name=\"column2\"):\n    df = pd.read_csv(csv_file_path)\n    groupby_data = df.groupby(col1_name)[col2_name].mean()\n\n    ax = groupby_data.plot.barh(x=col1_name, y=col2_name, title=f\"Mean of {col2_name} Grouped by {col1_name}\")\n    ax.set_xlabel(col1_name)\n    ax.set_ylabel(f\"Mean of {col2_name}\")\n\n    return ax\n<end>"}
{"id": 282, "answer": " <start>\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n\ndef task_func(data_list):\n\n    if not data_list:\n        raise ValueError(\"The data list is empty.\")\n\n    data_series = pd.Series(data_list)\n    category_counts = data_series.value_counts()\n\n    # Prepare data for predefined categories\n    predefined_counts = category_counts.reindex(CATEGORIES, fill_value=0)\n\n    # Check for uniformity in predefined categories\n    if not all(x == predefined_counts.iloc[0] for x in predefined_counts):\n        print(\"The distribution of predefined categories is not uniform.\")\n\n    # Handling extra categories not in predefined list\n    extra_categories = category_counts.drop(CATEGORIES, errors=\"ignore\").index.tolist()\n    all_categories = CATEGORIES + extra_categories\n\n    _, ax = plt.subplots()\n    ax.bar(\n        all_categories,\n        category_counts.reindex(all_categories, fill_value=0),\n        width=0.8,\n        align=\"center\",\n    )\n    ax.set_xticks(all_categories)\n\n    return ax\n<end>"}
{"id": 284, "answer": " <start>\nfrom datetime import datetime\nimport pandas as pd\nfrom itertools import product\nfrom pandas.core.tools.datetimes import to_datetime\n\n# Constants\nEMPLOYEES = [\"John\", \"Alice\", \"Bob\", \"Charlie\", \"Dave\"]\n\ndef task_func(date_str):\n    start_date = to_datetime(date_str, errors='coerce')\n    dates = pd.date_range(start_date, periods=10)\n\n    # Creating a DataFrame from the product of EMPLOYEES and dates\n    df = pd.DataFrame(list(product(EMPLOYEES, dates)), columns=[\"Employee\", \"Date\"])\n\n    return df\n<end>"}
{"id": 286, "answer": " <start>\nfrom datetime import datetime\nimport pandas as pd\nfrom pandas.core.tools.datetimes import to_datetime\n\n# Constants\nEMPLOYEES = [\"John\", \"Alice\", \"Bob\", \"Charlie\", \"Dave\"]\n\ndef task_func(date_str):\n    start_date = to_datetime(date_str, errors='coerce').to_timestamp()\n    dates = pd.date_range(start_date, periods=10)\n\n    # Creating a DataFrame from the product of EMPLOYEES and dates\n    df = pd.DataFrame(list(product(EMPLOYEES, dates)), columns=[\"Employee\", \"Date\"])\n\n    return df\n<end>"}
{"id": 287, "answer": " <start>\n```python\nfrom datetime import datetime\nimport pandas as pd\n\n# Constants\nEMPLOYEES = [\"John\", \"Alice\", \"Bob\", \"Charlie\", \"Dave\"]\n\ndef task_func(date_str):\n    start_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n    dates = pd.date_range(start_date, periods=10)\n\n    # Creating a DataFrame from the product of EMPLOYEES and dates\n    df = pd.DataFrame({\"Employee\": EMPLOYEES, \"Date\": dates})\n\n    return df\n```\n<end>"}
{"id": 288, "answer": " <start>\n```python\nfrom datetime import datetime\nimport pandas as pd\n\n# Constants\nEMPLOYEES = [\"John\", \"Alice\", \"Bob\", \"Charlie\", \"Dave\"]\n\ndef task_func(date_str):\n    start_date = pd.to_datetime(date_str, format=\"%Y-%m-%d\")\n    dates = pd.date_range(start_date, periods=10)\n\n    # Creating a DataFrame from the product of EMPLOYEES and dates\n    df = pd.DataFrame(list(zip(EMPLOYEES, dates)), columns=[\"Employee\", \"Date\"])\n\n    return df\n```\n<end>"}
{"id": 289, "answer": " <start>\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Subplot\n\n# Constants\nLETTERS = list(\"abcdefghijklmnopqrstuvwxyz\")\nNUMBERS = list(range(1, 27))\n\ndef task_func(n_pairs=26):\n    if n_pairs > 26 or n_pairs < 1:\n        raise ValueError(\"n_pairs should be between 1 and 26\")\n\n    pairs = [f\"{letter}:{number}\" for letter, number in zip(LETTERS, NUMBERS)][:n_pairs]\n    random.seed(42)\n    random.shuffle(pairs)\n    counts = np.random.randint(1, 10, size=n_pairs)\n\n    # Create a figure and axis\n    fig, ax = plt.subplots()\n    bars = ax.bar(range(n_pairs), counts)\n\n    # Set label for each bar\n    ax.set_xticks(range(n_pairs))\n    ax.bar_label(bars, labels=pairs, padding=0)\n\n    ax.set_xlabel(\"Letter:Number Pairs\")\n    ax.set_ylabel(\"Counts\")\n    ax.set_title(\"Random Letter:Number Pairs Chart\")\n\n    return bars\n<end>"}
{"id": 290, "answer": " <start>\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nfrom matplotlib.figure import Axes\n\n# Constants\nLETTERS = list(\"abcdefghijklmnopqrstuvwxyz\")\nNUMBERS = list(range(1, 27))\n\ndef task_func(n_pairs=26):\n    if n_pairs > 26 or n_pairs < 1:\n        raise ValueError(\"n_pairs should be between 1 and 26\")\n\n    pairs = [f\"{letter}:{number}\" for letter, number in zip(LETTERS, NUMBERS)][:n_pairs]\n    random.seed(42)\n    random.shuffle(pairs)\n    counts = np.random.randint(1, 10, size=n_pairs)\n\n    fig, ax = plt.subplots()\n    bars = ax.bar(pairs, counts)\n\n    # Set label for each bar\n    for bar, pair in zip(bars, pairs):\n        bar.set_label(pair)\n\n    ax.set_xlabel(\"Letter:Number Pairs\")\n    ax.set_ylabel(\"Counts\")\n    ax.set_title(\"Random Letter:Number Pairs Chart\")\n\n    return bars\n<end>"}
{"id": 291, "answer": " <start>\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\n\ndef task_func(arr):\n    unique, counts = np.unique(arr, return_counts=True)\n    uniform_distribution = len(set(counts)) == 1\n\n    # Convert the array to integers for histogram plotting\n    unique_indices = {val: idx for idx, val in enumerate(unique)}\n    arr_indices = np.array([unique_indices[val] for val in arr])\n\n    fig = plt.figure(figsize=(8, 6))\n    gs = GridSpec(nrows=len(unique), ncols=1, figure=fig)\n\n    for i, (val, count) in enumerate(zip(unique, counts)):\n        ax = plt.Subplot(fig, gs[i])\n        ax.hist([arr_indices == i], bins=2, align=\"left\", rwidth=0.8)\n        ax.set_xticks([0, count])\n        ax.set_xticklabels([\"0\", f\"{count}\"])\n        ax.set_ylabel(f\"{val}\")\n        ax.set_title(f\"{val}\")\n\n    return uniform_distribution, fig\n<end>"}
{"id": 292, "answer": " <start>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(arr):\n    unique, counts = np.unique(arr, return_counts=True)\n    uniform_distribution = len(set(counts)) == 1\n\n    _, ax = plt.subplots()\n    ax.hist(arr, bins=np.unique(arr), rwidth=0.8, align=\"mid\")\n    ax.set_xticks(np.arange(len(unique)))\n    ax.set_xticklabels(unique)\n\n    return uniform_distribution, ax\n<end>"}
{"id": 283, "answer": " <start>\n```python\nfrom datetime import datetime\nimport pandas as pd\n\n# Constants\nEMPLOYEES = [\"John\", \"Alice\", \"Bob\", \"Charlie\", \"Dave\"]\n\ndef task_func(date_str):\n    start_date = pd.to_datetime(date_str, format=\"%Y-%m-%d\")\n    dates = start_date + pd.DateOffset(days=n) for n in range(10)\n\n    # Creating a DataFrame from the product of EMPLOYEES and dates\n    df = pd.DataFrame({\"Employee\": EMPLOYEES, \"Date\": dates})\n\n    return df\n```\n<end>"}
{"id": 285, "answer": " <start>\n```python\nfrom datetime import datetime\nimport pandas as pd\n\n# Constants\nEMPLOYEES = [\"John\", \"Alice\", \"Bob\", \"Charlie\", \"Dave\"]\n\ndef task_func(date_str):\n    start_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n    end_date = start_date + pd.Timedelta(days=9)\n\n    # Creating a DataFrame from the product of EMPLOYEES and dates\n    df = pd.DataFrame(list(product(EMPLOYEES, pd.date_range(start_date, end_date))), columns=[\"Employee\", \"Date\"])\n\n    return df\n```\n<end>"}
{"id": 295, "answer": " <start>\nimport subprocess\nimport shlex\nfrom datetime import datetime, timedelta\n\ndef task_func(script_path: str) -> dict:\n    start_time = datetime.now()\n    process = subprocess.Popen(shlex.split(\"Rscript --vanilla {}\".format(script_path)),\n                               stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    stdout, stderr = process.communicate()\n    end_time = datetime.now()\n\n    log_details = {\n        'Start Time': start_time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n        'End Time': end_time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n        'Stdout': stdout.decode('utf-8'),\n        'Stderr': stderr.decode('utf-8')\n    }\n\n    return log_details\n<end>"}
{"id": 296, "answer": " <start>\nimport subprocess\nimport shlex\nfrom datetime import int as _int\n\ndef task_func(script_path: str) -> dict:\n    start_time = _int(datetime.now())\n    process = subprocess.Popen(shlex.split(f\"/usr/bin/Rscript --vanilla {script_path}\"),\n                               stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    stdout, stderr = process.communicate()\n    end_time = _int(datetime.now())\n\n    log_details = {\n        'Start Time': str(start_time),\n        'End Time': str(end_time),\n        'Stdout': stdout.decode('utf-8'),\n        'Stderr': stderr.decode('utf-8')\n    }\n\n    return log_details\n<end>"}
{"id": 297, "answer": " <start>\n```python\nfrom datetime import datetime, timedelta, timezone\nfrom pathlib import Path\n\n# Constants\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n\ndef task_func(file_path):\n    if not Path(file_path).exists():\n        raise FileNotFoundError(f\"No such file or directory: '{file_path}'\")\n\n    creation_time = Path(file_path).stat().st_ctime\n    formatted_time = datetime.fromtimestamp(creation_time, timezone.utc).strftime(DATE_FORMAT)\n\n    return formatted_time\n```\n<end>"}
{"id": 298, "answer": " <start>\n```python\nfrom datetime import datetime\nfrom pathlib import Path\n\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n\ndef task_func(file_path):\n    if not Path(file_path).exists():\n        raise FileNotFoundError(f\"No such file or directory: '{file_path}'\")\n\n    creation_time = Path(file_path).stat().st_ctime\n    formatted_time = datetime.fromtimestamp(creation_time).strftime(DATE_FORMAT)\n\n    return formatted_time\n```\n<end>"}
{"id": 299, "answer": " <start>\nimport csv\nimport random\nfrom datetime import datetime, timedelta\n\n# Constants\nDATA = ['Temperature', 'Humidity', 'Pressure']\nRANGE = {\n    'Temperature': (-50, 50),\n    'Humidity': (0, 100),\n    'Pressure': (980, 1040)\n}\n\ndef task_func(file_name=\"data.csv\"):\n    current_date = datetime.today()\n    with open(file_name, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Time'] + DATA)\n\n        for _ in range(24):\n            row = [(current_date + timedelta(hours=_)).strftime('%H:00')]\n            for data_type in DATA:\n                min_val, max_val = RANGE[data_type]\n                row.append(random.uniform(min_val, max_val))\n            writer.writerow(row)\n\n    return file_name\n<end>"}
{"id": 300, "answer": " <start>\n```python\nimport csv\nimport random\nfrom datetime import datetime, timedelta, date, timezone\n\nDATA = ['Temperature', 'Humidity', 'Pressure']\nRANGE = {\n    'Temperature': (-50, 50),\n    'Humidity': (0, 100),\n    'Pressure': (980, 1040)\n}\n\ndef task_func(file_name=\"data.csv\"):\n    current_date = date.today()\n    current_time = time(hour=0)\n\n    with open(file_name, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Time'] + DATA)\n\n        for _ in range(24):\n            row = [(current_date + timedelta(hours=_)).strftime('%H:%M')]\n            for data_type in DATA:\n                min_val, max_val = RANGE[data_type]\n                row.append(random.uniform(min_val, max_val))\n            writer.writerow(row)\n\n    return file_name\n```\n<end>"}
{"id": 307, "answer": " <start>\nimport random\nimport seaborn as sns\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef task_func(length, range_limit=100, seed=0):\n    if range_limit <= 1:\n        raise ValueError(\"range_limit must be greater than 1\")\n\n    random.seed(seed)\n    np.random.seed(seed)\n\n    random_numbers = np.random.randint(1, range_limit, size=length)\n    random_numbers.sort()\n\n    # Initialize a fresh plot\n    plt.figure()\n    plot = sns.histplot(random_numbers, kde=False, bins=sns.HistogramBinParams(random_numbers))\n\n    return plot.axes, random_numbers\n<end>"}
{"id": 308, "answer": " <start>\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(length, range_limit=100, seed=0):\n    if range_limit <= 1:\n        raise ValueError(\"range_limit must be greater than 1\")\n\n    random_numbers = np.random.randint(1, range_limit, size=length)\n    random_numbers.sort()\n\n    # Initialize a fresh plot\n    plt.figure()\n    plot = sns.distplot(random_numbers, kde=False)\n\n    return plot.axes, random_numbers\n<end>"}
{"id": 293, "answer": " <start>\nimport pandas as pd\nfrom scipy.stats import pearsonr\n\ndef task_func(data):\n    df = pd.DataFrame(data)\n\n    # Check if the data frame has less than 2 rows\n    if len(df) < 2:\n        return float(\"nan\")\n\n    # Convert scores from string format to floats\n    df[\"Score_Float\"] = df[\"Score_String\"].astype(float)\n\n    # Encode categorical grades into numerical values based on their rank order\n    df[\"Grade_Encoded\"] = df[\"Grade\"].astype(\"category\").cat.codes\n\n    # Compute the Pearson correlation coefficient between the numerical scores and the encoded grades\n    correlation = pearsonr(df[\"Score_Float\"], df[\"Grade_Encoded\"])[0]\n\n    return correlation\n<end>"}
{"id": 294, "answer": " <start>\nimport pandas as pd\nfrom scipy.stats import pearsonr\n\ndef task_func(data):\n    df = pd.DataFrame(data)\n\n    # Check if the data frame has less than 2 rows\n    if len(df) < 2:\n        return float(\"nan\")\n\n    # Convert scores from string format to floats\n    df[\"Score_Float\"] = df[\"Score_String\"].astype(float)\n\n    # Encode categorical grades into numerical values based on their rank order\n    df[\"Grade_Encoded\"] = df[\"Grade\"].astype(\"category\").cat.codes\n\n    # Compute the Pearson correlation coefficient between the numerical scores and the encoded grades\n    correlation = pearsonr(df[\"Score_Float\"], df[\"Grade_Encoded\"])[0]\n\n    return correlation\n<end>"}
{"id": 311, "answer": " <start>\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom numpy.random import default_rng\n\ndef task_func(mu, sigma, seed=0):\n    # Create a random number generator instance\n    rng = default_rng(seed)\n\n    # Generate samples from the normal distribution\n    samples = rng.standard_normal(1000)\n\n    # Generate a KDE plot with shade instead of fill\n    mappable = sns.kdeplot(samples, shade=True)\n\n    # Add a colorbar to the plot\n    plt.colorbar(mappable=mappable.collections[0])\n\n    return mappable\n<end>"}
{"id": 312, "answer": " <start>\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib.colors import Normalize\nfrom matplotlib.cm import ScalarMappable\n\ndef task_func(mu, sigma, seed=0):\n    # Set the random seed\n    np.random.seed(seed)\n    # Generate samples from the normal distribution\n    samples = np.random.normal(mu, sigma, 1000)\n\n    # Create a new instance of the default RNG\n    rng = np.random.default_rng(seed)\n\n    # Generate a KDE plot\n    kde = sns.kdeplot(samples, fill=True, shade=True, linewidth=0)\n\n    # Normalize the KDE values\n    norm = Normalize(kde.get_array().min(), kde.get_array().max())\n\n    # Create a ScalarMappable for the KDE plot\n    sm = ScalarMappable(norm=norm, cmap=\"viridis\")\n    sm.set_array(kde.get_array())\n\n    # Add the ScalarMappable to the colorbar\n    cbar = plt.colorbar(sm)\n\n    return kde\n<end>"}
{"id": 309, "answer": " <start>\nimport pytz\nimport numpy as np\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\n\nSOLAR_CYCLE_YEARS = np.array([1986, 1996, 2008, 2019])\n\ndef task_func(date_str, from_tz, to_tz):\n    date = datetime.strptime(date_str, \"%Y-%m-%d %H:%M:%S\")\n    date = date.replace(tzinfo=pytz.timezone(from_tz))\n    converted_date = date.astimezone(pytz.timezone(to_tz))\n\n    solar_cycle_year = SOLAR_CYCLE_YEARS[np.argmin(np.abs(SOLAR_CYCLE_YEARS - converted_date.year))]\n    years_since_solar_cycle_year = abs(converted_date.year - solar_cycle_year)\n\n    solar_activity = np.cos(np.pi * years_since_solar_cycle_year / 11)\n\n    return solar_activity\n<end>"}
{"id": 310, "answer": " <start>\nimport pytz\nimport numpy as np\nfrom dateutil.parser import parse\nimport math\n\nSOLAR_CYCLE_YEARS = np.array([1986, 1996, 2008, 2019])\n\ndef task_func(date_str: str, from_tz: str, to_tz: str):\n    from_tz = pytz.timezone(from_tz)\n    to_tz = pytz.timezone(to_tz)\n    given_date = parse(date_str, fuzzy_with_tokens=True).replace(tzinfo=from_tz)\n    converted_date = given_date.astimezone(to_tz)\n\n    solar_cycle_year = SOLAR_CYCLE_YEARS[np.argmin(np.abs(SOLAR_CYCLE_YEARS - converted_date.year))]\n    years_since_solar_cycle_year = abs(converted_date.year - solar_cycle_year)\n\n    solar_activity = math.cos(math.pi * years_since_solar_cycle_year / 11)\n\n    return solar_activity\n<end>"}
{"id": 317, "answer": " <start>\n```python\nfrom datetime import datetime\nimport numpy as np\nfrom dateutil.relativedelta import relativedelta\n\nLEAP_SECONDS = np.array(\n    [\n        1972,\n        1973,\n        1974,\n        1975,\n        1976,\n        1977,\n        1978,\n        1979,\n        1980,\n        1981,\n        1982,\n        1983,\n        1985,\n        1988,\n        1990,\n        1993,\n        1994,\n        1997,\n        1999,\n        2006,\n        2009,\n        2012,\n        2015,\n        2016,\n        2020,\n    ]\n)\n\ndef task_func(date_str):\n    given_date = datetime.strptime(date_str, \"%Y-%m-%d %H:%M:%S\")\n    current_date = datetime.now()\n\n    # Calculate the number of days between the two dates, including leap years\n    days_diff = (current_date.year - given_date.year) + (current_date.month - given_date.month) + current_date.day - given_date.day\n\n    # Add the number of leap seconds that occurred between the two dates\n    days_diff += np.sum(relativedelta(years=1, leap=True)[given_date.year:current_date.year])\n\n    # Convert the number of days to seconds and add the remaining seconds\n    total_seconds = days_diff * 86400 + (current_date - given_date).seconds + (current_date.hour * 3600 + current_date.minute * 60 + current_date.second)\n\n    return int(total_seconds)\n```\n<end>"}
{"id": 316, "answer": " <start>\nimport os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\nfrom openpyxl.load_workbook import load_workbook\nfrom openpyxl.workbook import Workbook\nfrom openpyxl.cell.cell import copy\n\ndef task_func(source_directory: str, target_directory: str) -> int:\n    converted_files = 0\n    extensions = [\".txt\", \".docx\", \".xlsx\", \".csv\"]\n\n    if not os.path.exists(source_directory):\n        raise FileNotFoundError(\"source_directory must exist.\")\n    if not os.path.exists(target_directory):\n        os.makedirs(target_directory, exist_ok=True)\n\n    for root, dirs, files in os.walk(source_directory):\n        for file in files:\n            extension = Path(file).suffix\n            if extension in extensions:\n                filepath = os.path.join(root, file)\n                target_filepath = os.path.join(\n                    target_directory, Path(file).stem + \".csv\"\n                )\n                if extension == \".csv\":\n                    df = pd.read_csv(filepath)\n                elif extension == \".xlsx\":\n                    wb = load_workbook(filepath, read_only=True)\n                    df = pd.DataFrame(wb.active.values)\n                    df.columns = [copy(v) for v in wb.active.column_headers]\n                elif extension == \".docx\":\n                    doc = docx.Document(filepath)\n                    data = [p.text for p in doc.paragraphs]\n                    df = pd.DataFrame({\"Text\": data})\n                elif extension == \".txt\":\n                    with open(filepath, \"r\") as f:\n                        data = f.readlines()\n                    df = pd.DataFrame({\"Text\": data})\n\n                df.to_csv(target_filepath, index=False)\n                converted_files += 1\n\n    return converted_files\n<end>"}
{"id": 315, "answer": " <start>\nimport os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\nfrom docx.opc.container import Document\n\ndef task_func(source_directory: str, target_directory: str) -> int:\n    converted_files = 0\n    extensions = [\".txt\", \".docx\", \".xlsx\", \".csv\"]\n\n    if not os.path.exists(source_directory):\n        raise FileNotFoundError(\"source_directory must exist.\")\n    if not os.path.exists(target_directory):\n        os.makedirs(target_directory, exist_ok=True)\n\n    for root, dirs, files in os.walk(source_directory):\n        for file in files:\n            extension = Path(file).suffix\n            if extension in extensions:\n                filepath = os.path.join(root, file)\n                target_filepath = os.path.join(\n                    target_directory, Path(file).stem + \".csv\"\n                )\n                if extension == \".csv\":\n                    df = pd.read_csv(filepath)\n                elif extension == \".xlsx\":\n                    try:\n                        df = pd.read_excel(filepath, engine=\"openpyxl\")\n                    except ImportError:\n                        # Fallback to older openpyxl version\n                        wb = Document(filepath)\n                        sheets = wb.sheets\n                        df = pd.DataFrame()\n                        for sheet in sheets:\n                            data = [row[1:] for row in sheet.rows]\n                            df = pd.concat([df, pd.DataFrame(data, columns=sheet.columns)], ignore_index=True)\n                elif extension == \".docx\":\n                    doc = Document(filepath)\n                    data = [p.text for p in doc.paragraphs]\n                    df = pd.DataFrame({\"Text\": data})\n                elif extension == \".txt\":\n                    with open(filepath, \"r\") as f:\n                        data = f.readlines()\n                    df = pd.DataFrame({\"Text\": data})\n\n                df.to_csv(target_filepath, index=False)\n                converted_files += 1\n\n    return converted_files\n<end>"}
{"id": 320, "answer": " <start>\nimport subprocess\nimport os\nimport json\nfrom datetime import int\n\ndef task_func(script_name='backup.sh', log_file='/home/user/backup_log.json'):\n    log_data = {}\n\n    if not os.path.isfile(script_name):\n        raise FileNotFoundError(f\"Script {script_name} does not exist.\")\n\n    start_time = int(datetime.now())\n    log_data['start_time'] = start_time\n\n    try:\n        exit_status = subprocess.call(['./' + script_name])\n    except Exception as e:\n        raise RuntimeError(f\"Failed to run {script_name}: {str(e)}\")\n\n    end_time = int(datetime.now())\n    log_data['end_time'] = end_time\n    log_data['exit_status'] = exit_status\n\n    with open(log_file, 'w') as f:\n        json.dump(log_data, f)\n\n    return log_data\n<end>"}
{"id": 319, "answer": " <start>\nimport subprocess\nimport os\nimport json\nfrom datetime import datetime, timedelta\n\ndef task_func(script_name='backup.sh', log_file='/home/user/backup_log.json'):\n    log_data = {}\n\n    if not os.path.isfile(script_name):\n        raise FileNotFoundError(\"Script {} does not exist.\".format(script_name))\n\n    start_time = datetime.now()\n    log_data['start_time'] = start_time.strftime('%Y-%m-%d %H:%M:%S')\n\n    try:\n        exit_status = subprocess.call(['./' + script_name])\n    except Exception as e:\n        raise RuntimeError(\"Failed to run {}: {}\".format(script_name, str(e)))\n\n    end_time = datetime.now()\n    log_data['end_time'] = end_time.strftime('%Y-%m-%d %H:%M:%S')\n    log_data['exit_status'] = exit_status\n\n    with open(log_file, 'w') as f:\n        json.dump(log_data, f)\n\n    return log_data\n<end>"}
{"id": 318, "answer": " <start>\n```python\nfrom datetime import datetime\nimport numpy as np\nfrom dateutil.relativedelta import relativedelta\n\nLEAP_SECONDS = np.array(\n    [\n        1972,\n        1973,\n        1974,\n        1975,\n        1976,\n        1977,\n        1978,\n        1979,\n        1980,\n        1981,\n        1982,\n        1983,\n        1985,\n        1988,\n        1990,\n        1993,\n        1994,\n        1997,\n        1999,\n        2006,\n        2009,\n        2012,\n        2015,\n        2016,\n        2020,\n    ]\n)\n\ndef task_func(date_str):\n    given_date = datetime.strptime(date_str, \"%Y-%m-%d %H:%M:%S\")\n    current_date = datetime.now()\n\n    total_seconds = (current_date - given_date).total_seconds()\n\n    # Count leap seconds that occurred between the two dates\n    leap_seconds = np.sum(LEAP_SECONDS >= given_date.year)\n\n    total_seconds += leap_seconds\n\n    return int(total_seconds)\n```\n<end>"}
{"id": 322, "answer": " <start>\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Subplot\n\ndef task_func(car_dict):\n    car_data = list(car_dict.items())\n    df = pd.DataFrame(car_data, columns=['Car', 'Color'])\n\n    figure, ax = plt.subplots()\n\n    color_counts = df[\"Color\"].value_counts()\n\n    ax.bar(color_counts.index, color_counts.values, color=\"maroon\", width=0.4)\n\n    ax.set_xlabel(\"Color\")\n    ax.set_ylabel(\"Frequency\")\n    ax.set_title(\"Distribution of Vehicle Colors\")\n\n    return df, ax\n<end>"}
{"id": 321, "answer": " <start>\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\ndef task_func(car_dict):\n    car_data = list(car_dict.items())\n    df = pd.DataFrame(car_data, columns=['Car', 'Color'])\n\n    # Create the bar chart visualization\n    color_counts = df[\"Color\"].value_counts()\n    unique_colors = df[\"Color\"].unique()\n    color_map = {color: i for i, color in enumerate(unique_colors)}\n    cmap = ListedColormap(unique_colors)\n\n    figure = plt.figure(figsize=(10, 6))\n    ax = figure.add_subplot(111)\n\n    ax.bar(color_counts.index, color_counts.values, color=cmap(color_map[color_counts.index]))\n    ax.set_xlabel(\"Color\")\n    ax.set_ylabel(\"Frequency\")\n    ax.set_title(\"Distribution of Vehicle Colors\")\n\n    return df, ax\n<end>"}
{"id": 333, "answer": " <start>\nimport pandas as pd\nfrom datetime import datetime\nfrom dateutil.parser import parse\nimport holidays\n\ndef task_func(start_date=datetime(2023, 1, 1), end_date=datetime(2023, 12, 31), country='US'):\n    if not isinstance(start_date, (datetime, str)) or not isinstance(end_date, (datetime, str)):\n        raise ValueError(\"start_date and end_date must be datetime or string objects.\")\n\n    if isinstance(start_date, str):\n        start_date = parse(start_date)\n    if isinstance(end_date, str):\n        end_date = parse(end_date)\n\n    if start_date > end_date:\n        raise ValueError(\"start_date must not be after end_date.\")\n\n    # For UK in holidays 0.29, we need to specify state\n    if country == 'GB':\n        country_holidays = holidays.CountryHoliday(country, state='England')\n    else:\n        country_holidays = holidays.CountryHoliday(country)\n\n    dates = pd.date_range(start_date, end_date)\n    business_days = [date for date in dates if date.weekday() < 5 and date not in country_holidays]\n\n    return [date for date in business_days if isinstance(date, datetime)]\n<end>"}
{"id": 335, "answer": " <start>\nimport re\nimport json\nfrom requests import get, Request, Response\n\ndef task_func(data_url: str) -> list:\n    try:\n        response = get(data_url)\n        response.raise_for_status()  # \u68c0\u67e5\u54cd\u5e94\u72b6\u6001\u7801\n        data = response.json()\n        data_string = json.dumps(data['names'])\n        names = re.findall(r'(?<!\\[)(\\w+)(?![\\w]*\\])', data_string)\n        return names\n    except Exception as e:\n        return \"Invalid url input\"\n<end>"}
{"id": 326, "answer": " <start>\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(mean, std_dev, num_samples):\n    samples = norm.rvs(mean, std_dev, size=num_samples)\n\n    fig, ax = plt.subplots()\n    ax.hist(samples, bins=30, density=True, alpha=0.6, color='g')\n\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mean, std_dev)\n    ax.plot(x, p, 'k', linewidth=2)\n\n    title = f\"Fit results: mean = {mean:.2f}, std = {std_dev:.2f}\"\n    ax.set_title(title)\n\n    return samples, fig\n<end>"}
{"id": 325, "answer": " <start>\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(mean, std_dev, num_samples):\n    samples = np.random.normal(mean, std_dev, num_samples)\n\n    fig, ax = plt.subplots()\n    ax.hist(samples, bins=30, density=True, alpha=0.6, color='g')\n\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mean, std_dev)\n    ax.plot(x, p, 'k', linewidth=2)\n\n    title = f\"Fit results: mean = {mean:.2f}, std = {std_dev:.2f}\"\n    ax.set_title(title)\n\n    return samples, fig\n<end>"}
{"id": 337, "answer": " <start>\nimport sqlite3\nimport pandas as pd\nimport csv\nfrom io import StringIO\nfrom pandas.io.sql import Series\n\n# Constants\nDATABASE_NAME = 'test.db'\nTABLE_NAME = 'test_table'\n\ndef task_func(csv_input):\n    # Check if the input is a StringIO object or a file path\n    if isinstance(csv_input, StringIO):\n        dr = csv.DictReader(csv_input)\n    else:\n        with open(csv_input, 'r', newline='') as f:\n            dr = csv.DictReader(f)\n\n    conn = sqlite3.connect(DATABASE_NAME)\n    cursor = conn.cursor()\n\n    # Create table and insert data\n    cursor.execute(f'DROP TABLE IF EXISTS {TABLE_NAME}')\n    cursor.execute(f'CREATE TABLE {TABLE_NAME} ({\", \".join([f\"{col} TEXT\" for col in dr.fieldnames])})')\n    for index, row in dr:\n        Series(index=index, data=row).to_sql(TABLE_NAME, conn, if_exists='append')\n\n    conn.commit()\n    dataframe = pd.read_sql_query(f'SELECT * from {TABLE_NAME}', conn)\n\n    conn.close()\n\n    return dataframe\n<end>"}
{"id": 338, "answer": " <start>\nimport sqlite3\nimport pandas as pd\n\ndef task_func(csv_input):\n    # Read CSV data into a pandas DataFrame\n    df = pd.read_csv(csv_input)\n\n    # Connect to the SQLite database\n    conn = sqlite3.connect(DATABASE_NAME)\n\n    # Write DataFrame to SQLite table\n    df.to_sql(TABLE_NAME, conn, if_exists='replace')\n\n    # Query the table and return the data as a DataFrame\n    dataframe = pd.read_sql_query(f\"SELECT * FROM {TABLE_NAME}\", conn)\n\n    # Close the connection\n    conn.close()\n\n    return dataframe\n<end>"}
{"id": 340, "answer": " <start>\nimport numpy as np\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(L):\n    stats = {\n        'mean': np.mean(L),\n        'median': np.median(L),\n        'mode': Counter(L).most_common(1)[0][0],\n        'std_dev': np.std(L)\n    }\n\n    fig, ax = plt.subplots()\n    ax.hist(L, bins='auto')\n    ax.set_title('Histogram of Data')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return stats, fig, ax\n<end>"}
{"id": 336, "answer": " <start>\nimport re\nimport json\nfrom requests import get, Session\n\ndef task_func(data_url: str) -> list:\n    session = Session()\n    response = session.get(data_url)\n    response.raise_for_status()\n    data = response.json()\n    data_string = json.dumps(data['names'])\n    names = re.findall(r'(?<!\\[)(\\w+)(?![\\w]*\\])', data_string)\n    return names\n<end>"}
{"id": 339, "answer": " <start>\nimport numpy as np\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(L):\n    mean = np.mean(L)\n    median = np.median(L)\n    mode = Counter(L).most_common(1)[0][0]\n    std_dev = np.std(L)\n\n    # Create a figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot the histogram\n    ax.hist(L, bins=10)\n    ax.set_title('Histogram of Data')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return {'mean': mean, 'median': median, 'mode': mode, 'std_dev': std_dev, 'plot': ax}\n<end>"}
{"id": 341, "answer": " <start>\nimport numpy as np\n\ndef task_func(input_list):\n    degrees = np.degrees(np.array(input_list))\n    sorted_list = np.sort(degrees)\n    mean = np.mean(sorted_list)\n    median = np.median(sorted_list)\n    mode = np.mode(sorted_list)\n\n    fft = np.abs(np.fft.fft(degrees))\n    mean_fft = np.mean(fft)\n    median_fft = np.median(fft)\n    mode_fft = np.mode(fft)\n\n    return (np.round(mean) if np.isscalar(mean) else mean,\n            np.round(median) if np.isscalar(median) else median,\n            mode,\n            np.round(mean_fft),\n            np.round(median_fft),\n            np.round(mode_fft))\n<end>"}
{"id": 342, "answer": " <start>\nimport numpy as np\n\ndef task_func(input_list):\n    degrees = np.degrees(input_list)\n    fft = np.abs(np.fft.fft(degrees))\n\n    sorted_list = np.sort(input_list)\n    mean = np.mean(sorted_list)\n    median = np.median(sorted_list)\n    mode = np.mode(sorted_list)\n\n    mean_fft = round(np.mean(fft))\n    median_fft = round(np.median(fft))\n    mode_fft = round(np.mode(fft))\n\n    return (mean, median, mode, mean_fft, median_fft, mode_fft)\n<end>"}
{"id": 349, "answer": " <start>\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Subplot\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"The input df is not a DataFrame\")\n\n    value_counts = df['value'].value_counts()\n    fig, ax = plt.subplots()\n    ax = Subplot(fig, 111)\n    ax.bar(range(len(value_counts)), value_counts.values)\n    ax.set_xticks(range(len(value_counts)))\n    ax.set_xticklabels(value_counts.index)\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Count')\n    ax.set_title('Value Distribution')\n    return ax\n<end>"}
{"id": 350, "answer": " <start>\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Subplot\nfrom matplotlib.axes import unpack_labeled_data\n\ndef task_func(df):\n\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"The input df is not a DataFrame\")\n\n    value_counts = df['value'].value_counts()\n\n    @unpack_labeled_data\n    def plot_bar(ax, data):\n        ax.bar(data['index'], data['values'])\n        ax.set_xlabel('Value')\n        ax.set_ylabel('Count')\n        ax.set_title('Value Distribution')\n\n    fig, ax = plt.subplots()\n    plot_bar(ax, value_counts)\n\n    return ax\n<end>"}
{"id": 351, "answer": " <start>\nimport pandas as pd\nfrom statistics import mean\nimport random\n\n# Constants for generating the report data\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\n\ndef task_func(additional_fields=[]):\n    FIELDS_ALL = FIELDS + additional_fields\n\n    # Generate random grades for each student in each field\n    report_data = [(field, pd.Series(random.randint(0, 100, len(STUDENTS)))) for field in FIELDS_ALL]\n\n    # Create DataFrame from the generated data with ordered columns\n    df = pd.DataFrame(report_data, columns=['Subject', 'Grades'])\n    df.set_index('Subject', inplace=True)\n\n    # Calculate the average grade for each student\n    df['Average Grade'] = df.groupby(level=0).mean('Grades')\n\n    # Calculate the average grade for each subject\n    df.loc['Average'] = df.groupby(level=0).mean()\n\n    return df\n<end>"}
{"id": 352, "answer": " <start>\nimport pandas as pd\nimport random\n\n# Constants for generating the report data\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\n\ndef task_func(additional_fields=[]):\n\n    FIELDS_ALL = FIELDS + additional_fields\n\n    # Generate random grades for each student in each field\n    report_data = {field: [random.randint(0, 100) for _ in STUDENTS] for field in FIELDS_ALL}\n\n    # Create DataFrame from the generated data\n    df = pd.DataFrame(report_data)\n\n    # Set index to students\n    df.index = STUDENTS\n\n    # Calculate the average grade for each student\n    df['Average Grade'] = df.mean(axis=1)\n\n    # Calculate the average grade for each subject\n    df.loc['Average'] = df.mean(axis=0)\n\n    return df\n<end>"}
{"id": 345, "answer": " <start>\nimport numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\n\ndef task_func(L, M, N, audio_file):\n    # Ensure the audio file exists\n    if not os.path.isfile(audio_file):\n        raise FileNotFoundError(f\"{audio_file} does not exist.\")\n\n    # Read the audio file\n    data, samplerate = sf.read(audio_file)\n\n    # Calculate the sound pressure level (SPL)\n    spl = 20 * np.log10(np.sqrt(np.mean(data ** 2)))\n\n    # Generate the matrix\n    matrix = np.array(L).reshape(M, N)\n\n    # Normalize the matrix to match the SPL\n    matrix = matrix / np.max(matrix) * spl\n\n    # Generate the spectrogram\n    D = librosa.amplitude_to_db(np.abs(librosa.stft(matrix)), ref=np.max, sr=samplerate)\n\n    # Display the spectrogram\n    fig = plt.figure(figsize=(10, 5))\n    ax = fig.add_subplot(111)\n    librosa.display.specshow(D, x_axis='time', y_axis='log', ax=ax)\n    ax.set_title('Spectrogram')\n    fig.colorbar(ax=ax, format='%+2.0f dB')\n\n    return matrix, fig\n<end>"}
{"id": 346, "answer": " <start>\nimport numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\n\ndef task_func(L, M, N, audio_file):\n    # Ensure the audio file exists\n    if not os.path.isfile(audio_file):\n        raise FileNotFoundError(f\"{audio_file} does not exist.\")\n\n    # Read the audio file\n    data, samplerate = sf.read(audio_file)\n\n    # Calculate the sound pressure level (SPL)\n    spl = 20 * np.log10(np.sqrt(np.mean(data ** 2)))\n\n    # Generate the matrix\n    matrix = np.array(L).reshape(M, N)\n\n    # Normalize the matrix to match the SPL\n    matrix = matrix / np.max(matrix) * spl\n\n    # Generate the spectrogram using the new version of librosa\n    D = librosa.feature.spectral.melspectrogram(y=matrix, sr=samplerate, n_mels=N, n_fft=M, hop_length=1)\n\n    # Display the spectrogram with a logarithmic scale for frequency and a linear scale for time\n    fig = plt.figure(figsize=(10, 4))\n    ax = fig.add_subplot(111)\n    img = librosa.display.specshow(librosa.power_to_db(D, ref=np.max), x_axis='time', y_axis='mel', sr=samplerate, ax=ax)\n    fig.colorbar(img, ax=ax, format='%+2.0f dB')\n    ax.set(title='Spectrogram')\n\n    return matrix, fig\n<end>"}
{"id": 359, "answer": " <start>\n```python\nfrom texttable import Texttable\nimport os\nimport psutil\n\ndef task_func():\n    cpu_usage = psutil.Process(os.getpid()).cpu_percent(interval=1)\n    memory_info = psutil.Process(os.getpid()).memory_info()\n    disk_usage = psutil.disk_usage(os.sep)\n\n    table = Texttable()\n    table.add_rows([\n        ['Item', 'Value'],\n        ['CPU Usage (%)', cpu_usage],\n        ['Memory Usage (%)', memory_info.percent],\n        ['Disk Usage (%)', disk_usage.percent]\n    ])\n    return table.draw()\n```\n<end>"}
{"id": 360, "answer": " <start>\n```python\nfrom texttable import Texttable\nimport psutil\n\ndef task_func():\n    cpu_usage = psutil.cpu_percent(interval=1)\n    memory_info = psutil.virtual_memory()\n    disk_usage = psutil.disk_usage('/')\n\n    table = Texttable()\n    table.set_cols_dtype([None, None])\n    table.add_rows([\n        ['Item', 'Value'],\n        ['CPU Usage (%)', cpu_usage],\n        ['Memory Usage (%)', memory_info.percent],\n        ['Disk Usage (%)', disk_usage.percent]\n    ])\n    return table.draw()\n```\n<end>"}
{"id": 363, "answer": " <start>\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef task_func(n_points=100, random_seed=None):\n    if not isinstance(n_points, int):\n        raise TypeError(\"n_points must be an integer\")\n    if n_points <= 0:\n        raise ValueError(\"n_points must be positive\")\n    np.random.seed(random_seed)\n    points = np.random.rand(n_points, 3)\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection=Axes3D())\n    ax.scatter(points[:, 0], points[:, 1], points[:, 2])\n\n    return points, ax\n<end>"}
{"id": 364, "answer": " <start>\nimport numpy as np\nfrom matplotlib.artist import namedtuple\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef task_func(n_points=100, random_seed=None):\n    np.random.seed(random_seed)\n    points = np.random.random((n_points, 3))\n\n    Point = namedtuple('Point', ['x', 'y', 'z'])\n    points = [Point(points[i][0], points[i][1], points[i][2]) for i in range(n_points)]\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection=Axes3D())\n    ax.scatter(*zip(*[p.x, p.y, p.z] for p in points))\n\n    return points, ax\n<end>"}
{"id": 365, "answer": " <start>\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpy.random\n\ndef task_func(n, seed=None):\n    # Validate input type\n    if not isinstance(n, int):\n        raise TypeError(\"n must be an integer\")\n    if n < 0:\n        raise ValueError(\"n must be non-negative\")\n\n    # Setting the random seed for reproducibility\n    rng = numpy.random.default_rng(seed)\n\n    # Generating random points\n    x = rng.random()\n    y = rng.random()\n\n    # Plotting\n    fig, ax = plt.subplots()\n    ax.scatter(x, y)\n    ax.set_title(\"Scatter plot of random points\")\n    ax.set_xlabel(\"X\")\n    ax.set_ylabel(\"Y\")\n\n    return fig, list(zip(x, y))\n<end>"}
{"id": 366, "answer": " <start>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(n, seed=0):\n    # Setting the random seed for reproducibility\n    np.random.seed(seed)\n\n    # Generating random points\n    x = np.random.rand(n)\n    y = np.random.rand(n)\n\n    # Creating a new figure and axes\n    fig, ax = plt.subplots()\n\n    # Plotting\n    ax.scatter(x, y)\n    ax.set_title(\"Scatter plot of random points\")\n    ax.set_xlabel(\"X\")\n    ax.set_ylabel(\"Y\")\n\n    # Returning the generated plot and the points\n    return fig, list(zip(x, y))\n<end>"}
{"id": 371, "answer": " <start>\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\n\ndef task_func(X, Y):\n\n    def func(x, *params):\n        return params[0] * x ** 2 + params[1] * x + params[2]\n\n    popt, pcov = curve_fit(func, X, Y)\n\n    fig, ax = plt.subplots()\n    ax.scatter(X, Y)\n    ax.plot(X, func(X, *popt), \"r-\")\n\n    return popt, ax\n<end>"}
{"id": 372, "answer": " <start>\nimport matplotlib.pylab as plt\nfrom scipy.optimize import curve_fit\n\ndef task_func(X, Y):\n\n    def func(x, *p):\n        return sum(p[i] * x**i for i in range(len(p)))\n\n    popt, pcov = curve_fit(func, X, Y, p0=[0, 0, 0])\n\n    fig, ax = plt.subplots()\n    ax.scatter(X, Y)\n    ax.plot(X, func(X, *popt), \"r-\")\n\n    return list(popt), ax\n<end>"}
{"id": 373, "answer": " <start>\n```python\nfrom datetime import datetime\nimport pytz\nimport re\nfrom faker import Faker\n\ndef task_func(epoch_milliseconds: int, seed: int = 0, timezones: list = [\"UTC\"]) -> dict:\n    faker = Faker.create(seed=seed)\n\n    event_datetime = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n\n    event_name = faker.first_name()\n\n    validated_timezones = []\n    utc_offset_regex = r\"^UTC([+-])(0[0-9]|1[0-4]):([0-5][0-9])$\"\n    for tz in timezones:\n        if (\n            (tz == \"UTC\")\n            or (re.match(utc_offset_regex, tz))\n            or (tz in pytz.all_timezones)\n        ):\n            validated_timezones.append(tz)\n    if not validated_timezones:\n        validated_timezones = [\"UTC\"]\n\n    timezone = faker.random_element(elements=validated_timezones)\n\n    event_schedule = {\n        event_name: [\n            {\n                \"date\": event_datetime.date(),\n                \"time\": event_datetime.time(),\n                \"timezone\": timezone,\n            }\n        ]\n    }\n\n    return event_schedule\n```\n<end>"}
{"id": 374, "answer": " <start>\n```python\nfrom datetime import datetime\nimport pytz\nimport re\nfrom faker import Faker\n\ndef task_func(epoch_milliseconds: int, seed: int = 0, timezones: list = [\"UTC\"]) -> dict:\n    faker = Faker.create(seed=seed)\n\n    event_datetime = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n\n    event_name = faker.unique.first_name()\n\n    validated_timezones = []\n    utc_offset_regex = r\"^UTC([+-])(0[0-9]|1[0-4]):([0-5][0-9])$\"\n    for tz in timezones:\n        if (\n            (tz == \"UTC\")\n            or (re.match(utc_offset_regex, tz))\n            or (tz in pytz.all_timezones)\n        ):\n            validated_timezones.append(tz)\n    if not validated_timezones:\n        validated_timezones = [\"UTC\"]\n\n    timezone = faker.random_element(elements=validated_timezones)\n\n    event_schedule = {\n        event_name: [\n            {\n                \"date\": event_datetime.date(),\n                \"time\": event_datetime.time(),\n                \"timezone\": timezone,\n            }\n        ]\n    }\n\n    return event_schedule\n```\n<end>"}
{"id": 334, "answer": " <start>\nimport pandas as pd\nfrom datetime import datetime\nfrom holidays.calendars import GregorianCalendar\nfrom holidays.holiday_base import HolidayBase\nfrom holidays.holiday_collection import HolidayCollection\n\ndef task_func(start_date=datetime(2023, 1, 1), end_date=datetime(2023, 12, 31), country='US'):\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"start_date and end_date must be datetime objects.\")\n    if start_date > end_date:\n        raise ValueError(\"start_date must not be after end_date.\")\n\n    calendar = GregorianCalendar()\n    ltu_holidays = HolidayCollection([HolidayBase(**holidays.countries.LTU.get_list())], calendar=calendar)\n    uy_holidays = HolidayCollection([HolidayBase(**holidays.countries.UY.get_list())], calendar=calendar)\n    aw_holidays = HolidayCollection([HolidayBase(**holidays.countries.AW.get_list())], calendar=calendar)\n    eth_holidays = HolidayCollection([HolidayBase(**holidays.countries.ETH.get_list())], calendar=calendar)\n    svn_holidays = HolidayCollection([HolidayBase(**holidays.countries.SVN.get_list())], calendar=calendar)\n    se_holidays = HolidayCollection([HolidayBase(**holidays.countries.SE.get_list())], calendar=calendar)\n    et_holidays = HolidayCollection([HolidayBase(**holidays.countries.ET.get_list())], calendar=calendar)\n    nic_holidays = HolidayCollection([HolidayBase(**holidays.countries.NIC.get_list())], calendar=calendar)\n    grc_holidays = HolidayCollection([HolidayBase(**holidays.countries.GRC.get_list())], calendar=calendar)\n    dk_holidays = HolidayCollection([HolidayBase(**holidays.countries.DK.get_list())], calendar=calendar)\n    gr_holidays = HolidayCollection([HolidayBase(**holidays.countries.GR.get_list())], calendar=calendar)\n    ls_holidays = HolidayCollection([HolidayBase(**holidays.countries.LS.get_list())], calendar=calendar)\n    dj_holidays = HolidayCollection([HolidayBase(**holidays.countries.DJI.get_list())], calendar=calendar)\n    nld_holidays = HolidayCollection([HolidayBase(**holidays.countries.NLD.get_list())], calendar=calendar)\n    lt_holidays = HolidayCollection([HolidayBase(**holidays.countries.LT.get_list())], calendar=calendar)\n    isr_holidays = HolidayBase(**holidays.countries.ISR.get_list())\n    col_holidays = HolidayCollection([HolidayBase(**holidays.countries.COL.get_list())], calendar=calendar)\n    lso_holidays = HolidayCollection([HolidayBase(**holidays.countries.LSO.get_list())], calendar=calendar)\n    bd_holidays = HolidayCollection([HolidayBase(**holidays.countries.BD.get_list())], calendar=calendar)\n    us_holidays = HolidayCollection([HolidayBase(**holidays.countries.united_states.US.get_list())], calendar=calendar)\n    est_holidays = HolidayCollection([HolidayBase(**holidays.countries.EST.get_list())], calendar=calendar)\n    deu_holidays = HolidayCollection([HolidayBase(**holidays.countries.DEU.get_list())], calendar=calendar)\n\n    country_holidays = None\n    if country == 'LTU':\n        country_holidays = ltu_holidays\n    elif country == 'UY':\n        country_holidays = uy_holidays\n    elif country == 'AW':\n        country_holidays = aw_holidays\n    elif country == 'ETH':\n        country_holidays = eth_holidays\n    elif country == 'SVN':\n        country_holidays = svn_holidays\n    elif country == 'SE':\n        country_holidays = se_holidays\n    elif country == 'ET':\n        country_holidays = et_holidays\n    elif country == 'NIC':\n        country_holidays = nic_holidays\n    elif country == 'GRC':\n        country_holidays = grc_holidays\n    elif country == 'DK':\n        country_holidays = dk_holidays\n    elif country == 'GR':\n        country_holidays = gr_holidays\n    elif country == 'LS':\n        country_holidays = ls_holidays\n    elif country == 'DJI':\n        country_holidays = dj_holidays\n    elif country == 'NLD':\n        country_holidays = nld_holidays\n    elif country == 'LT':\n        country_holidays = lt_holidays\n    elif country == 'ISR':\n        country_holidays = isr_holidays\n    elif country == 'COL':\n        country_holidays = col_holidays\n    elif country == 'LSO':\n        country_holidays = lso_holidays\n    elif country == 'BD':\n        country_holidays = bd_holidays\n    elif country == 'US':\n        country_holidays = us_holidays\n    elif country == 'EST':\n        country_holidays = est_holidays\n    elif country == 'DEU':\n        country_holidays = deu_holidays\n\n    dates = pd.date_range(start_date, end_date)\n    business_days = [date for date in dates if date.weekday() < 5 and date not in country_holidays]\n\n    return business_days\n<end>"}
{"id": 356, "answer": " <start>\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport pandas as pd\n\ndef task_func(documents):\n    vectorizer = TfidfVectorizer(tokenizer=word_tokenize)\n    tfidf_matrix = vectorizer.fit_transform(documents)\n    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return tfidf_df\n<end>"}
{"id": 379, "answer": " <start>\nimport collections\nimport itertools\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import BarContainer\n\nITEMS = ['apple', 'banana']\n\ndef task_func(a, b, items=ITEMS):\n    # Combine lists\n    combined = list(itertools.chain(a, b))\n    # Count occurrences of each item\n    counter = collections.Counter(combined)\n    # Get counts for predefined items\n    item_counts = [counter.get(item, 0) for item in items]\n\n    # Create a bar plot\n    fig, ax = plt.subplots()\n    container = ax.bar(range(len(items)), item_counts)\n    ax.set_xlabel('Items')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Item Frequency in Combined List')\n    ax.set_xticks(range(len(items)))\n    ax.set_xticklabels(items)\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return ax, container\n<end>"}
{"id": 380, "answer": " <start>\nimport collections\nimport itertools\nfrom matplotlib.cbook import Sorter\nimport matplotlib.pyplot as plt\n\ndef task_func(a, b, items=['apple', 'banana']):\n    # Combine lists\n    combined = list(itertools.chain(a, b))\n\n    # Count occurrences of each item\n    counter = collections.Counter(combined)\n\n    # Sort items by frequency\n    sorted_items = Sorter().sort_list(list(counter.keys()), key=lambda x: counter[x])\n\n    # Get counts for predefined items\n    item_counts = [counter.get(item, 0) for item in sorted_items]\n\n    # Create a bar plot\n    fig, ax = plt.subplots()\n    ax.bar(sorted_items, item_counts, color='skyblue')\n    ax.set_xlabel('Items')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Item Frequency in Combined List')\n    ax.set_xticklabels(sorted_items, rotation=45)\n    ax.legend(sorted_items, loc='best')\n    plt.tight_layout()  # Adjust layout to make room for item labels\n\n    return ax\n<end>"}
{"id": 383, "answer": " <start>\n```python\nimport csv\nimport os\nimport shutil\nfrom datetime import datetime, timedelta, date, timezone\nfrom random import choice\nfrom pathlib import Path\n\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = Path('./output')\nBACKUP_DIR = OUTPUT_DIR / 'backup'\n\ndef task_func(hours: int, output_dir: Path = OUTPUT_DIR):\n    FILE_PATH = output_dir / 'weather_data.csv'\n    BACKUP_PATH = BACKUP_DIR / FILE_PATH.name\n\n    data = [['Time', 'Condition']]\n    current_date = date.today()\n    for _ in range(hours):\n        current_time = time(hour=current_date.hour, minute=current_date.minute, second=current_date.second, microsecond=0, tzinfo=timezone.utc)\n        row = [current_time.strftime('%H:%M:%S.%f'), choice(WEATHER_CONDITIONS)]\n        data.append(row)\n        current_time += timedelta(hours=1)\n\n    with open(FILE_PATH, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerows(data)\n\n    if not BACKUP_DIR.exists():\n        BACKUP_DIR.mkdir(parents=True, exist_ok=True)\n    shutil.copy(FILE_PATH, BACKUP_PATH)\n\n    return str(FILE_PATH)\n<end>\n```\n\nThis refactored code uses the new version of the datetime library and the pathlib module for better file handling. It also uses the timezone.utc and timedelta classes from the datetime module to handle time more efficiently."}
{"id": 384, "answer": " <start>\n```python\nimport csv\nimport os\nimport shutil\nfrom datetime import datetime, timedelta\nfrom random import choice\nfrom pathlib import Path\n\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = Path('./output')\nBACKUP_DIR = OUTPUT_DIR / 'backup'\n\ndef task_func(hours):\n    FILE_PATH = OUTPUT_DIR / 'weather_data.csv'\n    BACKUP_PATH = BACKUP_DIR / FILE_PATH.name\n\n    data = [['Time', 'Condition']]\n    for _ in range(hours):\n        time_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        condition = choice(WEATHER_CONDITIONS)\n        data.append([time_str, condition])\n\n    with open(FILE_PATH, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerows(data)\n\n    if not BACKUP_DIR.exists():\n        BACKUP_DIR.mkdir(parents=True, exist_ok=True)\n    shutil.copy(FILE_PATH, BACKUP_PATH)\n\n    return str(FILE_PATH)\n```\n<end>"}
{"id": 385, "answer": " <start>\nimport seaborn as sns\nfrom pandas.core.frame import DataFrame\n\ndef task_func(df: DataFrame, tuples: list, n_plots: int):\n    if not df.empty:\n        df = df.loc[~df.isin(tuples).all(axis=1)]\n\n    plots = []\n    if n_plots > 0 and not df.empty:\n        available_columns = df.columns.tolist()\n        for _ in range(min(n_plots, len(available_columns) // 2)):  # Ensure we have enough columns\n            # Randomly select two columns for pairplot\n            selected_columns = sample(available_columns, 2)\n            plot = sns.pairplot(df[selected_columns], diag_kws={'bw': 0.2})\n            plots.append(plot)\n\n    return df, plots\n<end>"}
{"id": 386, "answer": " <start>\nimport seaborn as sns\nfrom pandas.core.frame import DataFrame\nfrom random import sample\n\ndef task_func(df: DataFrame, tuples: list, n_plots: int):\n    if not df.empty:\n        df = df.drop(df[df.apply(tuple, axis=1).isin(tuples)].index)\n\n    plots = []\n    if n_plots > 0 and not df.empty:\n        available_columns = df.columns.tolist()\n        for _ in range(min(n_plots, len(available_columns) // 2)):  # Ensure we have enough columns\n            # Randomly select two columns for pairplot\n            selected_columns = sample(available_columns, 2)\n            plot = sns.pairplot(df[selected_columns], diag_kws={'bw': 0.2})\n            plots.append(plot)\n\n    return df, plots\n<end>"}
{"id": 387, "answer": " <start>\nimport pandas as pd\nimport seaborn as sns\n\ndef task_func(goals, penalties):\n    # Constants\n    TEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\n\n    data = [(team, goals.get(team, 0), penalties.get(team, 0)) for team in TEAMS]\n\n    df = pd.DataFrame(data, columns=['Team', 'Goals', 'Penalties'])\n\n    plot = sns.pairplot(df, hue='Team')\n\n    return df, plot\n<end>"}
{"id": 388, "answer": " <start>\nimport pandas as pd\nimport seaborn as sns\n\ndef task_func(goals, penalties):\n    # Constants\n    TEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\n\n    data = [(team, goals.get(team, 0), penalties.get(team, 0)) for team in TEAMS]\n\n    df = pd.DataFrame(data, columns=['Team', 'Goals', 'Penalties'])\n\n    plot = sns.pairplot(df, hue='Team')\n\n    return df, plot\n<end>"}
{"id": 389, "answer": " <start>\nfrom random import choice\nimport pytz\nfrom dateutil.parser import parse\n\n# Constants\nTIMEZONES = ['America/New_York', 'Europe/London', 'Asia/Shanghai', 'Asia/Tokyo', 'Australia/Sydney']\n\ndef task_func(date_str, from_tz):\n    from_tz = pytz.timezone(from_tz)\n    to_tz = pytz.timezone(choice(TIMEZONES))\n    given_date = parse(date_str)\n    given_date = default_tzinfo(given_date, tz=from_tz)\n    converted_date = given_date.astimezone(to_tz)\n    return converted_date.strftime('%Y-%m-%d %H:%M:%S'), to_tz.zone\n<end>"}
{"id": 390, "answer": " <start>\nfrom random import choice\nimport pytz\nfrom dateutil.parser import isoparse\n\n# Constants\nTIMEZONES = ['America/New_York', 'Europe/London', 'Asia/Shanghai', 'Asia/Tokyo', 'Australia/Sydney']\n\ndef task_func(date_str, from_tz):\n    from_tz = pytz.timezone(from_tz)\n    to_tz = pytz.timezone(choice(TIMEZONES))\n    given_date = isoparse(date_str).replace(tzinfo=from_tz)\n    converted_date = given_date.astimezone(to_tz)\n\n    return converted_date.isoformat(), to_tz.zone\n<end>"}
{"id": 391, "answer": " <start>\nimport pandas as pd\nfrom pandas.core.methods.to_dict import DataFrame as DataFrameToDict\n\nOUTPUT_DIR = './output'\n\ndef task_func(df: pd.DataFrame, filename: str) -> str:\n    start_time = time.time()\n    # Ensure the data directory exists\n    if not os.path.exists(OUTPUT_DIR):\n        os.makedirs(OUTPUT_DIR)\n\n    file_path = os.path.join(OUTPUT_DIR, filename)\n\n    # Save DataFrame as JSON Lines\n    with open(file_path, 'w') as file:\n        df_to_dict = DataFrameToDict(df, orient='records')\n        for record in df_to_dict:\n            json.dump(record, file)\n            file.write('\\n')\n    end_time = time.time()  # End timing\n    cost = f\"Operation completed in {end_time - start_time} seconds.\"\n    return os.path.abspath(file_path)\n<end>"}
{"id": 375, "answer": " <start>\n```python\nfrom datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\nfrom pandas.plotting import register_matplotlib_converters\n\nregister_matplotlib_converters()\n\ndef task_func(days_in_past=7, random_seed=0):\n\n    random.seed(random_seed)\n\n    if days_in_past < 1:\n        raise ValueError(\"days_in_past must be in the past\")\n\n    ACTIVITIES = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n\n    data = []\n    for i in range(days_in_past):\n        date = datetime.now().date() - timedelta(days=i)\n        for activity in ACTIVITIES:\n            duration = random.randint(0, 120)\n            data.append((date, activity, duration))\n\n    df = pd.DataFrame(data, columns=[\"Date\", \"Activity\", \"Duration\"])\n    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n    ax = sns.lineplot(data=df, x=\"Date\", y=\"Duration\", hue=\"Activity\")\n    return ax, df\n```\n<end>"}
{"id": 393, "answer": " <start>\nimport pytz\nfrom dateutil import parser\n\ndef task_func(date_str, from_tz, to_tz):\n    # Parse the given date string using dateutil.parser\n    given_date = parser.parse(date_str)\n\n    # Convert the given date to the destination timezone\n    given_date_in_to_tz = given_date.astimezone(pytz.timezone(to_tz))\n\n    # Get the current time in the destination timezone\n    current_date_in_to_tz = pytz.utc.localize(parser.parse(parser.now().strftime('%Y-%m-%d %H:%M:%S')))\n\n    # Calculate the time difference in seconds\n    time_difference = (current_date_in_to_tz - given_date_in_to_tz).total_seconds()\n\n    return int(time_difference)\n<end>"}
{"id": 394, "answer": " <start>\nimport pytz\nfrom dateutil.parser import parse\nfrom datetime import datetime\n\ndef task_func(date_str, from_tz, to_tz):\n    # Get timezone objects for the source and destination timezones\n    from_tz_obj = pytz.timezone(from_tz)\n    to_tz_obj = pytz.timezone(to_tz)\n\n    # Parse the given date string and localize it to the source timezone\n    given_date = parse(date_str, tzinfos={from_tz: from_tz_obj}).astimezone(from_tz_obj)\n\n    # Convert the given date to the destination timezone\n    given_date_in_to_tz = given_date.astimezone(to_tz_obj)\n\n    # Get the current time in the destination timezone\n    current_date_in_to_tz = datetime.now(to_tz_obj)\n\n    # Calculate the time difference in seconds\n    time_difference = (current_date_in_to_tz - given_date_in_to_tz).total_seconds()\n\n    return int(time_difference)\n<end>"}
{"id": 395, "answer": " <start>\nfrom datetime import datetime\nfrom dateutil import tz\n\ndef task_func(date_str, tz_str):\n    tz_obj = tz.gettz(tz_str)\n    given_date = datetime.fromisoformat(date_str + \"Z\")\n    given_date = tz_obj.localize(given_date)\n\n    next_year = given_date.year + 1\n    new_year = tz_obj.normalize(given_date.replace(year=next_year, month=1, day=1))\n\n    time_until_new_year = (new_year - given_date).total_seconds()\n\n    return int(time_until_new_year)\n<end>"}
{"id": 396, "answer": " <start>\nfrom datetime import datetime, timedelta\nimport pytz\nfrom dateutil.parser import parse\n\ndef task_func(date_str, tz_str):\n    tz = pytz.timezone(tz_str)\n    given_date = parse(date_str).replace(tzinfo=tz)  # Correctly handle timezone conversion\n\n    next_year = given_date.replace(year=given_date.year + 1)\n    new_year = next_year.replace(month=1, day=1, hour=0, minute=0, second=0, microsecond=0, tzinfo=tz)  # Correctly create the New Year moment in the specified timezone\n\n    time_until_new_year = new_year - given_date\n\n    return int(time_until_new_year.total_seconds())\n<end>"}
{"id": 397, "answer": " <start>\nimport numpy as np\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nfrom matplotlib.cbook import index_of\n\ndef task_func(x, y, labels):\n\n    if not x or not y or not labels:\n        raise ValueError(\"Empty data lists provided.\")\n\n    def exponential_func(x, a, b, c):\n        \"\"\"Exponential function model for curve fitting.\"\"\"\n        return a * np.exp(-b * x) + c\n\n    fig, ax = plt.subplots()\n\n    for i in range(len(x)):\n        # Fit the exponential model to the data\n        x_ = index_of(y[i])\n        popt, _ = curve_fit(exponential_func, x_[0], y[i])\n\n        # Plot the fitted curve\n        ax.plot(x_[0], exponential_func(x_[0], *popt), label=labels[i])\n\n    ax.legend()\n\n    return fig\n<end>"}
{"id": 398, "answer": " <start>\nimport numpy as np\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\n\ndef task_func(x, y, labels):\n\n    if not x or not y or not labels:\n        raise ValueError(\"Empty data lists provided.\")\n\n    def exponential_func(x, a, b, c):\n        \"\"\"Exponential function model for curve fitting.\"\"\"\n        return a * np.exp(-b * x) + c\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n\n    for i in range(len(x)):\n        # Fit the exponential model to the data\n        popt, _ = curve_fit(exponential_func, x[i], y[i])\n\n        # Plot the fitted curve\n        ax.plot(x[i], exponential_func(x[i], *popt), label=labels[i])\n\n    ax.legend()\n\n    return fig\n<end>"}
{"id": 399, "answer": " <start>\nimport pandas as pd\nimport nltk\nfrom nltk.tokenize import word_tokenize\n\ndef task_func(text):\n    if not isinstance(text, str):\n        raise ValueError(\"The input should be a string.\")\n\n    tokens = word_tokenize(text)\n    dollar_words = [token for token in tokens if token.startswith('$')]\n    freq = nltk.FreqDist(dollar_words)\n    df = pd.DataFrame(list(freq.items()), columns=[\"Word\", \"Frequency\"])\n    return df\n<end>"}
{"id": 355, "answer": " <start>\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport pandas as pd\n\ndef task_func(documents):\n    vectorizer = TfidfVectorizer()\n\n    def custom_tokenizer(doc):\n        return [token for token in word_tokenize(doc) if token]\n\n    vectorizer.set_params(tokenizer=custom_tokenizer)\n    tfidf_matrix = vectorizer.fit_transform(documents)\n    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names())\n\n    return tfidf_df\n<end>"}
{"id": 401, "answer": " <start>\nimport pandas as pd\nfrom pandas._libs.tslibs.timezones import tz_standardize\n\ndef task_func(articles, timezone):\n\n    if not isinstance(articles, list):\n        raise TypeError(\"articles should be a list of dictionaries.\")\n\n    if not all(isinstance(item, dict) for item in articles):\n        raise TypeError(\"articles should be a list of dictionaries.\")\n\n    if len(articles) == 0:\n        raise ValueError(\"input articles list should contain at least one article.\")\n\n    if any(not sorted(dic.keys()) == ['category', 'id', 'published_time', 'title', 'title_url'] for dic in articles):\n        raise ValueError(\n            \"input dictionaries must contain the following keys: 'category', 'id', 'title', 'title_url', 'published_time'\")\n\n    tz = tz_standardize(pytz.timezone(timezone))\n    articles_tz = [{\"category\": article[\"category\"], \"id\": article[\"id\"], \"title\": article[\"title\"], \"title_url\": article[\"title_url\"], \"published_time\": article[\"published_time\"].astimezone(tz)} for article in articles]\n\n    df = pd.DataFrame(articles_tz)\n    df['published_time'] = df['published_time'].dt.hour\n\n    analysis_df = df.groupby('category')['published_time'].agg(['count', 'mean', 'min', 'max'])\n\n    return analysis_df\n<end>"}
{"id": 376, "answer": " <start>\n```python\nfrom datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\nfrom pandas.plotting import register_matplotlib_converters\n\nregister_matplotlib_converters()\n\ndef task_func(days_in_past=7, random_seed=0):\n\n    random.seed(random_seed)\n\n    if days_in_past < 1:\n        raise ValueError(\"days_in_past must be in the past\")\n\n    ACTIVITIES = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n\n    data = []\n    for i in range(days_in_past):\n        date = datetime.now().date() - timedelta(days=i)\n        for activity in ACTIVITIES:\n            duration = random.randint(0, 120)\n            data.append([pd.to_datetime(date), activity, duration])\n\n    df = pd.DataFrame(data, columns=[\"Date\", \"Activity\", \"Duration\"])\n    ax = sns.lineplot(data=df, x=\"Date\", y=\"Duration\", hue=\"Activity\")\n    return ax, df\n```\n<end>"}
{"id": 403, "answer": " <start>\n```python\nimport pandas as pd\nfrom itertools import combinations\n\ndef task_func(number_list, element):\n    combinations_list = list(combinations(number_list, 3))\n    valid_combinations = [comb for comb in combinations_list if sum(comb) == element]\n\n    # Return only unique combinations\n    return pd.DataFrame(valid_combinations, columns=['Combinations'])\n```\n<end>"}
{"id": 404, "answer": " <start>\nfrom itertools import combinations\nimport pandas as pd\n\ndef task_func(number_list, element):\n    combinations_list = list(combinations(number_list, 3))\n    valid_combinations = [comb for comb in combinations_list if sum(comb) == element]\n\n    # Fill missing values with NaN\n    valid_combinations = pd.DataFrame(valid_combinations, columns=['Combinations'])\n\n    # Return only unique combinations\n    return valid_combinations.drop_duplicates()\n<end>"}
{"id": 405, "answer": " <start>\nimport sqlite3\nfrom faker import Faker\n\ndef task_func(db_path, num_entries, users=None, countries=None, random_seed=None):\n    fake = Faker()\n\n    if users is None:\n        users = ['Alice', 'Bob', 'Charlie', 'Dave', 'Eve']\n\n    if countries is None:\n        countries = ['USA', 'UK', 'Canada', 'Australia', 'India']\n\n    random.seed(random_seed)\n\n    conn = sqlite3.connect(db_path)\n    c = conn.cursor()\n\n    c.execute('''\n        CREATE TABLE users\n        (id INTEGER PRIMARY KEY, name TEXT, age INTEGER, country TEXT)\n    ''')\n\n    for _ in range(num_entries):\n        user = fake.random_element(users)\n        age = random.randint(20, 60)\n        country = fake.random_element(countries)\n        c.execute('INSERT INTO users (name, age, country) VALUES (?, ?, ?)', (user, age, country))\n\n    conn.commit()\n    conn.close()\n\n    return db_path\n<end>"}
{"id": 406, "answer": " <start>\nimport sqlite3\nfrom faker import Faker\n\ndef task_func(db_path, num_entries, users=None, countries=None, random_seed=None):\n\n    fake = Faker()\n\n    if users is None:\n        users = ['Alice', 'Bob', 'Charlie', 'Dave', 'Eve']\n\n    if countries is None:\n        countries = ['USA', 'UK', 'Canada', 'Australia', 'India']\n\n    if random_seed is not None:\n        fake.seed_instance(random_seed)\n\n    conn = sqlite3.connect(db_path)\n    c = conn.cursor()\n\n    c.execute('''\n        CREATE TABLE users\n        (id INTEGER PRIMARY KEY, name TEXT, age INTEGER, country TEXT)\n    ''')\n\n    for _ in range(num_entries):\n        user = fake.random_element(users)\n        age = fake.random_int(min=20, max=60)\n        country = fake.random_element(countries)\n        c.execute('INSERT INTO users (name, age, country) VALUES (?, ?, ?)', (user, age, country))\n\n    conn.commit()\n    conn.close()\n\n    return db_path\n<end>"}
{"id": 411, "answer": " <start>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nARRAY_SIZE = 10000\n\ndef task_func():\n    array = np.random.randint(1, 500, size=ARRAY_SIZE)\n    mean = np.mean(array)\n    std = np.std(array)\n\n    fig, ax = plt.subplots()\n    ax.hist(array, bins=50, density=True)  # Added density parameter\n    ax.set_title('Histogram of Random Values')\n    ax.set_xlabel('Val')\n    ax.set_ylabel('Freq Density')\n\n    # Plot mean and standard deviation\n    ax.axvline(mean, color='red', linestyle='--')\n    ax.axvspan(mean - std, mean + std, color='purple', alpha=0.2, linestyle='--')\n\n    return array, mean, std, ax\n<end>"}
{"id": 412, "answer": " <start>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func():\n    array = np.random.randint(1, 500, size=ARRAY_SIZE)\n    mean = np.mean(array)\n    std = np.std(array)\n\n    fig, ax = plt.subplots()\n    ax.hist(array, bins='auto')\n    ax.set_title('Histogram of Random Values')\n    ax.set_xlabel('Val')\n    ax.set_ylabel('Freq')\n\n    # Plot mean and standard deviation\n    ax.axvline(mean, color='red', linestyle='--')\n    ax.axvspan(mean - std, mean + std, color='purple', alpha=0.2, linestyle='--')\n\n    return array, mean, std, ax\n<end>"}
{"id": 415, "answer": " <start>\nimport pandas.io.sql as sql\n\ndef task_func(db_path: str, table_name: str, column_name: str) -> pd.DataFrame:\n    with sql.SQLiteDatabase(db_path) as conn:\n        df = pd.read_sql_table(table_name, conn)\n        df[column_name] = df[column_name].replace('\\n', '<br>', regex=True)\n    return df\n<end>"}
{"id": 392, "answer": " <start>\nimport pandas as pd\nimport os\n\nOUTPUT_DIR = './output'\n\ndef task_func(df: pd.DataFrame, filename: str) -> str:\n    start_time = time.time()\n\n    # Ensure the data directory exists\n    if not os.path.exists(OUTPUT_DIR):\n        os.makedirs(OUTPUT_DIR)\n\n    file_path = os.path.join(OUTPUT_DIR, filename)\n\n    # Save DataFrame as JSON Lines\n    df.to_json(file_path, lines=True, orient='records')\n\n    end_time = time.time()  # End timing\n    cost = f\"Operation completed in {end_time - start_time} seconds.\"\n    return os.path.abspath(file_path)\n<end>"}
{"id": 417, "answer": " <start>\nimport numpy as np\nfrom datetime import datetime, date\nfrom numpy.core.multiarray import busday_count\n\ndef task_func(rows=3, columns=2, start_date=datetime(2021, 1, 1), end_date=datetime(2021, 12, 31), seed=0):\n    # Convert start_date and end_date to numpy datetime64 objects\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Calculate the number of days between start_date and end_date\n    total_days = (end_date - start_date).days + 1\n\n    # Generate an array of dates between start_date and end_date\n    dates = np.array(list(date.fromisoformat(start_date.isoformat() + ' +%i days' % i) for i in range(total_days)))\n\n    # Filter out weekends and holidays (if any)\n    weekmask = np.array([1, 1, 1, 1, 1, 0, 0])\n    holidays = np.array([])\n    filtered_dates = dates[busday_count(dates, dates[-1], weekmask=weekmask, holidays=holidays) > 0]\n\n    # Randomly select unique dates within the filtered dates without replacement using numpy.random.choice\n    selected_dates = np.random.choice(filtered_dates, size=(rows, columns), replace=False)\n\n    # Generate the matrix with selected unique dates\n    matrix = selected_dates.reshape(rows, columns)\n\n    return matrix\n<end>"}
{"id": 418, "answer": " <start>\nimport numpy as np\nfrom datetime import datetime, timedelta\n\ndef task_func(rows=3, columns=2, start_date=datetime(2021, 1, 1), end_date=datetime(2021, 12, 31), seed=0):\n    if seed is not None:\n        np.random.seed(seed)\n\n    total_days = (end_date - start_date).days + 1\n    selected_dates = np.random.choice(total_days, size=(rows, columns), replace=False)\n    matrix = (start_date + timedelta(days=selected_dates)).reshape(rows, columns)\n\n    return matrix\n<end>"}
{"id": 419, "answer": " <start>\nimport itertools\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nSHAPES = [\"Circle\", \"Square\", \"Triangle\", \"Rectangle\", \"Pentagon\", \"Hexagon\",\n          \"Heptagon\", \"Octagon\", \"Nonagon\", \"Decagon\"]\nCOLORS = [\"Red\", \"Blue\", \"Green\", \"Yellow\", \"Black\", \"White\", \"Purple\",\n          \"Orange\", \"Pink\", \"Brown\"]\n\ndef task_func(num_pairs=10):\n    max_pairs = len(SHAPES) * len(COLORS)\n    num_pairs = max(1, min(num_pairs, max_pairs))\n\n    pairs = [f\"{s}:{c}\" for s, c in itertools.product(SHAPES, COLORS)][:num_pairs]\n    data = {'pairs': pairs}\n\n    # Drawing the countplot\n    ax = sns.countplot(x='pairs', data=data, hue='pairs', palette=\"Set3\", dodge=False)\n    ax.set_xticklabels(pairs)\n    ax.tick_params(rotation=90)\n\n    return ax\n<end>"}
{"id": 420, "answer": " <start>\nimport itertools\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nSHAPES = [\"Circle\", \"Square\", \"Triangle\", \"Rectangle\", \"Pentagon\", \"Hexagon\",\n          \"Heptagon\", \"Octagon\", \"Nonagon\", \"Decagon\"]\nCOLORS = [\"Red\", \"Blue\", \"Green\", \"Yellow\", \"Black\", \"White\", \"Purple\",\n          \"Orange\", \"Pink\", \"Brown\"]\n\ndef task_func(num_pairs=10):\n    max_pairs = len(SHAPES) * len(COLORS)\n    num_pairs = min(num_pairs, max_pairs)\n\n    pairs = [f\"{s}:{c}\" for s, c in itertools.product(SHAPES, COLORS)][:num_pairs]\n\n    # Drawing the countplot\n    ax = sns.countplot(x=pairs, hue=pairs, palette=\"Set3\", legend=False)\n    ax.set_xticklabels(pairs, rotation=90)\n\n    return ax\n<end>"}
{"id": 421, "answer": " <start>\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\ndef task_func(arr: np.ndarray) -> (plt.Axes, np.ndarray):\n    # Calculating row sums\n    row_sums = arr.sum(axis=1)\n\n    # Normalizing the data\n    mean, std_dev = np.mean(row_sums), np.std(row_sums)\n    normalized_data = (row_sums - mean) / std_dev if std_dev != 0 else np.zeros_like(row_sums)\n\n    # Plotting the histogram\n    _, ax = plt.subplots()\n    ax.hist(normalized_data, bins=30, density=True, alpha=0.6, color=\"g\")\n\n    # Plotting the PDF of a standard normal distribution\n    x = np.linspace(norm.ppf(0.01), norm.ppf(0.99), 100)\n    ax.plot(x, norm.pdf(x), \"r-\", lw=2)\n    ax.set_title(\"Histogram of Normalized Data with Standard Normal PDF\")\n\n    return ax, normalized_data\n<end>"}
{"id": 422, "answer": " <start>\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\ndef task_func(arr: np.ndarray) -> (plt.Axes, np.ndarray):\n    # Calculating row sums\n    row_sums = arr.sum(axis=1)\n\n    # Normalizing the data\n    mean = np.mean(row_sums)\n    std_dev = np.std(row_sums)\n    normalized_data = (\n        (row_sums - mean) / std_dev if std_dev != 0 else np.zeros_like(row_sums)\n    )\n\n    # Plotting the histogram\n    fig, ax = plt.subplots()\n    ax.hist(normalized_data, bins=30, density=True, alpha=0.6, color=\"g\")\n\n    # Plotting the PDF of a standard normal distribution\n    x = np.linspace(norm.ppf(0.01), norm.ppf(0.99), 100)\n    ax.plot(x, norm.pdf(x), \"r-\", lw=2)\n    ax.set_title(\"Histogram of Normalized Data with Standard Normal PDF\")\n\n    return fig, ax, normalized_data\n<end>"}
{"id": 423, "answer": " <start>\nimport requests\nimport logging\nfrom requests.adapters import Retry\n\ndef task_func(repo_url: str) -> dict:\n    session = requests.Session()\n    retry_strategy = Retry(\n        total=5,\n        backoff_factor=0.3,\n        status_forcelist=[429],  # Handle 429 (API rate limit)\n    )\n    session.mount(\"https://\", retry_strategy)\n\n    try:\n        response = session.get(repo_url, timeout=2)\n        response.raise_for_status()  # Raises HTTPError for bad requests\n\n        if response.status_code == 403 and response.json().get(\"message\") == \"API rate limit exceeded\":\n            raise requests.exceptions.HTTPError(\"API rate limit exceeded\")\n\n        if response.json().get(\"open_issues_count\", 0) > 10000:\n            logging.warning(\"The repository has more than 10000 open issues.\")\n\n        return response.json()\n\n    except requests.exceptions.RequestException as e:\n        raise requests.exceptions.RequestException(\n            f\"Error fetching repo info: {e}\"\n        ) from e\n<end>"}
{"id": 424, "answer": " <start>\nimport requests\nimport logging\nfrom requests.compat import parse_http_list, urlunparse, urlsplit, quote\nfrom requests.exceptions import HTTPError, Timeout, ConnectionError, TooManyRedirects, RequestException\n\ndef task_func(repo_url: str) -> dict:\n    try:\n        response = requests.get(repo_url, timeout=2)\n        response.raise_for_status(allow_redirects=False)  # Raises HTTPError for bad requests\n        repo_info = response.json()\n\n        if response.status_code == 403 and repo_info.get(\"message\") == \"API rate limit exceeded\":\n            raise HTTPError(\"API rate limit exceeded\")\n\n        if repo_info.get(\"open_issues_count\", 0) > 10000:\n            logging.warning(\"The repository has more than 10000 open issues.\")\n\n        return repo_info\n\n    except HTTPError as e:\n        raise HTTPError(e)\n    except Timeout:\n        raise ConnectionError(\"Request timed out\")\n    except ConnectionError:\n        raise ConnectionError(\"Connection error occurred\")\n    except TooManyRedirects:\n        raise ConnectionError(\"Too many redirects\")\n    except RequestException as e:\n        raise RequestException(f\"Error fetching repo info: {e}\") from e\n<end>"}
{"id": 427, "answer": " <start>\nimport re\nfrom collections import Counter\nimport matplotlib.fontconfig_pattern as mfcp\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Process text and count words\n    cleaned_text = re.sub(f\"[{mfcp.one_of(punctuation)}]\", \"\", text).lower()\n    words = cleaned_text.split()\n    word_counts = Counter(words)\n    most_common_words = word_counts.most_common(10)\n\n    # Plotting\n    _, ax = plt.subplots()\n    if most_common_words:  # Check if the list is not empty\n        words, counts = zip(*most_common_words)\n        ax.bar(range(len(words)), counts)\n        ax.set_xticks(range(len(words)))\n        ax.set_xticklabels(words)\n    else:  # Handle empty case\n        ax.bar([], [])\n\n    return most_common_words, ax\n<end>"}
{"id": 428, "answer": " <start>\nimport re\nfrom collections import Counter\nimport matplotlib.fontconfig_pattern as mfontconfig_pattern\nimport matplotlib.mathtext as mmathtext\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Define punctuation pattern\n    punctuation = mfontconfig_pattern.StringEnd(mfontconfig_pattern.Literal(',') |\n                                                 mfontconfig_pattern.Literal('.') |\n                                                 mfontconfig_pattern.Literal(';') |\n                                                 mfontconfig_pattern.Literal(':') |\n                                                 mfontconfig_pattern.Literal('/') |\n                                                 mfontconfig_pattern.Literal('-') |\n                                                 mfontconfig_pattern.Literal('!') |\n                                                 mfontconfig_pattern.Literal('?') |\n                                                 mfontconfig_pattern.Literal(' ') |\n                                                 mfontconfig_pattern.Literal('\\n'))\n\n    # Process text and count words\n    cleaned_text = re.sub(f\"[{punctuation.search_string(text)}]\", \"\", text).lower()\n    words = cleaned_text.split()\n    word_counts = Counter(words)\n    most_common_words = word_counts.most_common(10)\n\n    # Plotting\n    _, ax = plt.subplots()\n    if most_common_words:\n        ax.bar(*zip(*most_common_words))\n    else:\n        ax.bar([], [])\n\n    return most_common_words, ax\n<end>"}
{"id": 429, "answer": " <start>\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef task_func(texts):\n\n    # Handle empty input\n    if not texts:\n        return [], []\n\n    # Remove URLs\n    cleaned_texts = [re.sub('http[s]?://\\S+', '', text) for text in texts]\n\n    # Create and fit the TfidfVectorizer\n    vectorizer = TfidfVectorizer()\n    vectorizer.fit(cleaned_texts)\n\n    # Transform the cleaned texts using the fitted vectorizer\n    tfidf_matrix = vectorizer.transform(cleaned_texts)\n\n    # Convert the sparse matrix to a dense format, round the values, convert to tuples and return along with feature names\n    dense_matrix = [tuple(round(val, 8) for val in row) for row in tfidf_matrix.toarray().tolist()]\n    return dense_matrix, list(vectorizer.get_feature_names())\n<end>"}
{"id": 430, "answer": " <start>\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef task_func(texts):\n\n    # Handle empty input\n    if not texts:\n        return [], []\n\n    # Remove URLs\n    cleaned_texts = [re.sub('http[s]?://\\S+', '', text) for text in texts]\n\n    # Initialize TfidfVectorizer with default parameters\n    vectorizer = TfidfVectorizer()\n\n    # Fit and transform the data in one step\n    tfidf_matrix = vectorizer.fit_transform(cleaned_texts, autofit=True)\n\n    # Convert the sparse matrix to a dense format, round the values, convert to tuples and return along with feature names\n    dense_matrix = [tuple(round(val, 8) for val in row) for row in tfidf_matrix.toarray().tolist()]\n    return dense_matrix, list(vectorizer.get_feature_names_out())\n<end>"}
{"id": 400, "answer": " <start>\nimport nltk\nimport string\nimport pandas as pd\n\ndef task_func(text):\n    if not isinstance(text, str):\n        raise ValueError(\"The input should be a string.\")\n\n    tk = nltk.tokenize.word_tokenize\n    words = tk(text)\n    dollar_words = [word for word in words if word.startswith('$') and not any(c in string.punctuation for c in word)]\n    freq = nltk.FreqDist(dollar_words)\n    df = pd.DataFrame(list(freq.items()), columns=[\"Word\", \"Frequency\"])\n    return df\n<end>"}
{"id": 402, "answer": " <start>\nimport pandas as pd\nimport pytz\nfrom datetime import datetime\n\ndef task_func(articles, timezone):\n\n    if not isinstance(articles, list):\n        raise TypeError(\"articles should be a list of dictionaries.\")\n\n    if not all(isinstance(item, dict) for item in articles):\n        raise TypeError(\"articles should be a list of dictionaries.\")\n\n    if len(articles) == 0:\n        raise ValueError(\"input articles list should contain at least one article.\")\n\n    required_keys = ['category', 'id', 'published_time', 'title', 'title_url']\n    for article in articles:\n        if not set(article.keys()) >= set(required_keys):\n            raise ValueError(\n                \"input dictionaries must contain the following keys: 'category', 'id', 'title', 'title_url', 'published_time'\")\n\n    tz = pytz.timezone(timezone)\n    articles_with_tz = [\n        {k: (v if k != 'published_time' else pd.to_datetime(v).astimezone(tz)) for k, v in article.items()}\n        for article in articles\n    ]\n\n    df = pd.DataFrame(articles_with_tz)\n    df['published_time'] = df['published_time'].dt.hour\n\n    analysis_df = df.groupby('category')['published_time'].agg(['count', 'mean', 'min', 'max'])\n\n    return analysis_df\n<end>"}
{"id": 416, "answer": " <start>\nimport pandas.io.sql as sql\n\ndef task_func(db_path: str, table_name: str, column_name: str) -> pd.DataFrame:\n    with sql.SQLiteDatabase(db_path) as conn:\n        df = pd.read_sql_table(table_name, conn)\n        df[column_name] = df[column_name].replace('\\n', '<br>', regex=True)\n    return df\n<end>"}
